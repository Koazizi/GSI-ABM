{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afdb98a6-740d-4929-b1c7-fb7184f5561a",
   "metadata": {},
   "source": [
    "### Importing Required Libraries and Loading Geospatial Data\n",
    "In this initial cell, we import all necessary Python libraries and load the geospatial data required for the simulation. This includes shapefiles for tracts, districts, and parcels, which provide the geographic context for the agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001fe4a2-4aa1-4bf9-b47e-f34e2d9b3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from mesa import Agent, Model\n",
    "from mesa.time import RandomActivation\n",
    "from mesa.space import MultiGrid\n",
    "from mesa.datacollection import DataCollector\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Load the shapefile containing geographic information about the tracts\n",
    "tracts = gpd.read_file(\"yourlocation/Shapefiles/New folder/Austin_tract_Merged1.shp\")\n",
    "\n",
    "districts = gpd.read_file(\"yourlocation/Shapefiles/New folder//Council_Districts.shp\")\n",
    "\n",
    "# Load the shapefile containing parcel information\n",
    "parcels = gpd.read_file(\"yourlocation/Shapefiles/New folder/Parcel_with_Building_Area1.shp\")\n",
    "\n",
    "# Print the column names of both shapefiles\n",
    "print(\"Tracts Shapefile Columns:\")\n",
    "print(tracts.columns)\n",
    "\n",
    "print(\"\\ndistricts Shapefile Columns:\")\n",
    "print(districts.columns)\n",
    "\n",
    "print(\"\\nParcels Shapefile Columns:\")\n",
    "print(parcels.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d36b93-5972-47b8-8259-7225267f5b3d",
   "metadata": {},
   "source": [
    "### Data Preprocessing: Reprojecting and Filtering Parcels\n",
    "This cell handles the preprocessing of parcel data, including reprojection to match the coordinate reference system (CRS) of the tracts, spatial joining to assign parcels to tracts, and filtering parcels based on area criteria to determine eligibility for agent placement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e5ab3-4de4-4fc1-b3a1-f924f959c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reproject the parcels to match the CRS of the tracts (EPSG:4269)\n",
    "parcels = parcels.to_crs(tracts.crs)\n",
    "\n",
    "# Perform a spatial join to assign each parcel to the tract it belongs to\n",
    "parcels_with_tracts = gpd.sjoin(parcels, tracts, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Drop rows with missing 'GEOID' values before filtering\n",
    "parcels_with_tracts = parcels_with_tracts.dropna(subset=['GEOID'])\n",
    "\n",
    "# Convert 'GEOID' to string if necessary\n",
    "parcels_with_tracts['GEOID'] = parcels_with_tracts['GEOID'].astype(str)\n",
    "tracts['GEOID'] = tracts['GEOID'].astype(str)\n",
    "\n",
    "# Define the target projected CRS for UTM Zone 14N (EPSG:26914)\n",
    "target_crs = \"EPSG:26914\"\n",
    "\n",
    "# Reproject the tracts and parcels to the target CRS\n",
    "tracts = tracts.to_crs(target_crs)\n",
    "parcels = parcels.to_crs(target_crs)\n",
    "parcels_with_tracts = parcels_with_tracts.to_crs(target_crs)\n",
    "\n",
    "# Filter parcels based on the area and building area criteria for agent placement\n",
    "eligible_parcels = parcels_with_tracts[(parcels_with_tracts[\"P_Area\"] >= 500) & \n",
    "                                       (parcels_with_tracts[\"P_Area\"] <= 10000) &\n",
    "                                       (parcels_with_tracts[\"B_Area\"] >= 500) & \n",
    "                                       (parcels_with_tracts[\"B_Area\"] <= 5000)]\n",
    "\n",
    "problematic_parcels = eligible_parcels[eligible_parcels['B_Area'] > eligible_parcels['P_Area']]\n",
    "eligible_parcels = eligible_parcels[eligible_parcels['B_Area'] <= eligible_parcels['P_Area']]\n",
    "\n",
    "print(\"\\neligible_parcels:\")\n",
    "print(eligible_parcels.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4e058-7db1-4b0b-86e3-eea160960704",
   "metadata": {},
   "source": [
    "### Defining Potential Score Calculation Functions\n",
    "Here, we define functions to calculate the potential scores for rain-catching and structural practices based on parcel areas. These scores influence the likelihood of agents adopting specific GSI practices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f09cc-6520-46bb-96bc-232312f315cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate potential scores for Rain-Catching\n",
    "def calculate_rain_catching_potential(B_Area):\n",
    "    if B_Area > 5000:\n",
    "        return 1.0  # High potential\n",
    "    elif 500 < B_Area <= 5000:\n",
    "        # Linear interpolation for moderate potential between 0.5 and 1.0\n",
    "        return 0.5 + ((B_Area - 500) / (5000 - 500)) * (1 - 0.5)\n",
    "    else:\n",
    "        return 0.5  # Lower potential\n",
    "\n",
    "# Function to calculate potential scores for Structural Practices\n",
    "def calculate_structural_practices_potential(P_Area, B_Area):\n",
    "    remaining_area = P_Area - B_Area\n",
    "    if remaining_area > 8000:\n",
    "        return 1.0  # High potential\n",
    "    elif 1000 < remaining_area <= 8000:\n",
    "        # Linear interpolation for moderate potential between 0.5 and 1.0\n",
    "        return 0.5 + ((remaining_area - 1000) / (8000 - 1000)) * (1 - 0.5)\n",
    "    else:\n",
    "        return 0.5  # Lower potential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3879bb39-134b-434b-a02d-6676fca3af6a",
   "metadata": {},
   "source": [
    "### Handling Data and Household Assignment\n",
    "This cell initializes data structures to store household information and behavior scores. It iterates over eligible parcels to assign households with various attributes based on socio-economic distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aee7b6-3c5e-4609-8fcd-eadae6fae916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that percentage columns are converted to numerical types\n",
    "tracts['White%'] = pd.to_numeric(tracts['White%'], errors='coerce')\n",
    "tracts['NonWht%'] = pd.to_numeric(tracts['NonWht%'], errors='coerce')\n",
    "tracts['NnAdvDg'] = pd.to_numeric(tracts['NnAdvDg'], errors='coerce')\n",
    "tracts['AdvDeg'] = pd.to_numeric(tracts['AdvDeg'], errors='coerce')\n",
    "\n",
    "# Precompute household data based on eligible parcels\n",
    "household_data = []  # Stores information about each household\n",
    "structural_behavior_scores = []  # Initialize the list to store total behavior scores\n",
    "raincatching_behavior_scores = []\n",
    "nonstructural_behavior_scores = []\n",
    "total_behavior_scores = []\n",
    "\n",
    "rain_catching_potential_values = []\n",
    "structural_practices_potential_values = []\n",
    "\n",
    "# Iterate over each eligible parcel and assign households\n",
    "for i, parcel in eligible_parcels.iterrows():\n",
    "    tract_id = parcel['GEOID']  # Using 'GEOID' as the tract ID\n",
    "    \n",
    "    # Extract distribution parameters from the tracts file for the current tract\n",
    "    tract_info = tracts[tracts['GEOID'] == tract_id].iloc[0]\n",
    "\n",
    "    # Income: Use log-normal distribution based on Incm_mn and Incm_sd\n",
    "    log_income_mean = tract_info['Incm_mn']\n",
    "    log_income_sd = tract_info['Incm_sd']\n",
    "\n",
    "    # Generate income\n",
    "    income = np.random.lognormal(log_income_mean, log_income_sd)\n",
    "\n",
    "    # Filter out parcels with income greater than 500,000\n",
    "    if income > 500000:\n",
    "        continue  # Skip this parcel and move to the next one\n",
    "\n",
    "    # Ensure income is at least 10,000\n",
    "    income = max(10000, income)\n",
    "    \n",
    "    # Race: Use White% and NonWht% to define the distribution\n",
    "    race_dist = [tract_info['White%'] / 100, tract_info['NonWht%'] / 100]\n",
    "    race = np.random.choice([\"White\", \"Non-White\"], p=race_dist)\n",
    "    \n",
    "    # Education Level: Use NnAdvDg and AdvDeg percentages\n",
    "    adv_degree_dist = [tract_info['AdvDeg'] / 100, tract_info['NnAdvDg'] / 100]\n",
    "    adv_degree = np.random.choice([\"Adv Degree\", \"No Adv Degree\"], p=adv_degree_dist)\n",
    "    \n",
    "    # Latent Variables: Use the mean (outside parentheses) and std (inside parentheses) for each latent variable\n",
    "    personal_norms_mean = tract_info['p((SD))'].split('(')[0].strip()\n",
    "    personal_norms_sd = tract_info['p((SD))'].split('(')[1].replace(')', '').strip()\n",
    "    personal_norms = np.random.normal(float(personal_norms_mean), float(personal_norms_sd))\n",
    "    \n",
    "    attitude_mean = tract_info['at((SD))'].split('(')[0].strip()\n",
    "    attitude_sd = tract_info['at((SD))'].split('(')[1].replace(')', '').strip()\n",
    "    attitude = np.random.normal(float(attitude_mean), float(attitude_sd))\n",
    "    \n",
    "    awareness_mean = tract_info['aw((SD))'].split('(')[0].strip()\n",
    "    awareness_sd = tract_info['aw((SD))'].split('(')[1].replace(')', '').strip()\n",
    "    awareness = np.random.normal(float(awareness_mean), float(awareness_sd))\n",
    "    \n",
    "    # Correct reference for social capital using 's_((SD)'\n",
    "    social_capital_mean = tract_info['s_((SD)'].split('(')[0].strip()\n",
    "    social_capital_sd = tract_info['s_((SD)'].split('(')[1].replace(')', '').strip()\n",
    "    social_capital = np.random.normal(float(social_capital_mean), float(social_capital_sd))\n",
    "\n",
    "    # Calculate Rain-Catching and Structural Practices Potential Scores in the loop\n",
    "    rain_catching_potential = calculate_rain_catching_potential(parcel['B_Area'])\n",
    "    structural_practices_potential = calculate_structural_practices_potential(parcel['P_Area'], parcel['B_Area'])\n",
    "\n",
    "\n",
    "    \n",
    "    # Calculate behavior scores based on potentials\n",
    "    # (Using initial latent variables for all behaviors)\n",
    "    structural_behavior = (\n",
    "        1.51  # Regression constant\n",
    "        + (-0.45 if race == \"Non-White\" else 0)  # Race effect\n",
    "\n",
    "        + (0.08 if income < 20000  # Very Low Income\n",
    "            else (-0.17  if income < 50000  # Low Income\n",
    "            else (0.15 )))\n",
    "        + (-0.26 if adv_degree == \"No Adv Degree\" else 0)  # Education effect\n",
    "      \n",
    "        + 0.37 * personal_norms  # Regression coefficient for personal norms\n",
    "        + 0.23 * social_capital  # Regression coefficient for social capital\n",
    "        - 0.02 * attitude\n",
    "        + 0.02 * awareness\n",
    "    ) * structural_practices_potential\n",
    "\n",
    "    raincatching_behavior = (\n",
    "        0.39  # Regression constant\n",
    "        + (-0.18 if race == \"Non-White\" else 0)  # Race effect\n",
    "        + (-0.05 if adv_degree == \"No Adv Degree\" else 0)  # Education effect\n",
    "        + (0.03 if income < 20000  # Very Low Income\n",
    "            else (0.02  if income < 50000  # Low Income\n",
    "            else (0.13)))\n",
    "        + 0.22 * personal_norms  # Regression coefficient for personal norms\n",
    "        + 0.07 * social_capital  # Regression coefficient for social capital\n",
    "        + 0.05 * attitude\n",
    "        - 0.02 * awareness\n",
    "    ) * rain_catching_potential\n",
    "\n",
    "    nonstructural_behavior = (\n",
    "        4.45  # Regression constant\n",
    "        + (-0.22 if race == \"Non-White\" else 0)  # Race effect\n",
    "        + (0.11 if adv_degree == \"No Adv Degree\" else 0)  # Education effect\n",
    "        + (0.13 if income < 20000  # Very Low Income\n",
    "            else (-0.21  if income < 50000  # Low Income\n",
    "            else (-0.24 if income < 80000 else 0)))\n",
    "        + 0.51 * personal_norms  # Regression coefficient for personal norms\n",
    "        + 0.11 * social_capital  # Regression coefficient for social capital\n",
    "        + 0.03 * attitude\n",
    "        + 0.14 * awareness\n",
    "    )  # No potential score applied for non-structural behavior\n",
    "\n",
    "    total_behavior = nonstructural_behavior + raincatching_behavior + structural_behavior\n",
    "\n",
    "    # Append behavior scores to respective lists\n",
    "    structural_behavior_scores.append(structural_behavior)\n",
    "    raincatching_behavior_scores.append(raincatching_behavior)\n",
    "    nonstructural_behavior_scores.append(nonstructural_behavior)\n",
    "    total_behavior_scores.append(total_behavior)\n",
    "    rain_catching_potential_values.append(rain_catching_potential)\n",
    "    structural_practices_potential_values.append(structural_practices_potential)\n",
    "\n",
    "    # Store the household data, including all behavior scores, potential scores, and the B_Area and P_Area values\n",
    "    household_data.append(\n",
    "        (\n",
    "            tract_id, parcel['GEOID'], attitude, awareness, social_capital, personal_norms,\n",
    "            income, race, adv_degree,\n",
    "            structural_behavior, raincatching_behavior, nonstructural_behavior, total_behavior,\n",
    "            structural_practices_potential, rain_catching_potential,  # Store the potential scores\n",
    "            parcel['B_Area'], parcel['P_Area']  # Store the building footprint and parcel shape area\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadac656-7736-415d-9356-b63cda8e6519",
   "metadata": {},
   "source": [
    "### Calculating Percentile Ranks and Thresholds for Behavior Scores\n",
    "This segment defines functions to calculate percentile ranks and sigmoid-based thresholds for behavior scores. These thresholds are used to determine whether agents adopt specific GSI practices based on their scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f69a4-4955-41ad-a462-2099878ca7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# Corrected function to calculate percentile ranks\n",
    "def calculate_percentile_ranks(behavior_scores):\n",
    "    \"\"\"\n",
    "    Calculate percentile ranks for behavior scores.\n",
    "\n",
    "    Args:\n",
    "        behavior_scores (list or np.array): List of behavior scores for which to calculate percentile ranks.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of percentile ranks (values between 0 and 1).\n",
    "    \"\"\"\n",
    "    sorted_indices = np.argsort(behavior_scores)  # Indices of sorted scores in ascending order\n",
    "    num_scores = len(behavior_scores)  # Total number of behavior scores\n",
    "    \n",
    "    ranks = np.zeros_like(behavior_scores)\n",
    "    for rank, index in enumerate(sorted_indices):\n",
    "        ranks[index] = rank + 1  # Rank starts from 1\n",
    "    \n",
    "    percentile_ranks = (ranks - 1) / (num_scores - 1)  # Adjust to get a rank between 0 and 1\n",
    "    \n",
    "    return percentile_ranks\n",
    "\n",
    "# Modified function to calculate sigmoid-based thresholds using percentile ranks\n",
    "def calculate_sigmoid_thresholds(behavior_scores, percentile_ranks):\n",
    "    \"\"\"\n",
    "    Calculate thresholds using a continuously scaled sigmoid function based on percentile ranks.\n",
    "    For higher percentiles, thresholds will be close to their score.\n",
    "    For lower percentiles, thresholds will be significantly higher than their score.\n",
    "\n",
    "    Args:\n",
    "        behavior_scores (list): The list of behavior scores.\n",
    "        percentile_ranks (list): The list of percentile ranks for agents.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of thresholds based on the sigmoid formula.\n",
    "    \"\"\"\n",
    "    behavior_scores = np.array(behavior_scores)\n",
    "    base = np.percentile(behavior_scores, 50)  # 75th percentile as the base\n",
    "\n",
    "    thresholds = np.zeros_like(behavior_scores)\n",
    "\n",
    "    for i, score in enumerate(behavior_scores):\n",
    "        x = percentile_ranks[i]  # Agent's percentile rank\n",
    "\n",
    "        # Dynamically calculate k based on the percentile rank\n",
    "        k = 1 + np.exp(-2 * (x - 0.5))  # Steepness grows as rank decreases\n",
    "\n",
    "        # Sigmoid-based threshold calculation with scaling for lower percentiles\n",
    "        sigmoid_value = 1 / (1 + np.exp(-k * (x - 0.5)))\n",
    "\n",
    "        # Threshold increases more significantly for lower percentiles\n",
    "        thresholds[i] = base + sigmoid_value * score * (1 + 0.5*(1 - x))\n",
    "\n",
    "    return thresholds\n",
    "\n",
    "# General function to calculate thresholds for any behavior type\n",
    "def calculate_behavior_thresholds(behavior_scores):\n",
    "    \"\"\"\n",
    "    General function to calculate thresholds for any behavior type based on sigmoid + score percentile.\n",
    "    \n",
    "    Args:\n",
    "        behavior_scores (list or np.array): List of behavior scores for a particular behavior type.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Array of calculated thresholds for the behavior scores.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate percentile ranks for the behavior scores\n",
    "    percentile_ranks = calculate_percentile_ranks(behavior_scores)\n",
    "    \n",
    "    # Step 2: Calculate thresholds using sigmoid + score percentile approach\n",
    "    thresholds = calculate_sigmoid_thresholds(behavior_scores, percentile_ranks)\n",
    "    \n",
    "    return thresholds\n",
    "\n",
    "# Recalculate thresholds using the adjusted function\n",
    "structural_behavior_thresholds = calculate_behavior_thresholds(structural_behavior_scores)\n",
    "raincatching_behavior_thresholds = calculate_behavior_thresholds(raincatching_behavior_scores)\n",
    "nonstructural_behavior_thresholds = calculate_behavior_thresholds(nonstructural_behavior_scores)\n",
    "total_behavior_thresholds = calculate_behavior_thresholds(total_behavior_scores)\n",
    "\n",
    "# Assign the new thresholds to households\n",
    "for i, household in enumerate(household_data):\n",
    "    household_data[i] = household + (\n",
    "        structural_behavior_thresholds[i],\n",
    "        raincatching_behavior_thresholds[i],\n",
    "        nonstructural_behavior_thresholds[i],\n",
    "        total_behavior_thresholds[i]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c047eb-da5d-4c22-9a32-ab9bd74807c0",
   "metadata": {},
   "source": [
    "### Visualizing Latent Variables and Socio-Economic Characteristics\n",
    "This cell generates histograms to visualize the distributions of latent variables (Attitude, Awareness, Social Capital, Personal Norms) and socio-economic characteristics (Income, Race, Advanced Degree) of the households.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579911f-ef53-4c9e-8b34-0e3cccb7dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the latent variables from the household_data\n",
    "attitude_values = [household[2] for household in household_data]  # Column index for attitude\n",
    "awareness_values = [household[3] for household in household_data]  # Column index for awareness\n",
    "social_capital_values = [household[4] for household in household_data]  # Column index for social capital\n",
    "personal_norms_values = [household[5] for household in household_data]  # Column index for personal norms\n",
    "\n",
    "# Create a 2x2 plot grid for the distributions\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Subplot for attitude\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(attitude_values, bins=30, color='blue', edgecolor='black')\n",
    "plt.title('Distribution of Attitude')\n",
    "plt.xlabel('Attitude')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Subplot for awareness\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(awareness_values, bins=30, color='green', edgecolor='black')\n",
    "plt.title('Distribution of Awareness')\n",
    "plt.xlabel('Awareness')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Subplot for social capital\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(social_capital_values, bins=30, color='orange', edgecolor='black')\n",
    "plt.title('Distribution of Social Capital')\n",
    "plt.xlabel('Social Capital')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Subplot for personal norms\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(personal_norms_values, bins=30, color='purple', edgecolor='black')\n",
    "plt.title('Distribution of Personal Norms')\n",
    "plt.xlabel('Personal Norms')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Now for the socio-economic characteristics plot\n",
    "import numpy as np\n",
    "\n",
    "# Flatten the lists for income, race, and advanced degree status\n",
    "income_values_flat = [household[6] for household in household_data]  # Column index for income\n",
    "race_values_flat = [household[7] for household in household_data]  # Column index for race\n",
    "adv_degree_values_flat = [household[8] for household in household_data]  # Column index for education level (Adv Degree)\n",
    "\n",
    "# Create a figure for the 3 side-by-side bar plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Income distribution plot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(income_values_flat, bins=100, color='blue', edgecolor='black')\n",
    "plt.title('Income Distribution')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Race distribution plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(['White', 'Non-White'], [race_values_flat.count('White'), race_values_flat.count('Non-White')], color=['blue', 'green'], edgecolor='black')\n",
    "plt.title('Race Distribution')\n",
    "plt.xlabel('Race')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Advanced degree distribution plot\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(['Adv Degree', 'No Adv Degree'], [adv_degree_values_flat.count('Adv Degree'), adv_degree_values_flat.count('No Adv Degree')], color=['blue', 'orange'], edgecolor='black')\n",
    "plt.title('Advanced Degree Distribution')\n",
    "plt.xlabel('Education Level')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a39d7-22d9-47a2-b7a9-5e0f8765c16f",
   "metadata": {},
   "source": [
    "### Plotting Behavior Score Distributions\n",
    "This section defines a helper function to plot distributions of various behavior scores and potential scores. It provides a comprehensive overview of how these scores are distributed across the households."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d2437-87ed-4e11-9f5e-526b627f3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_distribution(data, variable_name, bins=50):\n",
    "    \"\"\"\n",
    "    Plots the distribution of a given variable.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The data to be plotted (list, array, or pandas column).\n",
    "    variable_name (str): The name of the variable for labeling purposes.\n",
    "    bins (int): Number of bins for the histogram (default is 50).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.hist(data, bins=bins, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel(variable_name)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Distribution of {variable_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "plot_distribution(total_behavior_scores, \"total_behavior_scores\")\n",
    "plot_distribution(total_behavior_thresholds, \"total_behavior_thresholds\")\n",
    "plot_distribution(structural_behavior_scores, \"structural_behavior_scores\")\n",
    "plot_distribution(structural_behavior_thresholds, \"structural_behavior_thresholds\")\n",
    "plot_distribution(raincatching_behavior_thresholds, \"raincatching_behavior_thresholds\")\n",
    "plot_distribution(nonstructural_behavior_thresholds, \"nonstructural_behavior_thresholds\")\n",
    "plot_distribution(nonstructural_behavior_scores, \"nonstructural_behavior_scores\")\n",
    "# At the end of the loop, inspect the distribution of potential scores\n",
    "plot_distribution(rain_catching_potential_values, \"Rain Catching Potential Distribution\")\n",
    "plot_distribution(structural_practices_potential_values, \"Structural Practices Potential Distribution\")\n",
    "B_Area_values = eligible_parcels['B_Area'].tolist()\n",
    "plot_distribution(B_Area_values, \"Building Footprint (B_Area)\")\n",
    "\n",
    "remaining_values = eligible_parcels['P_Area'] - eligible_parcels['B_Area']\n",
    "\n",
    "plot_distribution(remaining_values, \"Remaining Footprint (P_Area - B_Area)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9383575-7586-445c-b2be-2b57354abe1a",
   "metadata": {},
   "source": [
    "### Assigning Importance Coefficients and Calculating Sub-Scores\n",
    "This cell assigns importance coefficients to various GSI subcategories and calculates sub-scores for rain-catching and structural practices based on these coefficients. These sub-scores further refine the behavior scores of the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b8c49-08ed-4490-85ab-0cbd83641de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance coefficients for rain-catching subcategories\n",
    "rain_catching_importance = {\n",
    "    \"rain_barrel\": 2,\n",
    "    \"cistern\": 8,\n",
    "    \"rain_garden\": 10\n",
    "}\n",
    "\n",
    "# Importance coefficients for structural behavior subcategories\n",
    "structural_practices_importance = {\n",
    "    \"yard_slope_change\": 2,\n",
    "    \"yard_vegetation_change\": 2,\n",
    "    \"reducing_irrigation_water\": 1\n",
    "}\n",
    "\n",
    "# Calculate total importance for rain-catching and structural practices\n",
    "total_rain_catching_importance = sum(rain_catching_importance.values())\n",
    "total_structural_practices_importance = sum(structural_practices_importance.values())\n",
    "\n",
    "# New calculations for rain-catching and structural behavior subcategories\n",
    "updated_household_data = []\n",
    "\n",
    "for i, household in enumerate(household_data):\n",
    "    # Extract the rain-catching and structural behavior scores\n",
    "    raincatching_behavior = household[10]\n",
    "    structural_behavior = household[9]\n",
    "    \n",
    "    # Calculate the sub-scores for rain-catching behavior\n",
    "    rain_barrel_score = (raincatching_behavior * rain_catching_importance['rain_barrel']) / total_rain_catching_importance\n",
    "    cistern_score = (raincatching_behavior * rain_catching_importance['cistern']) / total_rain_catching_importance\n",
    "    rain_garden_score = (raincatching_behavior * rain_catching_importance['rain_garden']) / total_rain_catching_importance\n",
    "\n",
    "    # Calculate the sub-scores for structural practices behavior\n",
    "    yard_slope_change_score = (structural_behavior * structural_practices_importance['yard_slope_change']) / total_structural_practices_importance\n",
    "    yard_vegetation_change_score = (structural_behavior * structural_practices_importance['yard_vegetation_change']) / total_structural_practices_importance\n",
    "    reducing_irrigation_water_score = (structural_behavior * structural_practices_importance['reducing_irrigation_water']) / total_structural_practices_importance\n",
    "\n",
    "    # Append the calculated sub-scores to the existing household data\n",
    "    household_data[i] = household + (\n",
    "        rain_barrel_score,\n",
    "        cistern_score,\n",
    "        rain_garden_score,\n",
    "        yard_slope_change_score,\n",
    "        yard_vegetation_change_score,\n",
    "        reducing_irrigation_water_score\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331fca7-5151-44f0-8a7d-196533093cad",
   "metadata": {},
   "source": [
    "### Assigning Price Sensitivity Based on Income Percentiles\n",
    "This part of the notebook assigns price sensitivity scores to households based on their income percentiles. These scores act as barriers to adopting certain GSI practices, reflecting the financial constraints faced by different income groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cbdd3b-190b-4a96-b388-c3d9884ebc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sort the household data based on income (assuming income is at index 6 of household data)\n",
    "household_data_sorted_by_income = sorted(household_data, key=lambda x: x[6])\n",
    "\n",
    "# Number of households\n",
    "num_households = len(household_data_sorted_by_income)\n",
    "\n",
    "# Function to assign price sensitivity based on percentiles\n",
    "def assign_price_sensitivity_by_percentile(household_index, total_households, thresholds):\n",
    "    # Calculate the percentile for the household\n",
    "    household_percentile = (household_index + 1) / total_households  # +1 to avoid zero-index issues\n",
    "\n",
    "    # Iterate through the thresholds and assign sensitivity\n",
    "    for threshold, sensitivity in thresholds:\n",
    "        if household_percentile <= threshold:\n",
    "            return sensitivity\n",
    "    return 0  # If none of the thresholds match, assign 0 sensitivity\n",
    "\n",
    "# Define thresholds and sensitivities for each practice\n",
    "# Ensure that thresholds cover distinct ranges\n",
    "rain_barrel_thresholds = [(0.02, 1), (0.05, 0.5), (0.10, 0.25)]  # Remaining will default to 0\n",
    "cistern_thresholds = [(0.15, 1), (0.25, 0.5), (0.60, 0.25)]  # Remaining will default to 0\n",
    "rain_garden_thresholds = [(0.20, 1), (0.40, 0.5), (0.60, 0.25)]  # Remaining will default to 0\n",
    "yard_vegetation_change_thresholds = [(0.10, 1), (0.20, 0.5), (0.40, 0.25)]  # Remaining will default to 0\n",
    "reducing_irrigation_water_thresholds = [(0.05, 1), (0.20, 0.5), (0.40, 0.25)]  # Remaining will default to 0\n",
    "yard_slope_change_thresholds = [(0.15, 1), (0.25, 0.5), (0.50, 0.25)]  # Remaining will default to 0\n",
    "\n",
    "# Iterate over each household and assign price sensitivity scores based on percentiles\n",
    "for i, household in enumerate(household_data_sorted_by_income):\n",
    "    # Calculate price sensitivities for each practice\n",
    "    price_sensitivity_rain_barrel = assign_price_sensitivity_by_percentile(i, num_households, rain_barrel_thresholds)\n",
    "    price_sensitivity_cistern = assign_price_sensitivity_by_percentile(i, num_households, cistern_thresholds)\n",
    "    price_sensitivity_rain_garden = assign_price_sensitivity_by_percentile(i, num_households, rain_garden_thresholds)\n",
    "    price_sensitivity_yard_vegetation_change = assign_price_sensitivity_by_percentile(i, num_households, yard_vegetation_change_thresholds)\n",
    "    price_sensitivity_reducing_irrigation_water = assign_price_sensitivity_by_percentile(i, num_households, reducing_irrigation_water_thresholds)\n",
    "    price_sensitivity_yard_slope_change = assign_price_sensitivity_by_percentile(i, num_households, yard_slope_change_thresholds)\n",
    "\n",
    "    # Update household data with the new price sensitivity values\n",
    "    household_data_sorted_by_income[i] = household + (\n",
    "        price_sensitivity_rain_barrel,\n",
    "        price_sensitivity_cistern,\n",
    "        price_sensitivity_rain_garden,\n",
    "        price_sensitivity_yard_vegetation_change,\n",
    "        price_sensitivity_reducing_irrigation_water,\n",
    "        price_sensitivity_yard_slope_change\n",
    "    )\n",
    "\n",
    "# Optionally, if you want to reflect these changes in the original household_data, you can copy back:\n",
    "household_data = household_data_sorted_by_income\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f828b8a1-e841-4fa8-bcf0-6481a768c2f6",
   "metadata": {},
   "source": [
    "### Reference Indices for Household Data\n",
    "This cell defines a dictionary to reference the indices of various attributes within the household_data tuples. This facilitates easier access to specific data points when building agents and analyzing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48953f0-9d3a-444b-adc8-3bcaa6ada71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference indices for household_data\n",
    "household_data_indices = {\n",
    "    'tract_id': 0,                                    # Tract GEOID\n",
    "    'GEOID': 1,                                       # Parcel GEOID (Optional, same as tract_id)\n",
    "    'attitude': 2,                                    # Latent variable: Attitude\n",
    "    'awareness': 3,                                   # Latent variable: Awareness\n",
    "    'social_capital': 4,                              # Latent variable: Social Capital\n",
    "    'personal_norms': 5,                              # Latent variable: Personal Norms\n",
    "    'income': 6,                                      # Household income\n",
    "    'race': 7,                                        # Race of the household\n",
    "    'adv_degree': 8,                                  # Advanced degree status\n",
    "    'structural_behavior': 9,                        # Behavior score for structural practices\n",
    "    'raincatching_behavior': 10,                     # Behavior score for rain-catching practices\n",
    "    'nonstructural_behavior': 11,                    # Behavior score for non-structural practices\n",
    "    'total_behavior': 12,                             # Total behavior score across all practices\n",
    "    'structural_practices_potential': 13,             # Potential score for structural practices\n",
    "    'rain_catching_potential': 14,                    # Potential score for rain-catching practices\n",
    "    'B_Area': 15,                                     # Building area\n",
    "    'P_Area': 16,                                     # Parcel shape area\n",
    "    'structural_behavior_threshold': 17,              # Threshold for structural practices behavior\n",
    "    'raincatching_behavior_threshold': 18,            # Threshold for rain-catching practices behavior\n",
    "    'nonstructural_behavior_threshold': 19,           # Threshold for non-structural behavior\n",
    "    'total_behavior_threshold': 20,                   # Threshold for total behavior score\n",
    "    'rain_barrel_score': 21,                          # Sub-score for rain barrel\n",
    "    'cistern_score': 22,                              # Sub-score for cistern\n",
    "    'rain_garden_score': 23,                          # Sub-score for rain garden\n",
    "    'yard_slope_change_score': 24,                    # Sub-score for yard slope change\n",
    "    'yard_vegetation_change_score': 25,               # Sub-score for yard vegetation change\n",
    "    'reducing_irrigation_water_score': 26,            # Sub-score for reducing irrigation water\n",
    "    'price_sensitivity_rain_barrel': 27,              # Price sensitivity for rain barrel\n",
    "    'price_sensitivity_cistern': 28,                  # Price sensitivity for cistern\n",
    "    'price_sensitivity_rain_garden': 29,              # Price sensitivity for rain garden\n",
    "    'price_sensitivity_yard_vegetation_change': 30,    # Price sensitivity for yard vegetation change\n",
    "    'price_sensitivity_reducing_irrigation_water': 31, # Price sensitivity for reducing irrigation water\n",
    "    'price_sensitivity_yard_slope_change': 32         # Price sensitivity for yard slope change\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc0b86-2be9-4249-b568-6e1336837159",
   "metadata": {},
   "source": [
    "### Plotting Price Sensitivity Distributions\n",
    "This cell defines a function to visualize the distribution of price sensitivity scores across different GSI practices. It utilizes bar plots to represent the percentage distribution of sensitivity levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843487f2-55be-4010-9000-e4746c83af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Function to plot the distribution of price sensitivities for a given index in household data\n",
    "def plot_price_sensitivity_distribution(household_data, index, title):\n",
    "    # Extract the price sensitivity values\n",
    "    sensitivities = [household[index] for household in household_data]\n",
    "    \n",
    "    # Count occurrences of each sensitivity value (0, 0.25, 0.5, 1)\n",
    "    sensitivity_counts = Counter(sensitivities)\n",
    "    \n",
    "    # Define the categories and their respective counts (ensuring all categories are represented)\n",
    "    categories = [0, 0.25, 0.5, 1]\n",
    "    counts = [sensitivity_counts.get(cat, 0) for cat in categories]\n",
    "    \n",
    "    # Normalize counts to represent percentages\n",
    "    total = sum(counts)\n",
    "    if total == 0:\n",
    "        print(f\"No data available for {title}.\")\n",
    "        return\n",
    "    percentages = [(count / total) * 100 for count in counts]\n",
    "    \n",
    "    # Create a bar plot for the categories\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar(categories, percentages, tick_label=[\"0\", \"0.25\", \"0.5\", \"1\"], edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Price Sensitivity')\n",
    "    plt.ylabel('Percentage')\n",
    "    \n",
    "    # Show grid\n",
    "    plt.grid(True, axis='y')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Reference indices for price sensitivities based on the corrected household_data_indices\n",
    "rain_barrel_index = household_data_indices['price_sensitivity_rain_barrel']           # 27\n",
    "cistern_index = household_data_indices['price_sensitivity_cistern']                   # 28\n",
    "rain_garden_index = household_data_indices['price_sensitivity_rain_garden']           # 29\n",
    "yard_vegetation_index = household_data_indices['price_sensitivity_yard_vegetation_change']  # 30\n",
    "reducing_irrigation_index = household_data_indices['price_sensitivity_reducing_irrigation_water'] # 31\n",
    "yard_slope_index = household_data_indices['price_sensitivity_yard_slope_change']     # 32\n",
    "\n",
    "# Plotting the distributions for each price sensitivity\n",
    "plot_price_sensitivity_distribution(\n",
    "    household_data_sorted_by_income,\n",
    "    rain_barrel_index,\n",
    "    'Distribution of Rain Barrel Price Sensitivity'\n",
    ")\n",
    "plot_price_sensitivity_distribution(\n",
    "    household_data_sorted_by_income,\n",
    "    cistern_index,\n",
    "    'Distribution of Cistern Price Sensitivity'\n",
    ")\n",
    "plot_price_sensitivity_distribution(\n",
    "    household_data_sorted_by_income,\n",
    "    rain_garden_index,\n",
    "    'Distribution of Rain Garden Price Sensitivity'\n",
    ")\n",
    "plot_price_sensitivity_distribution(\n",
    "    household_data_sorted_by_income,\n",
    "    yard_vegetation_index,\n",
    "    'Distribution of Yard Vegetation Change Price Sensitivity'\n",
    ")\n",
    "plot_price_sensitivity_distribution(\n",
    "    household_data_sorted_by_income,\n",
    "    reducing_irrigation_index,\n",
    "    'Distribution of Reducing Irrigation Water Price Sensitivity'\n",
    ")\n",
    "plot_price_sensitivity_distribution(\n",
    "    household_data_sorted_by_income,\n",
    "    yard_slope_index,\n",
    "    'Distribution of Yard Slope Change Price Sensitivity'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a5967-0f8d-4ed5-8453-485329a7807a",
   "metadata": {},
   "source": [
    "### Filtering Tracts with Sufficient Households and Plotting Distribution\n",
    "This cell filters out tracts with fewer than 10 households to ensure statistical reliability in the simulation. It then visualizes the distribution of the number of households per tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff664e-654b-4dd1-9092-ff18920a30f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of households per tract\n",
    "household_counts = pd.Series([household[0] for household in household_data]).value_counts()\n",
    "\n",
    "# Filter out tracts with fewer than 10 households\n",
    "household_data = [household for household in household_data if household_counts[household[0]] >= 10]\n",
    "\n",
    "# Extract the updated tract IDs from the filtered household_data\n",
    "tract_ids = [household[0] for household in household_data]\n",
    "\n",
    "# Count the number of households in each tract (after filtering)\n",
    "household_counts = pd.Series(tract_ids).value_counts()\n",
    "\n",
    "# Function to plot histogram\n",
    "def plot_distribution(data, variable_name, bins=50):\n",
    "    \"\"\"\n",
    "    Plots the distribution of a given variable.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The data to be plotted (list, array, or pandas column).\n",
    "    variable_name (str): The name of the variable for labeling purposes.\n",
    "    bins (int): Number of bins for the histogram (default is 50).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.hist(data, bins=bins, alpha=0.7, edgecolor='black')\n",
    "    plt.xlabel(variable_name)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Distribution of {variable_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the distribution of the number of households per tract\n",
    "plot_distribution(household_counts, \"Number of Households per Tract\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549aeb8-dd8b-403e-b037-1b62452b79b8",
   "metadata": {},
   "source": [
    "### Plotting Distributions of Various Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653fef06-13ff-4814-805e-165420934dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Variable names based on the provided indices, corresponding to columns 2 to 32\n",
    "variable_names = [\n",
    "    'Attitude', 'Awareness', 'Social Capital', 'Personal Norms', 'Income', 'Race', 'Advanced Degree',\n",
    "    'Structural Behavior Score', 'Raincatching Behavior Score', 'Nonstructural Behavior Score', 'Total Behavior Score',\n",
    "    'Structural Practices Potential', 'Rain Catching Potential', 'Building Area', 'Parcel Area',\n",
    "    'Structural Behavior Threshold', 'Raincatching Behavior Threshold', 'Nonstructural Behavior Threshold',\n",
    "    'Total Behavior Threshold', 'Rain Barrel Score', 'Cistern Score', 'Rain Garden Score',\n",
    "    'Yard Slope Change Score', 'Yard Vegetation Change Score', 'Reducing Irrigation Water Score',\n",
    "    'Price Sensitivity Rain Barrel', 'Price Sensitivity Cistern', 'Price Sensitivity Rain Garden',\n",
    "    'Price Sensitivity Yard Vegetation Change', 'Price Sensitivity Reducing Irrigation Water', 'Price Sensitivity Yard Slope Change'\n",
    "]\n",
    "\n",
    "# Plotting the variables from index 2 to 32 (inclusive)\n",
    "variables_to_plot = [[row[i] for row in household_data] for i in range(2, 33)]  # Extract columns 2 to 32\n",
    "\n",
    "# Create the plot with 4 plots per row (8 rows total to cover all 31 variables)\n",
    "fig, axs = plt.subplots(8, 4, figsize=(20, 30))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()[:len(variables_to_plot)]):\n",
    "    ax.hist(variables_to_plot[i], bins=20, color='black', alpha=0.7)\n",
    "    ax.set_title(variable_names[i])\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2229710-57a5-418c-bc46-1fce1a49eae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d575b49-a2f4-4799-933b-09b8d380aed8",
   "metadata": {},
   "source": [
    "## Start of Agent based model development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8994723-c14d-4e59-b77b-0f27dafde97b",
   "metadata": {},
   "source": [
    "### Defining the Household Agent Class\n",
    "This critical cell defines the Household class, representing individual agents in the simulation. Each household agent possesses attributes such as income, race, education level, latent variables, behavior scores, and price sensitivity. The class includes methods for:\n",
    "\n",
    "Initializing Communication Methods and Social Networks: Determines how agents interact based on social capital.\n",
    "Calculating Similarity and Influence Weights: Assesses how agents influence each other's behavior.\n",
    "Updating Latent Variables and Behavior Scores: Simulates the dynamic evolution of agent attributes over time.\n",
    "Determining Adoption of GSI Practices: Applies thresholds and price sensitivities to decide whether an agent adopts specific GSI practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d420552a-9816-4e81-a6ce-d0f1161eb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math  # Added for the sigmoid function\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from mesa import Agent, Model\n",
    "from mesa.time import RandomActivation\n",
    "from mesa.space import MultiGrid\n",
    "from mesa.datacollection import DataCollector\n",
    "\n",
    "# Define the indices for household_data\n",
    "household_data_indices = {\n",
    "    'tract_id': 0,                                    # Tract GEOID\n",
    "    'GEOID': 1,                                       # Parcel GEOID (Optional, same as tract_id)\n",
    "    'attitude': 2,                                    # Latent variable: Attitude\n",
    "    'awareness': 3,                                   # Latent variable: Awareness\n",
    "    'social_capital': 4,                              # Latent variable: Social Capital\n",
    "    'personal_norms': 5,                              # Latent variable: Personal Norms\n",
    "    'income': 6,                                      # Household income\n",
    "    'race': 7,                                        # Race of the household\n",
    "    'adv_degree': 8,                                  # Advanced degree status\n",
    "    'structural_behavior': 9,                         # Behavior score for structural practices\n",
    "    'raincatching_behavior': 10,                      # Behavior score for rain-catching practices\n",
    "    'nonstructural_behavior': 11,                     # Behavior score for non-structural practices\n",
    "    'total_behavior': 12,                             # Total behavior score across all practices\n",
    "    'structural_practices_potential': 13,             # Potential score for structural practices\n",
    "    'rain_catching_potential': 14,                    # Potential score for rain-catching practices\n",
    "    'B_Area': 15,                                     # Building area\n",
    "    'P_Area': 16,                                     # Parcel shape area\n",
    "    'structural_behavior_threshold': 17,              # Threshold for structural practices behavior\n",
    "    'raincatching_behavior_threshold': 18,            # Threshold for rain-catching practices behavior\n",
    "    'nonstructural_behavior_threshold': 19,           # Threshold for non-structural behavior\n",
    "    'total_behavior_threshold': 20,                   # Threshold for total behavior score\n",
    "    'rain_barrel_score': 21,                          # Sub-score for rain barrel\n",
    "    'cistern_score': 22,                              # Sub-score for cistern\n",
    "    'rain_garden_score': 23,                          # Sub-score for rain garden\n",
    "    'yard_slope_change_score': 24,                    # Sub-score for yard slope change\n",
    "    'yard_vegetation_change_score': 25,               # Sub-score for yard vegetation change\n",
    "    'reducing_irrigation_water_score': 26,            # Sub-score for reducing irrigation water\n",
    "    'price_sensitivity_rain_barrel': 27,              # Price sensitivity for rain barrel\n",
    "    'price_sensitivity_cistern': 28,                  # Price sensitivity for cistern\n",
    "    'price_sensitivity_rain_garden': 29,              # Price sensitivity for rain garden\n",
    "    'price_sensitivity_yard_vegetation_change': 30,   # Price sensitivity for yard vegetation change\n",
    "    'price_sensitivity_reducing_irrigation_water': 31,# Price sensitivity for reducing irrigation water\n",
    "    'price_sensitivity_yard_slope_change': 32         # Price sensitivity for yard slope change\n",
    "}\n",
    "\n",
    "class Household(Agent):\n",
    "    # Influence weights based on social influence literature\n",
    "    W_SIMILARITY = 0.3\n",
    "    W_COMMUNICATION = 0.1\n",
    "    W_VISUAL = 0.3\n",
    "    W_SPATIAL = 0.3\n",
    "\n",
    "    # Parameters for limiting influence over time\n",
    "    MAX_INFLUENCE_DECAY = 1  # Maximum decay factor to limit influence\n",
    "\n",
    "    def __init__(self, unique_id, model, household_data):\n",
    "        \"\"\"\n",
    "        Initialize a Household agent with its unique ID and various attributes from household_data.\n",
    "        \"\"\"\n",
    "        super().__init__(unique_id, model)\n",
    "\n",
    "        # Unpack household data\n",
    "        self.tract_id = household_data[household_data_indices['tract_id']]  # GEOID\n",
    "        self.income = household_data[household_data_indices['income']]      # Income\n",
    "        self.race = household_data[household_data_indices['race']]          # Race\n",
    "        self.adv_degree = household_data[household_data_indices['adv_degree']]  # Advanced Degree\n",
    "\n",
    "        # Initialize potential scores\n",
    "        self.structural_practices_potential = household_data[household_data_indices['structural_practices_potential']]\n",
    "        self.rain_catching_potential = household_data[household_data_indices['rain_catching_potential']]\n",
    "\n",
    "        # Initialize price sensitivity values for each practice\n",
    "        self.price_sensitivity = {\n",
    "            'rain_barrel': household_data[household_data_indices['price_sensitivity_rain_barrel']],\n",
    "            'cistern': household_data[household_data_indices['price_sensitivity_cistern']],\n",
    "            'rain_garden': household_data[household_data_indices['price_sensitivity_rain_garden']],\n",
    "            'yard_vegetation_change': household_data[household_data_indices['price_sensitivity_yard_vegetation_change']],\n",
    "            'reducing_irrigation_water': household_data[household_data_indices['price_sensitivity_reducing_irrigation_water']],\n",
    "            'yard_slope_change': household_data[household_data_indices['price_sensitivity_yard_slope_change']]\n",
    "        }\n",
    "\n",
    "        # Initialize adoption thresholds for each behavior\n",
    "        self.thresholds = {\n",
    "            'structural': household_data[household_data_indices['structural_behavior_threshold']],\n",
    "            'raincatching': household_data[household_data_indices['raincatching_behavior_threshold']],\n",
    "            'nonstructural': household_data[household_data_indices['nonstructural_behavior_threshold']]\n",
    "        }\n",
    "\n",
    "        # Unpack initial behavior scores\n",
    "        self.behaviors = {\n",
    "            'structural': household_data[household_data_indices['structural_behavior']],\n",
    "            'raincatching': household_data[household_data_indices['raincatching_behavior']],\n",
    "            'nonstructural': household_data[household_data_indices['nonstructural_behavior']]\n",
    "        }\n",
    "\n",
    "        # Initialize behavioral adoption flags\n",
    "        self.adoption_flags = {\n",
    "            'structural': False,\n",
    "            'raincatching': False,\n",
    "            'nonstructural': False\n",
    "        }\n",
    "\n",
    "        # Initialize Plan-to-Action adoption flags\n",
    "        self.plan_to_action_adoption_flags = {\n",
    "            'structural': False,\n",
    "            'raincatching': False\n",
    "        }\n",
    "\n",
    "        # Define importance coefficients for rain-catching and structural behavior subcategories\n",
    "        self.rain_catching_importance = {\n",
    "            \"rain_barrel\": 2,\n",
    "            \"cistern\": 8,\n",
    "            \"rain_garden\": 15\n",
    "        }\n",
    "        self.structural_practices_importance = {\n",
    "            \"yard_slope_change\": 2,\n",
    "            \"yard_vegetation_change\": 2,\n",
    "            \"reducing_irrigation_water\": 1\n",
    "        }\n",
    "\n",
    "        # Calculate total importance for rain-catching and structural practices\n",
    "        self.total_rain_catching_importance = sum(self.rain_catching_importance.values())\n",
    "        self.total_structural_practices_importance = sum(self.structural_practices_importance.values())\n",
    "\n",
    "        # Initialize latent variables history to store the history of latent variables and behaviors\n",
    "        # Separate latent variables for each behavior\n",
    "        self.latent_variables_history = {\n",
    "            'attitude': {\n",
    "                'structural': [household_data[household_data_indices['attitude']]],\n",
    "                'raincatching': [household_data[household_data_indices['attitude']]],\n",
    "                'nonstructural': [household_data[household_data_indices['attitude']]]\n",
    "            },\n",
    "            'awareness': {\n",
    "                'structural': [household_data[household_data_indices['awareness']]],\n",
    "                'raincatching': [household_data[household_data_indices['awareness']]],\n",
    "                'nonstructural': [household_data[household_data_indices['awareness']]]\n",
    "            },\n",
    "            'personal_norms': {\n",
    "                'structural': [household_data[household_data_indices['personal_norms']]],\n",
    "                'raincatching': [household_data[household_data_indices['personal_norms']]],\n",
    "                'nonstructural': [household_data[household_data_indices['personal_norms']]]\n",
    "            },\n",
    "            'social_capital': [household_data[household_data_indices['social_capital']]],\n",
    "            'behavior': {\n",
    "                'structural': [self.behaviors['structural']],\n",
    "                'raincatching': [self.behaviors['raincatching']],\n",
    "                'nonstructural': [self.behaviors['nonstructural']]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Initialize adoption flags history\n",
    "        self.adoption_flags_history = {\n",
    "            'structural': [self.adoption_flags['structural']],\n",
    "            'raincatching': [self.adoption_flags['raincatching']],\n",
    "            'nonstructural': [self.adoption_flags['nonstructural']]\n",
    "        }\n",
    "\n",
    "        # Initialize SEM coefficients (ensure these match your SEM model)\n",
    "        self.SEM_COEFFICIENTS = {\n",
    "            # Structural Behavior Influence Coefficients\n",
    "            'delta_awareness_structural': [-2.203, 1.89],\n",
    "            'delta_attitude_structural': [0.4964],  # [Coeff for social capital, Coeff for structural behavior change]\n",
    "            'delta_personal_norms_structural': [-0.6296, 0.5337, 3.094],  # [Coeff for social capital, awareness change, attitude change]\n",
    "            'delta_structural_behavior': [-0.2984, 6.73, -0.1442],  # [Coeff for awareness change, attitude change, personal norms change]\n",
    "\n",
    "            # Raincatching Behavior Influence Coefficients\n",
    "            'delta_awareness_rain': -1.251,\n",
    "            'delta_attitude_rain': 0.5042,\n",
    "            'delta_personal_norms_rain': [-0.5878, 0.5827, 3.165],\n",
    "            'delta_rain_behavior': [-0.4172, 4.84, -0.1972],\n",
    "\n",
    "            # Nonstructural Behavior Influence Coefficients\n",
    "            'delta_awareness_nonstructural': -1.073,\n",
    "            'delta_attitude_nonstructural': 0.5021,\n",
    "            'delta_personal_norms_nonstructural': [-0.5215, 0.549, 2.743],\n",
    "            'delta_nonstructural_behavior': [0.3297, 1.269, 0.2957]\n",
    "        }\n",
    "\n",
    "        # Initialize communication method and social network\n",
    "        # These will be initialized **after** the agent is placed on the grid\n",
    "        self.contact_method = None\n",
    "        self.social_contacts = []\n",
    "\n",
    "        # Initialize attribute to store previous social capital\n",
    "        self.prev_social_capital = self.latent_variables_history['social_capital'][-1]\n",
    "\n",
    "        # Initialize network and neighborhood influence rates (to be assigned from the model)\n",
    "        self.network_influence_rate = 0.0\n",
    "        self.neighborhood_influence_rate = 0.0\n",
    "\n",
    "    def initialize_communication_method(self):\n",
    "        \"\"\"\n",
    "        Assign communication method based on social capital.\n",
    "        Ensures that social_capital is non-negative.\n",
    "        \"\"\"\n",
    "        # Assign communication method based on social capital\n",
    "        current_social_capital = self.latent_variables_history['social_capital'][-1]\n",
    "        if current_social_capital > -1:\n",
    "            self.contact_method = 'in_person'\n",
    "        elif current_social_capital > -2:\n",
    "            self.contact_method = 'social_media'\n",
    "        else:\n",
    "            self.contact_method = 'online_forum'  # Example of an additional method\n",
    "\n",
    "    def initialize_social_network(self):\n",
    "        \"\"\"Initialize social contacts based on social capital and proximity.\"\"\"\n",
    "        # Access the tract-specific grid\n",
    "        tract_grid = self.model.grids[self.tract_id]\n",
    "\n",
    "        # Get all neighbors within a certain radius (e.g., radius=2)\n",
    "        potential_contacts = tract_grid.get_neighbors(self.pos, moore=True, radius=3, include_center=False)\n",
    "\n",
    "        # Retrieve current social capital\n",
    "        current_social_capital = self.latent_variables_history['social_capital'][-1]\n",
    "\n",
    "        # Define network size based on social capital tiers (2 to 15)\n",
    "        if current_social_capital > 1:\n",
    "            desired_network_size = random.randint(9, 12)  # Very large network\n",
    "        elif current_social_capital > -1:\n",
    "            desired_network_size = random.randint(6, 9)   # Moderate network\n",
    "        else:\n",
    "            desired_network_size = random.randint(2, 6)   # Small network\n",
    "\n",
    "        # Ensure network_size does not exceed potential contacts\n",
    "        actual_network_size = min(desired_network_size, len(potential_contacts))\n",
    "\n",
    "        # Randomly select contacts from potential contacts\n",
    "        self.social_contacts = random.sample(potential_contacts, actual_network_size) if actual_network_size > 0 else []\n",
    "\n",
    "    def calculate_similarity(self, neighbor, behavior):\n",
    "        \"\"\"\n",
    "        Calculate the similarity between two agents based on their latent variables for a specific behavior.\n",
    "        Returns a similarity score between 0 and 1.\n",
    "        \"\"\"\n",
    "        latent_vars_agent = np.array([\n",
    "            self.latent_variables_history['attitude'][behavior][-1],\n",
    "            self.latent_variables_history['awareness'][behavior][-1],\n",
    "            self.latent_variables_history['personal_norms'][behavior][-1],\n",
    "            self.latent_variables_history['social_capital'][-1]\n",
    "        ])\n",
    "        latent_vars_neighbor = np.array([\n",
    "            neighbor.latent_variables_history['attitude'][behavior][-1],\n",
    "            neighbor.latent_variables_history['awareness'][behavior][-1],\n",
    "            neighbor.latent_variables_history['personal_norms'][behavior][-1],\n",
    "            neighbor.latent_variables_history['social_capital'][-1]\n",
    "        ])\n",
    "\n",
    "        # Standardize latent variables (z-score normalization)\n",
    "        all_latent_vars = np.vstack((latent_vars_agent, latent_vars_neighbor))\n",
    "        means = np.mean(all_latent_vars, axis=0)\n",
    "        stds = np.std(all_latent_vars, axis=0) + 1e-6  # Add a small value to prevent division by zero\n",
    "\n",
    "        standardized_agent = (latent_vars_agent - means) / stds\n",
    "        standardized_neighbor = (latent_vars_neighbor - means) / stds\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = np.dot(standardized_agent, standardized_neighbor) / (np.linalg.norm(standardized_agent) * np.linalg.norm(standardized_neighbor))\n",
    "        similarity = (similarity + 1) / 2  # Adjust similarity to be between 0 and 1\n",
    "\n",
    "        return similarity\n",
    "\n",
    "    def calculate_spatial_weight(self, agent_position, neighbor_position):\n",
    "        \"\"\"\n",
    "        Calculate spatial influence weight based on distance.\n",
    "        Returns a weight between 0 and 1.\n",
    "        \"\"\"\n",
    "        distance = np.sqrt((agent_position[0] - neighbor_position[0]) ** 2 + (agent_position[1] - neighbor_position[1]) ** 2)\n",
    "        max_distance = np.sqrt((self.model.grids[self.tract_id].width - 1) ** 2 + (self.model.grids[self.tract_id].height - 1) ** 2)\n",
    "        return 1 - (distance / max_distance) if max_distance > 0 else 0\n",
    "\n",
    "    def calculate_visual_influence(self, neighbor, behavior):\n",
    "        \"\"\"Calculate visual influence between self and a neighbor for a specific behavior.\"\"\"\n",
    "        return 1 if neighbor.adoption_flags[behavior] else 0.0\n",
    "\n",
    "    def calculate_communication_influence(self, neighbor, behavior):\n",
    "        \"\"\"Calculate communication influence between self and a neighbor for a specific behavior.\"\"\"\n",
    "        if self.contact_method == 'in_person' and neighbor.contact_method == 'in_person':\n",
    "            return 1.0  # Maximum influence\n",
    "        elif self.contact_method == 'social_media' and neighbor.contact_method == 'social_media':\n",
    "            return 0.5  # Moderate influence\n",
    "        else:\n",
    "            return 0.0  # No influence\n",
    "\n",
    "    def determine_behavioral_cluster(self, behavior):\n",
    "        \"\"\"Determine the agent's cluster influence based on the proportion of nearby neighbors who have adopted the specific behavior.\"\"\"\n",
    "        num_neighbors = len(self.social_contacts)\n",
    "        if num_neighbors > 0:\n",
    "            num_adopters = sum(1 for neighbor in self.social_contacts if neighbor.adoption_flags[behavior])\n",
    "            behavioral_cluster_influence = num_adopters / num_neighbors  # Value between 0 and 1\n",
    "        else:\n",
    "            behavioral_cluster_influence = 0.0\n",
    "        return behavioral_cluster_influence\n",
    "\n",
    "    def update_latent_variables(self):\n",
    "        \"\"\"\n",
    "        Update the agent's latent variables based on interactions with neighbors and overall dynamics for each behavior.\n",
    "        Incorporates:\n",
    "        - Limit Influence Over Time\n",
    "        - Weighted Network Effects\n",
    "        - Reducing Influence Weight from Non-Adopters\n",
    "        \"\"\"\n",
    "        behaviors = ['structural', 'raincatching', 'nonstructural']\n",
    "\n",
    "        # Retrieve global adoption rates from the model\n",
    "        global_adoption_rates = {\n",
    "            behavior: self.model.get_global_adoption_rate(behavior) for behavior in behaviors\n",
    "        }\n",
    "\n",
    "        for behavior in behaviors:\n",
    "            # Retrieve previous latent variable values\n",
    "            prev_attitude = self.latent_variables_history['attitude'][behavior][-1]\n",
    "            prev_awareness = self.latent_variables_history['awareness'][behavior][-1]\n",
    "            prev_personal_norms = self.latent_variables_history['personal_norms'][behavior][-1]\n",
    "            # Retrieve previous behavior value\n",
    "            prev_behavior = self.latent_variables_history['behavior'][behavior][-1]\n",
    "\n",
    "            # Retrieve previous social capital before this step\n",
    "            prev_social_capital = self.prev_social_capital\n",
    "\n",
    "            # Define initial weights for self and neighbors' influence\n",
    "            self_weight = 0.85  \n",
    "            neighbor_weight = 1 - self_weight   \n",
    "\n",
    "            # Initialize weighted sums for each latent variable\n",
    "            weighted_sum = {\n",
    "                'attitude': 0.0,\n",
    "                'awareness': 0.0,\n",
    "                'personal_norms': 0.0\n",
    "                # Note: social_capital will be handled separately\n",
    "            }\n",
    "            total_interaction_weight = 0.0  # For normalizing the influence\n",
    "\n",
    "            # Calculate neighbor adoption rate for the specific behavior\n",
    "            if len(self.social_contacts) > 0:\n",
    "                adopting_neighbors = sum(1 for neighbor in self.social_contacts if neighbor.adoption_flags[behavior])\n",
    "                neighbor_adoption_rate = adopting_neighbors / len(self.social_contacts)\n",
    "            else:\n",
    "                neighbor_adoption_rate = 0.0\n",
    "\n",
    "            # Adjust influence weights based on neighbor adoption rate (Weighted Network Effects)\n",
    "            influence_scaling_factor = 1 + neighbor_adoption_rate\n",
    "\n",
    "            for neighbor in self.social_contacts:\n",
    "                # Determine if neighbor is an adopter\n",
    "                is_adopter = neighbor.adoption_flags[behavior]\n",
    "\n",
    "                # Adjust interaction weight based on adoption status\n",
    "                if is_adopter:\n",
    "                    adopter_weight = 1.0  # Full influence\n",
    "                else:\n",
    "                    adopter_weight = 0.3  # Minimal influence\n",
    "\n",
    "                # Calculate the per-neighbor influence components specific to the behavior\n",
    "                similarity = self.calculate_similarity(neighbor, behavior)  # Between 0 and 1\n",
    "                spatial_weight = self.calculate_spatial_weight(self.pos, neighbor.pos)  # Between 0 and 1\n",
    "                communication_influence = self.calculate_communication_influence(neighbor, behavior)  # Between 0 and 1\n",
    "                visual_influence = self.calculate_visual_influence(neighbor, behavior)  # Between 0 and 1\n",
    "\n",
    "                # Combine interaction components into an overall interaction weight using assigned weights\n",
    "                interaction_weight = (\n",
    "                    self.W_SIMILARITY * similarity +\n",
    "                    self.W_SPATIAL * spatial_weight +\n",
    "                    self.W_COMMUNICATION * communication_influence +\n",
    "                    self.W_VISUAL * visual_influence\n",
    "                ) * adopter_weight  # Scale influence based on adoption\n",
    "\n",
    "                # Apply the influence scaling factor (Weighted Network Effects)\n",
    "                interaction_weight *= influence_scaling_factor\n",
    "\n",
    "                # Apply weighted sum for each latent variable\n",
    "                weighted_sum['attitude'] += interaction_weight * neighbor.latent_variables_history['attitude'][behavior][-1]\n",
    "                weighted_sum['awareness'] += interaction_weight * neighbor.latent_variables_history['awareness'][behavior][-1]\n",
    "                weighted_sum['personal_norms'] += interaction_weight * neighbor.latent_variables_history['personal_norms'][behavior][-1]\n",
    "\n",
    "                # Accumulate total interaction weight\n",
    "                total_interaction_weight += interaction_weight\n",
    "\n",
    "            # Adjust influence decay based on global adoption rate\n",
    "            global_adoption_rate = global_adoption_rates[behavior]  # Between 0 and 1\n",
    "            influence_decay = 1 - (global_adoption_rate * self.MAX_INFLUENCE_DECAY)  # Scale decay\n",
    "            adjusted_neighbor_weight = neighbor_weight * influence_decay  # Reduce neighbor influence as adoption increases\n",
    "            adjusted_self_weight = 1 - adjusted_neighbor_weight  # Adjust self weight accordingly\n",
    "\n",
    "            # Normalize the influence of neighbors based on total interaction weight\n",
    "            if total_interaction_weight > 0:\n",
    "                weighted_sum['attitude'] /= total_interaction_weight\n",
    "                weighted_sum['awareness'] /= total_interaction_weight\n",
    "                weighted_sum['personal_norms'] /= total_interaction_weight\n",
    "\n",
    "                # Update each latent variable by blending self and neighbor influence with adjusted weights\n",
    "                new_attitude = (adjusted_self_weight * prev_attitude) + (adjusted_neighbor_weight * weighted_sum['attitude'])\n",
    "                new_awareness = (adjusted_self_weight * prev_awareness) + (adjusted_neighbor_weight * weighted_sum['awareness'])\n",
    "                new_personal_norms = (adjusted_self_weight * prev_personal_norms) + (adjusted_neighbor_weight * weighted_sum['personal_norms'])\n",
    "            else:\n",
    "                # Apply a small decay if no influence from neighbors\n",
    "                decay_factor = 0.001\n",
    "                new_attitude = prev_attitude - decay_factor\n",
    "                new_awareness = prev_awareness - decay_factor\n",
    "                new_personal_norms = prev_personal_norms - decay_factor\n",
    "\n",
    "            # --- Apply the SEM Pathways and Feedback Loop for the specific behavior ---\n",
    "            if behavior == 'structural':\n",
    "                delta_awareness = (self.SEM_COEFFICIENTS['delta_awareness_structural'][0] *\n",
    "                                    (self.latent_variables_history['social_capital'][-1] - prev_social_capital)) + \\\n",
    "                                   (self.SEM_COEFFICIENTS['delta_awareness_structural'][1] *\n",
    "                                    (new_attitude - prev_attitude))\n",
    "                delta_attitude = (\n",
    "                    self.SEM_COEFFICIENTS['delta_attitude_structural'][0] *\n",
    "                    (self.latent_variables_history['social_capital'][-1] - prev_social_capital)\n",
    "                )\n",
    "                delta_personal_norms = (\n",
    "                    self.SEM_COEFFICIENTS['delta_personal_norms_structural'][0] *\n",
    "                    (self.latent_variables_history['social_capital'][-1] - prev_social_capital)\n",
    "                    + self.SEM_COEFFICIENTS['delta_personal_norms_structural'][1] *\n",
    "                    (new_awareness - prev_awareness)\n",
    "                    + self.SEM_COEFFICIENTS['delta_personal_norms_structural'][2] *\n",
    "                    (new_attitude - prev_attitude)\n",
    "                )\n",
    "                delta_behavior = (\n",
    "                    self.SEM_COEFFICIENTS['delta_structural_behavior'][0] *\n",
    "                    (new_awareness - prev_awareness)\n",
    "                    + self.SEM_COEFFICIENTS['delta_structural_behavior'][1] *\n",
    "                    (new_attitude - prev_attitude)\n",
    "                    + self.SEM_COEFFICIENTS['delta_structural_behavior'][2] *\n",
    "                    (new_personal_norms - prev_personal_norms)\n",
    "                )\n",
    "            elif behavior == 'raincatching':\n",
    "                delta_awareness = self.SEM_COEFFICIENTS['delta_awareness_rain'] * (self.latent_variables_history['social_capital'][-1] - prev_social_capital)\n",
    "                delta_attitude = self.SEM_COEFFICIENTS['delta_attitude_rain'] * (self.latent_variables_history['social_capital'][-1] - prev_social_capital)\n",
    "                delta_personal_norms = (\n",
    "                    self.SEM_COEFFICIENTS['delta_personal_norms_rain'][0] *\n",
    "                    (self.latent_variables_history['social_capital'][-1] - prev_social_capital)\n",
    "                    + self.SEM_COEFFICIENTS['delta_personal_norms_rain'][1] *\n",
    "                    (new_awareness - prev_awareness)\n",
    "                    + self.SEM_COEFFICIENTS['delta_personal_norms_rain'][2] *\n",
    "                    (new_attitude - prev_attitude)\n",
    "                )\n",
    "                delta_behavior = (\n",
    "                    self.SEM_COEFFICIENTS['delta_rain_behavior'][0] *\n",
    "                    (new_awareness - prev_awareness)\n",
    "                    + self.SEM_COEFFICIENTS['delta_rain_behavior'][1] *\n",
    "                    (new_personal_norms - prev_personal_norms)\n",
    "                    + self.SEM_COEFFICIENTS['delta_rain_behavior'][2] *\n",
    "                    (new_attitude - prev_attitude)\n",
    "                )\n",
    "            elif behavior == 'nonstructural':\n",
    "                delta_awareness = self.SEM_COEFFICIENTS['delta_awareness_nonstructural'] * (self.latent_variables_history['social_capital'][-1] - prev_social_capital)\n",
    "                delta_attitude = self.SEM_COEFFICIENTS['delta_attitude_nonstructural'] * (self.latent_variables_history['social_capital'][-1] - prev_social_capital)\n",
    "                delta_personal_norms = (\n",
    "                    self.SEM_COEFFICIENTS['delta_personal_norms_nonstructural'][0] *\n",
    "                    (self.latent_variables_history['social_capital'][-1] - prev_social_capital)\n",
    "                    + self.SEM_COEFFICIENTS['delta_personal_norms_nonstructural'][1] *\n",
    "                    (new_awareness - prev_awareness)\n",
    "                    + self.SEM_COEFFICIENTS['delta_personal_norms_nonstructural'][2] *\n",
    "                    (new_attitude - prev_attitude)\n",
    "                )\n",
    "                delta_behavior = (\n",
    "                    self.SEM_COEFFICIENTS['delta_nonstructural_behavior'][0] *\n",
    "                    (new_awareness - prev_awareness)\n",
    "                    + self.SEM_COEFFICIENTS['delta_nonstructural_behavior'][1] *\n",
    "                    (new_attitude - prev_attitude)\n",
    "                    + self.SEM_COEFFICIENTS['delta_nonstructural_behavior'][2] *\n",
    "                    (new_personal_norms - prev_personal_norms)\n",
    "                )\n",
    "            else:\n",
    "                delta_awareness = 0\n",
    "                delta_attitude = 0\n",
    "                delta_personal_norms = 0\n",
    "                delta_behavior = 0\n",
    "            self.SEM_attitude_factor = 0.4\n",
    "            self.SEM_awareness_factor = 0.4\n",
    "            self.SEM_personal_norms_factor = 0.4\n",
    "            # Update latent variables with SEM influences\n",
    "            new_attitude += self.SEM_attitude_factor * delta_attitude\n",
    "            new_awareness += self.SEM_awareness_factor * delta_awareness\n",
    "            new_personal_norms += self.SEM_personal_norms_factor * delta_personal_norms\n",
    "\n",
    "            # Update behavior scores with SEM influences\n",
    "            if behavior == 'structural':\n",
    "                self.behaviors['structural'] += 0 * delta_behavior  # Apply a fraction of the delta\n",
    "            elif behavior == 'raincatching':\n",
    "                self.behaviors['raincatching'] += 0 * delta_behavior\n",
    "            elif behavior == 'nonstructural':\n",
    "                self.behaviors['nonstructural'] += 0 * delta_behavior\n",
    "\n",
    "            # Introduce reduced stochasticity to maintain diversity\n",
    "            new_attitude += random.uniform(-0.1, 0.1)  # Reduced range\n",
    "            new_awareness += random.uniform(-0.1, 0.1)\n",
    "            new_personal_norms += random.uniform(-0.1, 0.1)\n",
    "\n",
    "            # Update behavior scores based on new latent variables\n",
    "            self.calculate_behavior_scores(behavior)\n",
    "\n",
    "            # Append updated latent variables to history\n",
    "            self.latent_variables_history['attitude'][behavior].append(new_attitude)\n",
    "            self.latent_variables_history['awareness'][behavior].append(new_awareness)\n",
    "            self.latent_variables_history['personal_norms'][behavior].append(new_personal_norms)\n",
    "            self.latent_variables_history['behavior'][behavior].append(self.behaviors[behavior])\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "\n",
    "    def update_social_capital(self):\n",
    "        \"\"\"\n",
    "        Update the agent's social capital based on network and neighborhood behavioral adoption.\n",
    "        - Influence from network adopters: Always active, regardless of agent's own adoption.\n",
    "        - Influence from neighborhood adopters: Only active if the agent has adopted.\n",
    "        Positive impact only; no negative impact when neighbors do not adopt.\n",
    "        Includes decay over time to model natural social capital loss.\n",
    "        \"\"\"\n",
    "        # Retrieve previous social capital\n",
    "        prev_social_capital = self.latent_variables_history['social_capital'][-1]\n",
    "\n",
    "        # Initialize social capital change\n",
    "        social_capital_change = 0.0\n",
    "\n",
    "        # --- Influence from Network Adopters ---\n",
    "        # Count number of network adopters\n",
    "        network_adopters = sum(\n",
    "            1 for neighbor in self.social_contacts if any(neighbor.adoption_flags.values())\n",
    "        )\n",
    "        network_adoption_rate = network_adopters / len(self.social_contacts) if self.social_contacts else 0\n",
    "\n",
    "        # Calculate social capital change from network adopters\n",
    "        network_change = self.network_influence_rate * self.sigmoid(network_adoption_rate * self.K)\n",
    "        social_capital_change += network_change\n",
    "\n",
    "        # --- Combined Influence from Neighborhood and Self-Adopters ---\n",
    "        if any(self.adoption_flags.values()):\n",
    "            # Fetch neighborhood adopters\n",
    "            neighborhood_radius = 2\n",
    "            tract_grid = self.model.grids[self.tract_id]\n",
    "            neighborhood_agents = tract_grid.get_neighbors(\n",
    "                self.pos, moore=True, radius=neighborhood_radius, include_center=False\n",
    "            )\n",
    "\n",
    "            # Count number of neighborhood adopters\n",
    "            neighborhood_adopters = sum(\n",
    "                1 for neighbor in neighborhood_agents if any(neighbor.adoption_flags.values())\n",
    "            )\n",
    "            neighborhood_adoption_rate = neighborhood_adopters / len(neighborhood_agents) if neighborhood_agents else 0\n",
    "\n",
    "            # Calculate additional social capital change from neighborhood adopters\n",
    "            neighborhood_change = self.neighborhood_influence_rate * self.sigmoid(neighborhood_adoption_rate * self.K)\n",
    "            social_capital_change += neighborhood_change\n",
    "\n",
    "        # --- Apply Decay ---\n",
    "        social_capital_decay = self.SC_DECAY_RATE * prev_social_capital\n",
    "        social_capital_change -= social_capital_decay\n",
    "\n",
    "        # --- Introduce Minimal Stochasticity ---\n",
    "        stochastic_factor = random.uniform(-0.002, 0.002)  # Reduced range\n",
    "        social_capital_change += stochastic_factor\n",
    "\n",
    "        # --- Update Social Capital ---\n",
    "        new_social_capital = prev_social_capital + social_capital_change\n",
    "        new_social_capital = max(-4.0, min(new_social_capital, 4.0))  # Clamp within bounds\n",
    "\n",
    "        # --- Append to History ---\n",
    "        self.latent_variables_history['social_capital'].append(new_social_capital)\n",
    "        self.prev_social_capital = new_social_capital\n",
    "\n",
    "        # --- Update Communication Method and Social Network ---\n",
    "        self.initialize_communication_method()\n",
    "        self.adjust_social_network()\n",
    "\n",
    "    def adjust_social_network(self):\n",
    "        \"\"\"\n",
    "        Adjust the social network size based on updated social capital.\n",
    "        Network size ranges dynamically from 2 to 15 based on social capital.\n",
    "        \"\"\"\n",
    "        # Access the tract-specific grid\n",
    "        tract_grid = self.model.grids[self.tract_id]\n",
    "\n",
    "        # Get all neighbors within a certain radius (e.g., radius=2)\n",
    "        potential_contacts = tract_grid.get_neighbors(self.pos, moore=True, radius=3, include_center=False)\n",
    "\n",
    "        # Define desired network size based on updated social capital tiers (2 to 15)\n",
    "        current_social_capital = self.latent_variables_history['social_capital'][-1]\n",
    "\n",
    "        if current_social_capital > 1:\n",
    "            desired_network_size = random.randint(9, 12)  # Very large network\n",
    "        elif current_social_capital > -1:\n",
    "            desired_network_size = random.randint(6, 9)   # Moderate network\n",
    "        else:\n",
    "            desired_network_size = random.randint(2, 6)   # Small network\n",
    "\n",
    "        # Ensure desired_network_size does not exceed potential contacts\n",
    "        actual_network_size = min(desired_network_size, len(potential_contacts))\n",
    "\n",
    "        # Update social contacts\n",
    "        if len(self.social_contacts) < actual_network_size:\n",
    "            # Add new contacts\n",
    "            available_contacts = list(set(potential_contacts) - set(self.social_contacts))\n",
    "            num_new_contacts = actual_network_size - len(self.social_contacts)\n",
    "            new_contacts = random.sample(available_contacts, num_new_contacts) if num_new_contacts > 0 else []\n",
    "            self.social_contacts.extend(new_contacts)\n",
    "        elif len(self.social_contacts) > actual_network_size:\n",
    "            # Remove existing contacts\n",
    "            self.social_contacts = random.sample(self.social_contacts, actual_network_size)\n",
    "\n",
    "        # Note: If the network size remains the same, no changes are made\n",
    "\n",
    "    def calculate_behavior_scores(self, behavior):\n",
    "        \"\"\"\n",
    "        Recalculate behavior scores using updated latent variables.\n",
    "        Incorporate SEM dynamics into the behavior score updates.\n",
    "        \"\"\"\n",
    "        if behavior == 'structural':\n",
    "            self.behaviors['structural'] = (\n",
    "                1.51  # Regression constant\n",
    "                + (-0.45 if self.race == \"Non-White\" else 0)\n",
    "                + (\n",
    "                    0.08 if self.income < 20000\n",
    "                    else (-0.17 if self.income < 50000 else 0.15)\n",
    "                )\n",
    "                + (-0.16 if self.adv_degree == \"No Adv Degree\" else 0)\n",
    "                + 0.37 * self.latent_variables_history['personal_norms']['structural'][-1]\n",
    "                + 0.23 * self.latent_variables_history['social_capital'][-1]\n",
    "                - 0.02 * self.latent_variables_history['attitude']['structural'][-1]\n",
    "                + 0.02 * self.latent_variables_history['awareness']['structural'][-1]\n",
    "            ) * self.structural_practices_potential\n",
    "\n",
    "        elif behavior == 'raincatching':\n",
    "            self.behaviors['raincatching'] = (\n",
    "                0.39  # Regression constant\n",
    "                + (-0.18 if self.race == \"Non-White\" else 0)\n",
    "                + (-0.05 if self.adv_degree == \"No Adv Degree\" else 0)\n",
    "                + (\n",
    "                    0.03  if self.income < 20000\n",
    "                    else (0.02 if self.income < 50000 else 0.13)\n",
    "                )\n",
    "                + 0.22 * self.latent_variables_history['personal_norms']['raincatching'][-1]\n",
    "                + 0.07 * self.latent_variables_history['social_capital'][-1]\n",
    "                + 0.05 * self.latent_variables_history['attitude']['raincatching'][-1]\n",
    "                - 0.02 * self.latent_variables_history['awareness']['raincatching'][-1]\n",
    "            ) * self.rain_catching_potential\n",
    "\n",
    "        elif behavior == 'nonstructural':\n",
    "            self.behaviors['nonstructural'] = (\n",
    "                4.45  # Regression constant\n",
    "                + (-0.22 if self.race == \"Non-White\" else 0)\n",
    "                + (0.11 if self.adv_degree == \"No Adv Degree\" else 0)\n",
    "                + (\n",
    "                    0.13\n",
    "                    if self.income < 20000\n",
    "                    else (-0.21 if self.income < 50000 else (-0.24 if self.income < 80000 else 0))\n",
    "                )\n",
    "                + 0.51 * self.latent_variables_history['personal_norms']['nonstructural'][-1]\n",
    "                + 0.11 * self.latent_variables_history['social_capital'][-1]\n",
    "                + 0.03 * self.latent_variables_history['attitude']['nonstructural'][-1]\n",
    "                + 0.14 * self.latent_variables_history['awareness']['nonstructural'][-1]\n",
    "            )\n",
    "\n",
    "    def apply_price_sensitivity_barriers(self):\n",
    "        \"\"\"\n",
    "        Apply price sensitivity as barriers to behavior scores right before threshold comparison.\n",
    "        This function returns the price-sensitive behavior scores for adoption comparison.\n",
    "        \"\"\"\n",
    "        structural_behavior_priceSen = 0\n",
    "\n",
    "        yard_slope_change_score = (\n",
    "            self.behaviors['structural'] * self.structural_practices_importance['yard_slope_change']\n",
    "        ) / self.total_structural_practices_importance\n",
    "        yard_vegetation_change_score = (\n",
    "            self.behaviors['structural'] * self.structural_practices_importance['yard_vegetation_change']\n",
    "        ) / self.total_structural_practices_importance\n",
    "        reducing_irrigation_water_score = (\n",
    "            self.behaviors['structural'] * self.structural_practices_importance['reducing_irrigation_water']\n",
    "        ) / self.total_structural_practices_importance\n",
    "\n",
    "        if self.price_sensitivity.get('yard_slope_change', 1) == 0:\n",
    "            structural_behavior_priceSen += yard_slope_change_score\n",
    "        if self.price_sensitivity.get('yard_vegetation_change', 1) == 0:\n",
    "            structural_behavior_priceSen += yard_vegetation_change_score\n",
    "        if self.price_sensitivity.get('reducing_irrigation_water', 1) == 0:\n",
    "            structural_behavior_priceSen += reducing_irrigation_water_score\n",
    "\n",
    "        raincatching_behavior_priceSen = 0\n",
    "\n",
    "        rain_barrel_score = (\n",
    "            self.behaviors['raincatching'] * self.rain_catching_importance['rain_barrel']\n",
    "        ) / self.total_rain_catching_importance\n",
    "        cistern_score = (\n",
    "            self.behaviors['raincatching'] * self.rain_catching_importance['cistern']\n",
    "        ) / self.total_rain_catching_importance\n",
    "        rain_garden_score = (\n",
    "            self.behaviors['raincatching'] * self.rain_catching_importance['rain_garden']\n",
    "        ) / self.total_rain_catching_importance\n",
    "\n",
    "        if self.price_sensitivity.get('rain_barrel', 1) == 0:\n",
    "            raincatching_behavior_priceSen += rain_barrel_score\n",
    "        if self.price_sensitivity.get('cistern', 1) == 0:\n",
    "            raincatching_behavior_priceSen += cistern_score\n",
    "        if self.price_sensitivity.get('rain_garden', 1) == 0:\n",
    "            raincatching_behavior_priceSen += rain_garden_score\n",
    "\n",
    "        return structural_behavior_priceSen, raincatching_behavior_priceSen\n",
    "\n",
    "    def determine_adoption(self):\n",
    "        \"\"\"\n",
    "        Determine if the agent adopts GSI practices based on their behavior scores and thresholds.\n",
    "        \"\"\"\n",
    "        structural_behavior_priceSen, raincatching_behavior_priceSen = self.apply_price_sensitivity_barriers()\n",
    "\n",
    "        if not self.adoption_flags['structural'] and structural_behavior_priceSen > self.thresholds['structural']:\n",
    "            self.adoption_flags['structural'] = True\n",
    "            self.model.adoption_counts[self.tract_id]['structural'] += 1\n",
    "\n",
    "        if not self.adoption_flags['raincatching'] and raincatching_behavior_priceSen > self.thresholds['raincatching']:\n",
    "            self.adoption_flags['raincatching'] = True\n",
    "            self.model.adoption_counts[self.tract_id]['raincatching'] += 1\n",
    "\n",
    "        if not self.adoption_flags['nonstructural'] and self.behaviors['nonstructural'] > self.thresholds['nonstructural']:\n",
    "            self.adoption_flags['nonstructural'] = True\n",
    "            self.model.adoption_counts[self.tract_id]['nonstructural'] += 1\n",
    "\n",
    "    def calculate_plan_to_action_scores(self):\n",
    "        \"\"\"\n",
    "        Calculate Plan to Action adoption scores for Structural and Raincatching actions directly\n",
    "        using hardcoded regression coefficients instead of accessing from a passed dictionary.\n",
    "\n",
    "        Returns:\n",
    "            dict: Regression adoption scores for 'structural' and 'raincatching'.\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "\n",
    "        # Structural behavior coefficients (hardcoded)\n",
    "        structural_intercept = 1.215\n",
    "        structural_nonwhite_coeff = 0.15\n",
    "        structural_no_adv_degree_coeff = -0.18\n",
    "        structural_very_low_income_coeff = -0.06\n",
    "        structural_low_income_coeff = 0.01\n",
    "        structural_median_income_coeff = 0.12\n",
    "        structural_personal_norms_coeff = 0.4\n",
    "        structural_attitude_coeff = 0.03\n",
    "        structural_awareness_coeff = -0.02\n",
    "        structural_social_capital_coeff = 0.13\n",
    "\n",
    "        # Raincatching behavior coefficients (hardcoded)\n",
    "        raincatching_intercept = 0.569\n",
    "        raincatching_nonwhite_coeff = 0.3\n",
    "        raincatching_no_adv_degree_coeff = -0.01\n",
    "        raincatching_very_low_income_coeff = -0.26\n",
    "        raincatching_low_income_coeff = 0.16\n",
    "        raincatching_median_income_coeff = -0.06\n",
    "        raincatching_personal_norms_coeff = 0.49\n",
    "        raincatching_attitude_coeff = 0.02\n",
    "        raincatching_awareness_coeff = -0.07\n",
    "        raincatching_social_capital_coeff = -0.07\n",
    "\n",
    "        # Structural behavior score calculation\n",
    "        structural_score = structural_intercept\n",
    "        if self.race == \"Non-White\":\n",
    "            structural_score += structural_nonwhite_coeff\n",
    "        if self.adv_degree == \"No Adv Degree\":\n",
    "            structural_score += structural_no_adv_degree_coeff\n",
    "        if self.income < 20000:\n",
    "            structural_score += structural_very_low_income_coeff\n",
    "        elif 20000 <= self.income < 50000:\n",
    "            structural_score += structural_low_income_coeff\n",
    "        elif 50000 <= self.income < 80000:\n",
    "            structural_score += structural_median_income_coeff\n",
    "\n",
    "        personal_norms_structural = self.latent_variables_history['personal_norms']['structural'][-1]\n",
    "        attitude_structural = self.latent_variables_history['attitude']['structural'][-1]\n",
    "        awareness_structural = self.latent_variables_history['awareness']['structural'][-1]\n",
    "        social_capital_structural = self.latent_variables_history['social_capital'][-1]\n",
    "\n",
    "        structural_score += (structural_personal_norms_coeff * personal_norms_structural +\n",
    "                             structural_attitude_coeff * attitude_structural +\n",
    "                             structural_awareness_coeff * awareness_structural +\n",
    "                             structural_social_capital_coeff * social_capital_structural) * self.structural_practices_potential\n",
    "\n",
    "        # Raincatching behavior score calculation\n",
    "        raincatching_score = raincatching_intercept\n",
    "        if self.race == \"Non-White\":\n",
    "            raincatching_score += raincatching_nonwhite_coeff\n",
    "        if self.adv_degree == \"No Adv Degree\":\n",
    "            raincatching_score += raincatching_no_adv_degree_coeff\n",
    "        if self.income < 20000:\n",
    "            raincatching_score += raincatching_very_low_income_coeff\n",
    "        elif 20000 <= self.income < 50000:\n",
    "            raincatching_score += raincatching_low_income_coeff\n",
    "        elif 50000 <= self.income < 80000:\n",
    "            raincatching_score += raincatching_median_income_coeff\n",
    "\n",
    "        personal_norms_raincatching = self.latent_variables_history['personal_norms']['raincatching'][-1]\n",
    "        attitude_raincatching = self.latent_variables_history['attitude']['raincatching'][-1]\n",
    "        awareness_raincatching = self.latent_variables_history['awareness']['raincatching'][-1]\n",
    "        social_capital_raincatching = self.latent_variables_history['social_capital'][-1]\n",
    "\n",
    "        raincatching_score += (raincatching_personal_norms_coeff * personal_norms_raincatching +\n",
    "                               raincatching_attitude_coeff * attitude_raincatching +\n",
    "                               raincatching_awareness_coeff * awareness_raincatching +\n",
    "                               raincatching_social_capital_coeff * social_capital_raincatching) * self.rain_catching_potential\n",
    "\n",
    "        # Return scores for both behaviors\n",
    "        scores['structural'] = structural_score\n",
    "        scores['raincatching'] = raincatching_score\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def apply_price_sensitivity_to_plan_to_action(self, plan_to_action_scores):\n",
    "        \"\"\"\n",
    "        Apply price sensitivity as barriers to plan-to-action scores right before threshold comparison.\n",
    "        This function returns the price-sensitive plan-to-action behavior scores for adoption comparison.\n",
    "\n",
    "        Parameters:\n",
    "            plan_to_action_scores (dict): Original plan-to-action scores.\n",
    "\n",
    "        Returns:\n",
    "            dict: Price-sensitive plan-to-action scores.\n",
    "        \"\"\"\n",
    "        # Initialize adjusted scores\n",
    "        adjusted_scores = {}\n",
    "\n",
    "        # Structural Plan-to-Action\n",
    "        structural_score = plan_to_action_scores.get('structural', 0)\n",
    "\n",
    "        # Calculate structural plan-to-action price sensitivity\n",
    "        yard_slope_change_score = (\n",
    "            self.behaviors['structural'] * self.structural_practices_importance['yard_slope_change']\n",
    "        ) / self.total_structural_practices_importance\n",
    "        yard_vegetation_change_score = (\n",
    "            self.behaviors['structural'] * self.structural_practices_importance['yard_vegetation_change']\n",
    "        ) / self.total_structural_practices_importance\n",
    "        reducing_irrigation_water_score = (\n",
    "            self.behaviors['structural'] * self.structural_practices_importance['reducing_irrigation_water']\n",
    "        ) / self.total_structural_practices_importance\n",
    "\n",
    "        structural_priceSen = 0\n",
    "        if self.price_sensitivity.get('yard_slope_change', 1) == 0:\n",
    "            structural_priceSen += yard_slope_change_score\n",
    "        if self.price_sensitivity.get('yard_vegetation_change', 1) == 0:\n",
    "            structural_priceSen += yard_vegetation_change_score\n",
    "        if self.price_sensitivity.get('reducing_irrigation_water', 1) == 0:\n",
    "            structural_priceSen += reducing_irrigation_water_score\n",
    "\n",
    "        # Adjust structural plan-to-action score\n",
    "        adjusted_structural_score = structural_score - structural_priceSen  # Adjust as per your model's logic\n",
    "\n",
    "        adjusted_scores['structural'] = max(adjusted_structural_score, 0)  # Ensure non-negative\n",
    "\n",
    "        # Raincatching Plan-to-Action\n",
    "        raincatching_score = plan_to_action_scores.get('raincatching', 0)\n",
    "\n",
    "        # Calculate raincatching plan-to-action price sensitivity\n",
    "        rain_barrel_score = (\n",
    "            self.behaviors['raincatching'] * self.rain_catching_importance['rain_barrel']\n",
    "        ) / self.total_rain_catching_importance\n",
    "        cistern_score = (\n",
    "            self.behaviors['raincatching'] * self.rain_catching_importance['cistern']\n",
    "        ) / self.total_rain_catching_importance\n",
    "        rain_garden_score = (\n",
    "            self.behaviors['raincatching'] * self.rain_catching_importance['rain_garden']\n",
    "        ) / self.total_rain_catching_importance\n",
    "\n",
    "        raincatching_priceSen = 0\n",
    "        if self.price_sensitivity.get('rain_barrel', 1) == 0:\n",
    "            raincatching_priceSen += rain_barrel_score\n",
    "        if self.price_sensitivity.get('cistern', 1) == 0:\n",
    "            raincatching_priceSen += cistern_score\n",
    "        if self.price_sensitivity.get('rain_garden', 1) == 0:\n",
    "            raincatching_priceSen += rain_garden_score\n",
    "\n",
    "        # Adjust raincatching plan-to-action score\n",
    "        adjusted_raincatching_score = raincatching_score - raincatching_priceSen  # Adjust as per your model's logic\n",
    "\n",
    "        adjusted_scores['raincatching'] = max(adjusted_raincatching_score, 0)  # Ensure non-negative\n",
    "\n",
    "        return adjusted_scores\n",
    "\n",
    "    def determine_plan_to_action_adoption(self, plan_to_action_scores):\n",
    "        \"\"\"\n",
    "        Determine if the agent adopts based on Plan to Action adoption scores.\n",
    "\n",
    "        Parameters:\n",
    "            plan_to_action_scores (dict): The calculated plan-to-action adoption scores.\n",
    "        \"\"\"\n",
    "        # Apply price sensitivity to plan-to-action scores\n",
    "        adjusted_plan_to_action_scores = self.apply_price_sensitivity_to_plan_to_action(plan_to_action_scores)\n",
    "\n",
    "        # Determine adoption for structural plan-to-action\n",
    "        if not self.plan_to_action_adoption_flags['structural'] and adjusted_plan_to_action_scores.get('structural', 0) > 1.63:\n",
    "            self.plan_to_action_adoption_flags['structural'] = True\n",
    "            self.model.adoption_counts[self.tract_id]['structural_plan_to_action'] += 1\n",
    "\n",
    "        # Determine adoption for raincatching plan-to-action\n",
    "        if not self.plan_to_action_adoption_flags['raincatching'] and adjusted_plan_to_action_scores.get('raincatching', 0) > 1.055:\n",
    "            self.plan_to_action_adoption_flags['raincatching'] = True\n",
    "            self.model.adoption_counts[self.tract_id]['raincatching_plan_to_action'] += 1\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Perform a single step of the agent's decision-making process.\n",
    "        \"\"\"\n",
    "        # Update social capital based on dual influence\n",
    "        self.update_social_capital()\n",
    "\n",
    "        # Update latent variables based on neighbors' influence and dynamics for each behavior\n",
    "        self.update_latent_variables()\n",
    "\n",
    "        # Determine GSI adoption for each behavior after updates\n",
    "        self.determine_adoption()\n",
    "\n",
    "        # Append the current adoption status to the history\n",
    "        self.adoption_flags_history['structural'].append(self.adoption_flags['structural'])\n",
    "        self.adoption_flags_history['raincatching'].append(self.adoption_flags['raincatching'])\n",
    "        self.adoption_flags_history['nonstructural'].append(self.adoption_flags['nonstructural'])\n",
    "\n",
    "        # Calculate Plan to Action scores using the hardcoded coefficients\n",
    "        plan_to_action_scores = self.calculate_plan_to_action_scores()\n",
    "\n",
    "        # Determine Plan to Action adoption\n",
    "        self.determine_plan_to_action_adoption(plan_to_action_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e76284-cb8d-4ee3-b820-af4c75e59382",
   "metadata": {},
   "source": [
    "### Defining the GSI Adoption Model\n",
    "Building upon the Household class, this cell defines the GSIAdoptionModel, which orchestrates the simulation. Key functionalities include:\n",
    "\n",
    "Initialization: Sets up the simulation grid, places agents, and initializes adoption counts.\n",
    "Agent Scheduling: Uses RandomActivation to manage agent steps.\n",
    "Data Collection: Employs DataCollector to record adoption counts and latent variable histories.\n",
    "Model Stepping: Advances the simulation by one step, updating all agents and collecting data.\n",
    "Adoption Rate Calculations: Computes adoption rates both globally and per tract.\n",
    "Agent Inspection: Provides tools to inspect individual agent behaviors and latent variable changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce85f05-78e7-4cac-979b-92f76890cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GSIAdoptionModel(Model):\n",
    "    def __init__(self, tracts, parcels, eligible_parcels, household_data):\n",
    "        \"\"\"\n",
    "        Initialize the GSIAdoptionModel, simulating GSI adoption across multiple tracts with households.\n",
    "\n",
    "        Parameters:\n",
    "            tracts (GeoDataFrame): GeoDataFrame containing tract information with 'GEOID' column.\n",
    "            parcels (GeoDataFrame): GeoDataFrame containing all parcel information with 'GEOID' column.\n",
    "            eligible_parcels (GeoDataFrame): GeoDataFrame containing eligible parcel information with 'GEOID' column.\n",
    "            household_data (list): List containing household information, structured as lists or dicts.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.tracts = tracts.copy()\n",
    "        self.parcels = parcels.copy()  # GeoDataFrame with all parcels\n",
    "        self.eligible_parcels = eligible_parcels.copy()  # GeoDataFrame with 'GEOID' column for eligible parcels\n",
    "        self.schedule = RandomActivation(self)  # Random agent activation\n",
    "\n",
    "        # Ensure 'GEOID' is string for consistent mapping\n",
    "        self.tracts['GEOID'] = self.tracts['GEOID'].astype(str)\n",
    "        self.parcels['GEOID'] = self.parcels['GEOID'].astype(str)\n",
    "        self.eligible_parcels['GEOID'] = self.eligible_parcels['GEOID'].astype(str)\n",
    "\n",
    "        # Create mapping from tract_id to DistrictID\n",
    "        # Assuming 'concl_d' is the column in tracts that maps to DistrictID\n",
    "        tract_id_to_district = self.tracts.set_index('GEOID')['concl_d'].astype(str).to_dict()\n",
    "\n",
    "        # Initialize adoption counts for each behavior type per tract\n",
    "        self.adoption_counts = {\n",
    "            tract_id: {\n",
    "                'structural': 0,\n",
    "                'raincatching': 0,\n",
    "                'nonstructural': 0,\n",
    "                'structural_plan_to_action': 0,\n",
    "                'raincatching_plan_to_action': 0\n",
    "            }\n",
    "            for tract_id in self.tracts['GEOID']\n",
    "        }\n",
    "\n",
    "        # Calculate the number of parcels per tract using 'parcels' for grid sizing\n",
    "        parcels_per_tract = self.parcels.groupby('GEOID').size().to_dict()\n",
    "\n",
    "        # Initialize grids based on the number of parcels in each tract\n",
    "        self.grids = {}\n",
    "        tract_agents = {tract_id: [] for tract_id in self.tracts['GEOID']}\n",
    "\n",
    "        for tract_id in self.tracts['GEOID']:\n",
    "            num_parcels = parcels_per_tract.get(tract_id, 0)\n",
    "            if num_parcels == 0:\n",
    "                num_parcels = 1  # To avoid sqrt(0)\n",
    "\n",
    "            # Calculate grid size based on the number of parcels (~1 cell per parcel)\n",
    "            grid_size = int(np.ceil(np.sqrt(num_parcels)))\n",
    "\n",
    "            # Set a minimum and maximum grid size to avoid extremely small or large grids\n",
    "            min_grid_size = 10\n",
    "            max_grid_size = 200\n",
    "            grid_size = max(min_grid_size, grid_size)\n",
    "            grid_size = min(grid_size, max_grid_size)\n",
    "\n",
    "            # Initialize the grid for the tract\n",
    "            self.grids[tract_id] = MultiGrid(grid_size, grid_size, torus=False)\n",
    "\n",
    "        # Initialize district-specific parameters for Adjustment A\n",
    "        # Define ranges for network_influence_rate, neighborhood_influence_rate, influence_rate, decay_rate, and K\n",
    "        network_influence_rate_min, network_influence_rate_max = 0.001, 0.005\n",
    "        neighborhood_influence_rate_min, neighborhood_influence_rate_max = 0.0002, 0.001\n",
    "        influence_rate_min, influence_rate_max = 0.0001, 0.005\n",
    "        decay_rate_min, decay_rate_max = 0.01, 0.005\n",
    "        K_min, K_max = 0.5, 1\n",
    "\n",
    "        self.district_specific_network_influence_rate = {\n",
    "            tract_id: random.uniform(network_influence_rate_min, network_influence_rate_max) for tract_id in self.tracts['GEOID']\n",
    "        }\n",
    "        self.district_specific_neighborhood_influence_rate = {\n",
    "            tract_id: random.uniform(neighborhood_influence_rate_min, neighborhood_influence_rate_max) for tract_id in self.tracts['GEOID']\n",
    "        }\n",
    "        self.district_specific_influence_rate = {\n",
    "            tract_id: random.uniform(influence_rate_min, influence_rate_max) for tract_id in self.tracts['GEOID']\n",
    "        }\n",
    "        self.district_specific_decay_rate = {\n",
    "            tract_id: random.uniform(decay_rate_min, decay_rate_max) for tract_id in self.tracts['GEOID']\n",
    "        }\n",
    "        self.district_specific_K = {\n",
    "            tract_id: random.uniform(K_min, K_max) for tract_id in self.tracts['GEOID']\n",
    "        }\n",
    "\n",
    "        # Initialize total number of agents\n",
    "        self.num_agents = len(household_data)\n",
    "\n",
    "        agent_id = 0  # Initialize unique agent ID\n",
    "\n",
    "        # Dictionary to store precomputed neighbors for each agent\n",
    "        self.precomputed_neighbors = {}\n",
    "\n",
    "        for household in household_data:\n",
    "            agent = Household(agent_id, self, household)\n",
    "            self.schedule.add(agent)\n",
    "\n",
    "            tract_id = household[household_data_indices['tract_id']]  # Correctly accessing GEOID\n",
    "            tract_id = str(tract_id)  # Ensure tract_id is string\n",
    "\n",
    "            # Set agent's DistrictID\n",
    "            agent.DistrictID = tract_id_to_district.get(tract_id, 'Unknown')\n",
    "\n",
    "            if tract_id not in self.grids:\n",
    "                print(f\"Warning: Tract ID {tract_id} not found in grids. Skipping agent placement.\")\n",
    "                agent_id += 1\n",
    "                continue  # Skip agents with invalid tract IDs\n",
    "\n",
    "            grid = self.grids[tract_id]\n",
    "            grid_size = grid.width  # Assuming square grid\n",
    "\n",
    "            # Randomly place the agent on the tract's grid\n",
    "            x = random.randrange(grid_size)\n",
    "            y = random.randrange(grid_size)\n",
    "            grid.place_agent(agent, (x, y))\n",
    "\n",
    "            # Update agent's position\n",
    "            agent.pos = (x, y)\n",
    "\n",
    "            # Assign district-specific social capital parameters (Adjustment A)\n",
    "            agent.network_influence_rate = self.district_specific_network_influence_rate.get(tract_id, 0.01)\n",
    "            agent.neighborhood_influence_rate = self.district_specific_neighborhood_influence_rate.get(tract_id, 0.01)\n",
    "            agent.SC_INFLUENCE_RATE = self.district_specific_influence_rate.get(tract_id, 0.01)\n",
    "            agent.SC_DECAY_RATE = self.district_specific_decay_rate.get(tract_id, 0.05)\n",
    "            agent.K = self.district_specific_K.get(tract_id, 5)\n",
    "\n",
    "            # Initialize communication method and social network AFTER placement\n",
    "            agent.initialize_communication_method()\n",
    "            agent.initialize_social_network()\n",
    "\n",
    "            # Store agent by tract_id for initial adoption setup\n",
    "            tract_agents[tract_id].append(agent)\n",
    "\n",
    "            # Precompute neighbors and store them in a dictionary\n",
    "            neighbors = grid.get_neighbors((x, y), moore=True, radius=2, include_center=False)\n",
    "            self.precomputed_neighbors[agent_id] = neighbors  # Store neighbors for this agent\n",
    "\n",
    "            agent_id += 1\n",
    "\n",
    "        # Initial adoption percentage\n",
    "        initial_adopter_percentage = 0.005  # 0.5% initial adoption rate for each behavior\n",
    "\n",
    "        for tract_id, agents in tract_agents.items():\n",
    "            # Structural adopters\n",
    "            num_initial_adopters_structural = int(len(agents) * initial_adopter_percentage)\n",
    "            if num_initial_adopters_structural > 0:\n",
    "                initial_structural_adopters = random.sample(agents, num_initial_adopters_structural)\n",
    "                for adopter in initial_structural_adopters:\n",
    "                    adopter.adoption_flags['structural'] = True\n",
    "                    self.adoption_counts[tract_id]['structural'] += 1\n",
    "                    # Update the adoption_flags_history\n",
    "                    adopter.adoption_flags_history['structural'][0] = True  # Set the initial value to True\n",
    "\n",
    "            # Raincatching adopters\n",
    "            num_initial_adopters_raincatching = int(len(agents) * initial_adopter_percentage)\n",
    "            if num_initial_adopters_raincatching > 0:\n",
    "                initial_raincatching_adopters = random.sample(agents, num_initial_adopters_raincatching)\n",
    "                for adopter in initial_raincatching_adopters:\n",
    "                    adopter.adoption_flags['raincatching'] = True\n",
    "                    self.adoption_counts[tract_id]['raincatching'] += 1\n",
    "                    # Update the adoption_flags_history\n",
    "                    adopter.adoption_flags_history['raincatching'][0] = True\n",
    "\n",
    "            # Nonstructural adopters\n",
    "            num_initial_adopters_nonstructural = int(len(agents) * initial_adopter_percentage)\n",
    "            if num_initial_adopters_nonstructural > 0:\n",
    "                initial_nonstructural_adopters = random.sample(agents, num_initial_adopters_nonstructural)\n",
    "                for adopter in initial_nonstructural_adopters:\n",
    "                    adopter.adoption_flags['nonstructural'] = True\n",
    "                    self.adoption_counts[tract_id]['nonstructural'] += 1\n",
    "                    # Update the adoption_flags_history\n",
    "                    adopter.adoption_flags_history['nonstructural'][0] = True\n",
    "\n",
    "        # Initialize DataCollector\n",
    "        self.datacollector = DataCollector(\n",
    "            model_reporters={\"AdoptionCounts\": lambda m: m.adoption_counts.copy()},\n",
    "            agent_reporters={\n",
    "                \"LatentVariablesHistory\": lambda a: {\n",
    "                    \"attitude_structural\": a.latent_variables_history['attitude']['structural'],\n",
    "                    \"awareness_structural\": a.latent_variables_history['awareness']['structural'],\n",
    "                    \"personal_norms_structural\": a.latent_variables_history['personal_norms']['structural'],\n",
    "                    \"attitude_raincatching\": a.latent_variables_history['attitude']['raincatching'],\n",
    "                    \"awareness_raincatching\": a.latent_variables_history['awareness']['raincatching'],\n",
    "                    \"personal_norms_raincatching\": a.latent_variables_history['personal_norms']['raincatching'],\n",
    "                    \"social_capital\": a.latent_variables_history['social_capital'],\n",
    "                    \"behavior_structural\": a.latent_variables_history['behavior']['structural'],\n",
    "                    \"behavior_raincatching\": a.latent_variables_history['behavior']['raincatching']\n",
    "                },\n",
    "                \"AdoptionFlagsHistory\": lambda a: a.adoption_flags_history\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Initialize data structures for collecting statistics\n",
    "        self.latent_vars_stats = {\n",
    "            'structural': {'Adopter': {}, 'Non-Adopter': {}},\n",
    "            'raincatching': {'Adopter': {}, 'Non-Adopter': {}}\n",
    "        }\n",
    "        self.behavior_scores_stats = {  # Added this\n",
    "            'structural': {'Adopter': {}, 'Non-Adopter': {}},\n",
    "            'raincatching': {'Adopter': {}, 'Non-Adopter': {}}\n",
    "        }\n",
    "        self.district_ids = set(agent.DistrictID for agent in self.schedule.agents if agent.DistrictID != 'Unknown')\n",
    "        self.latent_variables = ['attitude', 'awareness', 'personal_norms', 'social_capital']\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Perform a single step of the model, including data collection and agent updates.\n",
    "        \"\"\"\n",
    "        self.schedule.step()\n",
    "        self.datacollector.collect(self)\n",
    "\n",
    "        # Initialize temporary data structures to collect agent data\n",
    "        agents_by_district = {district_id: [] for district_id in self.district_ids}\n",
    "        agents_by_district_and_behavior = {\n",
    "            behavior: {\n",
    "                'Adopter': {district_id: [] for district_id in self.district_ids},\n",
    "                'Non-Adopter': {district_id: [] for district_id in self.district_ids}\n",
    "            } for behavior in ['structural', 'raincatching']\n",
    "        }\n",
    "\n",
    "        # Iterate over agents once to collect and organize data\n",
    "        for agent in self.schedule.agents:\n",
    "            district_id = agent.DistrictID\n",
    "            if district_id == 'Unknown':\n",
    "                continue  # Skip agents with unknown district\n",
    "\n",
    "            # Add agent to the district's list\n",
    "            agents_by_district[district_id].append(agent)\n",
    "\n",
    "            for behavior in ['structural', 'raincatching']:\n",
    "                adoption_status = 'Adopter' if agent.adoption_flags[behavior] else 'Non-Adopter'\n",
    "                agents_by_district_and_behavior[behavior][adoption_status][district_id].append(agent)\n",
    "\n",
    "        # Collect latent variables statistics\n",
    "        for behavior in ['structural', 'raincatching']:\n",
    "            for adoption_status in ['Adopter', 'Non-Adopter']:\n",
    "                for latent_var in self.latent_variables:\n",
    "                    for district_id, agents_list in agents_by_district_and_behavior[behavior][adoption_status].items():\n",
    "                        if not agents_list:\n",
    "                            mean_value = np.nan\n",
    "                            std_value = np.nan\n",
    "                        else:\n",
    "                            if latent_var == 'social_capital':\n",
    "                                # Consider only the latest social capital value\n",
    "                                values = [agent.latent_variables_history[latent_var][-1] for agent in agents_list]\n",
    "                            else:\n",
    "                                values = [agent.latent_variables_history[latent_var][behavior][-1] for agent in agents_list]\n",
    "                            mean_value = np.mean(values)\n",
    "                            std_value = np.std(values)\n",
    "\n",
    "                        # Initialize nested dictionaries if necessary\n",
    "                        self.latent_vars_stats[behavior][adoption_status].setdefault(\n",
    "                            latent_var, {}\n",
    "                        ).setdefault(\n",
    "                            district_id, {'mean': [], 'std': []}\n",
    "                        )\n",
    "\n",
    "                        # Append the statistics\n",
    "                        self.latent_vars_stats[behavior][adoption_status][latent_var][district_id]['mean'].append(mean_value)\n",
    "                        self.latent_vars_stats[behavior][adoption_status][latent_var][district_id]['std'].append(std_value)\n",
    "\n",
    "        # Collect behavior scores statistics\n",
    "        for behavior in ['structural', 'raincatching']:\n",
    "            for adoption_status in ['Adopter', 'Non-Adopter']:\n",
    "                for district_id, agents_list in agents_by_district_and_behavior[behavior][adoption_status].items():\n",
    "                    if not agents_list:\n",
    "                        mean_value = np.nan\n",
    "                        std_value = np.nan\n",
    "                    else:\n",
    "                        values = [agent.behaviors[behavior] for agent in agents_list]\n",
    "                        mean_value = np.mean(values)\n",
    "                        std_value = np.std(values)\n",
    "\n",
    "                    # Initialize nested dictionaries if necessary\n",
    "                    self.behavior_scores_stats[behavior][adoption_status].setdefault(\n",
    "                        district_id, {'mean': [], 'std': []}\n",
    "                    )\n",
    "\n",
    "                    # Append the statistics\n",
    "                    self.behavior_scores_stats[behavior][adoption_status][district_id]['mean'].append(mean_value)\n",
    "                    self.behavior_scores_stats[behavior][adoption_status][district_id]['std'].append(std_value)\n",
    "\n",
    "    def get_adoption_counts(self):\n",
    "        \"\"\"\n",
    "        Retrieve the adoption counts from the DataCollector.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing adoption counts per tract at each step.\n",
    "        \"\"\"\n",
    "        return self.datacollector.get_model_vars_dataframe()[\"AdoptionCounts\"]\n",
    "\n",
    "    def get_adoption_percentage_per_tract(self):\n",
    "        \"\"\"\n",
    "        Calculate the percentage of GSI adoption for each tract.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary mapping tract IDs to their respective adoption percentages.\n",
    "        \"\"\"\n",
    "        adoption_percentage_per_tract = {}\n",
    "        for tract_id in self.adoption_counts.keys():\n",
    "            total_households_in_tract = sum(1 for agent in self.schedule.agents if agent.tract_id == tract_id)\n",
    "            # Calculate total adopted behaviors (structural, raincatching, nonstructural)\n",
    "            adopted_households_in_tract = sum([\n",
    "                self.adoption_counts[tract_id].get('structural', 0),\n",
    "                self.adoption_counts[tract_id].get('raincatching', 0),\n",
    "                self.adoption_counts[tract_id].get('nonstructural', 0)\n",
    "            ])\n",
    "            if total_households_in_tract > 0:\n",
    "                adoption_percentage_per_tract[tract_id] = (adopted_households_in_tract / total_households_in_tract) * 100\n",
    "            else:\n",
    "                adoption_percentage_per_tract[tract_id] = 0.0  # If no households, set adoption percentage to 0\n",
    "        return adoption_percentage_per_tract\n",
    "\n",
    "    def find_agent_with_significant_changes(self):\n",
    "        \"\"\"\n",
    "        Identify the agent with the most significant changes in latent variables.\n",
    "\n",
    "        Returns:\n",
    "            Household: The agent with the highest total change in latent variables.\n",
    "        \"\"\"\n",
    "        max_change = 0\n",
    "        selected_agent = None\n",
    "        for agent in self.schedule.agents:\n",
    "            # Calculate total change across all behaviors and latent variables\n",
    "            total_change = 0\n",
    "            for behavior in ['structural', 'raincatching', 'nonstructural']:\n",
    "                total_change += abs(agent.latent_variables_history['attitude'][behavior][-1] - agent.latent_variables_history['attitude'][behavior][0])\n",
    "                total_change += abs(agent.latent_variables_history['awareness'][behavior][-1] - agent.latent_variables_history['awareness'][behavior][0])\n",
    "                total_change += abs(agent.latent_variables_history['personal_norms'][behavior][-1] - agent.latent_variables_history['personal_norms'][behavior][0])\n",
    "            # Include social capital change\n",
    "            total_change += abs(agent.latent_variables_history['social_capital'][-1] - agent.latent_variables_history['social_capital'][0])\n",
    "\n",
    "            if total_change > max_change:\n",
    "                max_change = total_change\n",
    "                selected_agent = agent\n",
    "        return selected_agent\n",
    "\n",
    "    def get_global_adoption_rate(self, behavior):\n",
    "        \"\"\"\n",
    "        Calculate the global adoption rate for a specific behavior across all tracts.\n",
    "\n",
    "        Parameters:\n",
    "            behavior (str): The behavior type ('structural', 'raincatching', 'nonstructural').\n",
    "\n",
    "        Returns:\n",
    "            float: Adoption rate between 0 and 1.\n",
    "        \"\"\"\n",
    "        total_adoptions = sum(\n",
    "            tract_adoptions.get(behavior, 0) for tract_adoptions in self.adoption_counts.values()\n",
    "        )\n",
    "        return total_adoptions / self.num_agents if self.num_agents > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b5082d-1d5b-46c1-8e48-a376fb4278a7",
   "metadata": {},
   "source": [
    "### Running the Simulation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390cdad2-3e96-47a2-b3a6-c08b710c7543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to run the model\n",
    "model = GSIAdoptionModel(tracts, parcels_with_tracts, eligible_parcels, household_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9563e-332c-414a-a388-227d20aa888a",
   "metadata": {},
   "source": [
    "### Assigning District Identifiers and Preparing for Visualization\n",
    "Before visualizing the simulation results, this cell assigns council district identifiers to the districts GeoDataFrame. It ensures that district IDs are correctly formatted and prepares the data structures needed for aggregating adoption counts at the district level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7904da9-095d-48b3-877a-47bf90cfe48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ----- INITIALIZATION BEFORE THE SIMULATION LOOP -----\n",
    "\n",
    "# Ensure 'council_di' is correctly formatted as string\n",
    "districts[\"council_di\"] = districts[\"council_di\"].apply(\n",
    "    lambda x: str(int(float(x))) if pd.notnull(x) else None\n",
    ")\n",
    "\n",
    "# Initialize lists to store images for GIFs\n",
    "tract_images = []\n",
    "district_images = []\n",
    "\n",
    "# Initialize dictionaries to store household counts and adoption counts for districts\n",
    "households_per_district = {district_id: 0 for district_id in districts[\"council_di\"].unique()}\n",
    "adoption_counts_per_district = {\n",
    "    district_id: {\"structural\": 0, \"raincatching\": 0, \"nonstructural\": 0} \n",
    "    for district_id in districts[\"council_di\"].unique()\n",
    "}\n",
    "\n",
    "# Initialize dictionary to store adoption counts over time for each tract\n",
    "adoption_over_time = {\n",
    "    tract_id: {\"structural\": [], \"raincatching\": [], \"nonstructural\": []} \n",
    "    for tract_id in tracts[\"GEOID\"]\n",
    "}\n",
    "\n",
    "# Initialize dictionary to store adoption counts over time for each district\n",
    "adoption_over_time_district = {\n",
    "    district_id: {\n",
    "        \"structural\": [],\n",
    "        \"raincatching\": [],\n",
    "        \"nonstructural\": [],\n",
    "        \"structural_count\": [],\n",
    "        \"raincatching_count\": [],\n",
    "        \"nonstructural_count\": []\n",
    "    } \n",
    "    for district_id in districts[\"council_di\"].unique()\n",
    "}\n",
    "\n",
    "# Count households per tract and assign them to the correct district\n",
    "households_per_tract = {tract_id: 0 for tract_id in tracts[\"GEOID\"]}\n",
    "for household in household_data:\n",
    "    tract_id = household[0]  # Assuming the first element is tract_id\n",
    "    if tract_id in households_per_tract:\n",
    "        households_per_tract[tract_id] += 1\n",
    "\n",
    "# Aggregate household counts for districts based on their tracts\n",
    "for tract_id in households_per_tract:\n",
    "    try:\n",
    "        district_id = tracts.loc[tracts[\"GEOID\"] == tract_id, \"concl_d\"].values[0]  # Get district ID for the tract\n",
    "        district_id = str(district_id)  # Convert to string to match district keys\n",
    "\n",
    "        # Sum household counts into district level\n",
    "        if district_id in households_per_district:\n",
    "            households_per_district[district_id] += households_per_tract[tract_id]\n",
    "        else:\n",
    "            print(f\"Warning: District ID {district_id} not found for tract {tract_id}\")\n",
    "    except KeyError:\n",
    "        print(f\"Warning: District ID not found for tract {tract_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3228c-8fe8-47d4-93c0-5b8df3b45e5f",
   "metadata": {},
   "source": [
    "### Running the Simulation for Multiple Steps and Generating GIFs\n",
    "This extensive cell runs the simulation over multiple steps (e.g., 36 steps) and performs the following tasks at each step:\n",
    "\n",
    "Model Stepping: Advances the simulation by one step.\n",
    "Adoption Level Mapping: Maps adoption counts to tracts and calculates adoption percentages.\n",
    "District-Level Aggregation: Aggregates adoption counts and percentages at the district level.\n",
    "Visualization: Plots tract-level and district-level adoption percentages using maps.\n",
    "GIF Creation: Captures the visualization frames and compiles them into GIFs to illustrate the evolution of adoption over time.\n",
    "Adoption Count Plotting: Generates plots showing the progression of adoption counts across behaviors over the simulation steps.\n",
    "This cell provides both visual and quantitative insights into how GSI practices are adopted across different geographic and socio-economic contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a5068e-8b58-41a7-89a1-fc0949a99105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- INITIALIZE LISTS TO STORE AVERAGE SCORES -----\n",
    "import matplotlib\n",
    "matplotlib.rcdefaults()  # Reset Matplotlib settings to default\n",
    "# Initialize lists to store average scores over time\n",
    "average_behavior_structural = []\n",
    "average_behavior_raincatching = []\n",
    "average_plan_to_action_structural = []\n",
    "average_plan_to_action_raincatching = []\n",
    "\n",
    "# ----- RUN THE SIMULATION FOR MULTIPLE STEPS -----\n",
    "for step in range(40):\n",
    "    model.step()  # Step through the model\n",
    "\n",
    "    # Map adoption counts to tracts and calculate adoption percentages\n",
    "    tracts[\"structural_adoption_level\"] = tracts[\"GEOID\"].map(\n",
    "        lambda tract_id: (\n",
    "            model.adoption_counts[tract_id]['structural'] / households_per_tract.get(tract_id, 1)\n",
    "        ) * 100 if households_per_tract.get(tract_id, 1) != 0 else 0\n",
    "    )\n",
    "    tracts[\"raincatching_adoption_level\"] = tracts[\"GEOID\"].map(\n",
    "        lambda tract_id: (\n",
    "            model.adoption_counts[tract_id]['raincatching'] / households_per_tract.get(tract_id, 1)\n",
    "        ) * 100 if households_per_tract.get(tract_id, 1) != 0 else 0\n",
    "    )\n",
    "    tracts[\"nonstructural_adoption_level\"] = tracts[\"GEOID\"].map(\n",
    "        lambda tract_id: (\n",
    "            model.adoption_counts[tract_id]['nonstructural'] / households_per_tract.get(tract_id, 1)\n",
    "        ) * 100 if households_per_tract.get(tract_id, 1) != 0 else 0\n",
    "    )\n",
    "\n",
    "    # Store adoption levels for each tract over time\n",
    "    for tract_id in tracts[\"GEOID\"]:\n",
    "        if tract_id in adoption_over_time:\n",
    "            structural_level = tracts.loc[tracts[\"GEOID\"] == tract_id, \"structural_adoption_level\"].values[0]\n",
    "            raincatching_level = tracts.loc[tracts[\"GEOID\"] == tract_id, \"raincatching_adoption_level\"].values[0]\n",
    "            nonstructural_level = tracts.loc[tracts[\"GEOID\"] == tract_id, \"nonstructural_adoption_level\"].values[0]\n",
    "            \n",
    "            adoption_over_time[tract_id][\"structural\"].append(structural_level)\n",
    "            adoption_over_time[tract_id][\"raincatching\"].append(raincatching_level)\n",
    "            adoption_over_time[tract_id][\"nonstructural\"].append(nonstructural_level)\n",
    "\n",
    "    # Initialize dictionary to store total household counts and sum of adopted households for each district\n",
    "    district_adoption_counts = {\n",
    "        district_id: {\"structural\": 0, \"raincatching\": 0, \"nonstructural\": 0}\n",
    "        for district_id in districts[\"council_di\"].unique()\n",
    "    }\n",
    "\n",
    "    # Loop through tracts and sum up adoption values for each district\n",
    "    for tract_id in households_per_tract:\n",
    "        try:\n",
    "            district_id = tracts.loc[tracts[\"GEOID\"] == tract_id, \"concl_d\"].values[0]\n",
    "            district_id = str(district_id)  # Convert to string to match district keys\n",
    "\n",
    "            # Sum the structural, raincatching, and nonstructural adoption counts for each tract\n",
    "            district_adoption_counts[district_id][\"structural\"] += model.adoption_counts[tract_id][\"structural\"]\n",
    "            district_adoption_counts[district_id][\"raincatching\"] += model.adoption_counts[tract_id][\"raincatching\"]\n",
    "            district_adoption_counts[district_id][\"nonstructural\"] += model.adoption_counts[tract_id][\"nonstructural\"]\n",
    "\n",
    "        except KeyError:\n",
    "            print(f\"Warning: District ID not found for tract {tract_id}\")\n",
    "\n",
    "    # Calculate final district-level adoption percentages by dividing sum of adopted households by total households in the district\n",
    "    district_adoption_percentages = {}\n",
    "    for district_id in district_adoption_counts:\n",
    "        if households_per_district[district_id] > 0:\n",
    "            district_adoption_percentages[district_id] = {\n",
    "                \"structural\": (\n",
    "                    district_adoption_counts[district_id][\"structural\"] / households_per_district[district_id]\n",
    "                ) * 100,\n",
    "                \"raincatching\": (\n",
    "                    district_adoption_counts[district_id][\"raincatching\"] / households_per_district[district_id]\n",
    "                ) * 100,\n",
    "                \"nonstructural\": (\n",
    "                    district_adoption_counts[district_id][\"nonstructural\"] / households_per_district[district_id]\n",
    "                ) * 100,\n",
    "            }\n",
    "        else:\n",
    "            district_adoption_percentages[district_id] = {\n",
    "                \"structural\": 0.0,\n",
    "                \"raincatching\": 0.0,\n",
    "                \"nonstructural\": 0.0,\n",
    "            }\n",
    "\n",
    "    # Store district-level adoption counts over time\n",
    "    for district_id in district_adoption_counts:\n",
    "        adoption_over_time_district[district_id][\"structural\"].append(\n",
    "            district_adoption_percentages[district_id][\"structural\"]\n",
    "        )\n",
    "        adoption_over_time_district[district_id][\"raincatching\"].append(\n",
    "            district_adoption_percentages[district_id][\"raincatching\"]\n",
    "        )\n",
    "        adoption_over_time_district[district_id][\"nonstructural\"].append(\n",
    "            district_adoption_percentages[district_id][\"nonstructural\"]\n",
    "        )\n",
    "        # Store raw counts over time as well\n",
    "        adoption_over_time_district[district_id][\"structural_count\"].append(\n",
    "            district_adoption_counts[district_id][\"structural\"]\n",
    "        )\n",
    "        adoption_over_time_district[district_id][\"raincatching_count\"].append(\n",
    "            district_adoption_counts[district_id][\"raincatching\"]\n",
    "        )\n",
    "        adoption_over_time_district[district_id][\"nonstructural_count\"].append(\n",
    "            district_adoption_counts[district_id][\"nonstructural\"]\n",
    "        )\n",
    "\n",
    "    # ----- NEW SECTION: Compute Average Behavior and Plan-to-Action Scores -----\n",
    "    # Initialize variables to store total scores\n",
    "    total_behavior_structural = 0\n",
    "    total_behavior_raincatching = 0\n",
    "    total_plan_structural = 0\n",
    "    total_plan_raincatching = 0\n",
    "    num_agents = len(model.schedule.agents)\n",
    "\n",
    "    # Iterate through all agents to sum up behaviors and plan-to-action scores\n",
    "    for agent in model.schedule.agents:\n",
    "        total_behavior_structural += agent.behaviors['structural']\n",
    "        total_behavior_raincatching += agent.behaviors['raincatching']\n",
    "        plan_scores = agent.calculate_plan_to_action_scores()\n",
    "        total_plan_structural += plan_scores.get('structural', 0)\n",
    "        total_plan_raincatching += plan_scores.get('raincatching', 0)\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_behavior_structural = (\n",
    "        total_behavior_structural / num_agents if num_agents > 0 else 0\n",
    "    )\n",
    "    avg_behavior_raincatching = (\n",
    "        total_behavior_raincatching / num_agents if num_agents > 0 else 0\n",
    "    )\n",
    "    avg_plan_structural = (\n",
    "        total_plan_structural / num_agents if num_agents > 0 else 0\n",
    "    )\n",
    "    avg_plan_raincatching = (\n",
    "        total_plan_raincatching / num_agents if num_agents > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Append averages to their respective lists\n",
    "    average_behavior_structural.append(avg_behavior_structural)\n",
    "    average_behavior_raincatching.append(avg_behavior_raincatching)\n",
    "    average_plan_to_action_structural.append(avg_plan_structural)\n",
    "    average_plan_to_action_raincatching.append(avg_plan_raincatching)\n",
    "    # ----- END OF NEW SECTION -----\n",
    "\n",
    "    # ----- MAP DISTRICT ADOPTION PERCENTAGES TO GEODataFrame -----\n",
    "    districts[\"structural_adoption_level\"] = districts[\"council_di\"].map(\n",
    "        lambda district_id: district_adoption_percentages[district_id][\"structural\"]\n",
    "        if district_id in district_adoption_percentages else 0\n",
    "    )\n",
    "    districts[\"raincatching_adoption_level\"] = districts[\"council_di\"].map(\n",
    "        lambda district_id: district_adoption_percentages[district_id][\"raincatching\"]\n",
    "        if district_id in district_adoption_percentages else 0\n",
    "    )\n",
    "    districts[\"nonstructural_adoption_level\"] = districts[\"council_di\"].map(\n",
    "        lambda district_id: district_adoption_percentages[district_id][\"nonstructural\"]\n",
    "        if district_id in district_adoption_percentages else 0\n",
    "    )\n",
    "\n",
    "    # ----- EXISTING PLOTTING CODE FOR ADOPTION COUNTS -----\n",
    "    # Plot tract-level and district-level GSI adoption percentages\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(20, 20))\n",
    "\n",
    "    # Tract-level plots\n",
    "    tracts.plot(\n",
    "        column=\"structural_adoption_level\",\n",
    "        ax=ax[0, 0],\n",
    "        legend=True,\n",
    "        cmap=\"viridis\",\n",
    "        legend_kwds={\n",
    "            'label': \"Tract Structural Adoption (%)\",\n",
    "            'orientation': \"horizontal\"\n",
    "        },\n",
    "        vmin=0,\n",
    "        vmax=30\n",
    "    )\n",
    "    ax[0, 0].set_title(f\"Tract Structural Adoption at Step {step}\")\n",
    "\n",
    "    tracts.plot(\n",
    "        column=\"raincatching_adoption_level\",\n",
    "        ax=ax[0, 1],\n",
    "        legend=True,\n",
    "        cmap=\"viridis\",\n",
    "        legend_kwds={\n",
    "            'label': \"Tract Raincatching Adoption (%)\",\n",
    "            'orientation': \"horizontal\"\n",
    "        },\n",
    "        vmin=0,\n",
    "        vmax=30\n",
    "    )\n",
    "    ax[0, 1].set_title(f\"Tract Raincatching Adoption at Step {step}\")\n",
    "\n",
    "    tracts.plot(\n",
    "        column=\"nonstructural_adoption_level\",\n",
    "        ax=ax[0, 2],\n",
    "        legend=True,\n",
    "        cmap=\"viridis\",\n",
    "        legend_kwds={\n",
    "            'label': \"Tract Nonstructural Adoption (%)\",\n",
    "            'orientation': \"horizontal\"\n",
    "        },\n",
    "        vmin=0,\n",
    "        vmax=30\n",
    "    )\n",
    "    ax[0, 2].set_title(f\"Tract Nonstructural Adoption at Step {step}\")\n",
    "\n",
    "    # District-level plots\n",
    "    districts.plot(\n",
    "        column=\"structural_adoption_level\",\n",
    "        ax=ax[1, 0],\n",
    "        legend=True,\n",
    "        cmap=\"viridis\",\n",
    "        legend_kwds={\n",
    "            'label': \"District Structural Adoption (%)\",\n",
    "            'orientation': \"horizontal\"\n",
    "        },\n",
    "        vmin=0,\n",
    "        vmax=30\n",
    "    )\n",
    "    ax[1, 0].set_title(f\"District Structural Adoption at Step {step}\")\n",
    "\n",
    "    districts.plot(\n",
    "        column=\"raincatching_adoption_level\",\n",
    "        ax=ax[1, 1],\n",
    "        legend=True,\n",
    "        cmap=\"viridis\",\n",
    "        legend_kwds={\n",
    "            'label': \"District Raincatching Adoption (%)\",\n",
    "            'orientation': \"horizontal\"\n",
    "        },\n",
    "        vmin=0,\n",
    "        vmax=30\n",
    "    )\n",
    "    ax[1, 1].set_title(f\"District Raincatching Adoption at Step {step}\")\n",
    "\n",
    "    districts.plot(\n",
    "        column=\"nonstructural_adoption_level\",\n",
    "        ax=ax[1, 2],\n",
    "        legend=True,\n",
    "        cmap=\"viridis\",\n",
    "        legend_kwds={\n",
    "            'label': \"District Nonstructural Adoption (%)\",\n",
    "            'orientation': \"horizontal\"\n",
    "        },\n",
    "        vmin=0,\n",
    "        vmax=30\n",
    "    )\n",
    "    ax[1, 2].set_title(f\"District Nonstructural Adoption at Step {step}\")\n",
    "\n",
    "    plt.show()  # Display the plots inline during each iteration\n",
    "\n",
    "    # ----- CONVERT PLOTS TO IMAGES AND APPEND TO GIF LISTS -----\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Tract-level images\n",
    "    tract_image = Image.frombytes('RGBA', fig.canvas.get_width_height(), fig.canvas.buffer_rgba())\n",
    "    tract_images.append(np.asarray(tract_image))\n",
    "\n",
    "    # District-level images\n",
    "    district_image = Image.frombytes('RGBA', fig.canvas.get_width_height(), fig.canvas.buffer_rgba())\n",
    "    district_images.append(np.asarray(district_image))\n",
    "\n",
    "    plt.close(fig)  # Free memory\n",
    "\n",
    "# ----- AFTER THE SIMULATION LOOP -----\n",
    "\n",
    "# Save GIF for tract-level adoption evolution\n",
    "tract_gif_path = 'yourlocation/Shapefiles/New folder/gsi_tract_adoption_evolution.gif'\n",
    "imageio.mimsave(tract_gif_path, tract_images, fps=8, loop=0)  # Save GIF with 8 frames per second\n",
    "print(f\"Tract-level GIF saved at {tract_gif_path}\")\n",
    "\n",
    "# Save GIF for district-level adoption evolution\n",
    "district_gif_path = 'yourlocation/Shapefiles/New folder/gsi_district_adoption_evolution.gif'\n",
    "imageio.mimsave(district_gif_path, district_images, fps=8, loop=0)  # Save GIF with 8 frames per second\n",
    "print(f\"District-level GIF saved at {district_gif_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4b409-a49b-4b96-999c-432658ad0126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb6122-ab15-4188-9882-7e46081392cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab1a74c-9028-40e0-a6d9-6c7034177230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19628e-498b-4d20-a398-ffd55223c66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcdefaults()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f0db7-1757-4b31-99f1-8d610ca71391",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9523a-5dc1-49ff-b80e-6cdc6a8a52fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Prepare the GeoDataFrame and sort by 'council_di' for the bar chart\n",
    "# --------------------------------------------------------------------\n",
    "districts = districts.to_crs(epsg=4326)\n",
    "# Sort numerically by converting the 'council_di' values to integers\n",
    "districts_sorted = districts.sort_values(by='council_di', key=lambda col: col.astype(int))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. Create the Figure and GridSpec Layout\n",
    "#    - 2 rows, 2 columns\n",
    "#    - Top row for subplots (a) and (b) side by side\n",
    "#    - Bottom row for subplot (c) spanning both columns\n",
    "#    - Use height_ratios to make the bar chart row a bit smaller\n",
    "# --------------------------------------------------------------------\n",
    "fig = plt.figure(figsize=(16, 12), dpi=500)\n",
    "gs = fig.add_gridspec(nrows=2, ncols=2, height_ratios=[1, 0.5])\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. Subplot (a): District-Level Structural Adoption (Top-Left)\n",
    "# --------------------------------------------------------------------\n",
    "ax_a = fig.add_subplot(gs[0, 0])\n",
    "\n",
    "# Plot WITHOUT legend (we'll add a manual colorbar below)\n",
    "districts.plot(\n",
    "    column='structural_adoption_level',\n",
    "    ax=ax_a,\n",
    "    cmap='viridis',\n",
    "    vmin=0,\n",
    "    vmax=20,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Use Axes Grid to append a colorbar axis to the right of ax_a\n",
    "divider_a = make_axes_locatable(ax_a)\n",
    "cax_a = divider_a.append_axes(\"right\", size=\"5%\", pad=0.1)  # Adjust size & pad as desired\n",
    "\n",
    "# Create the colorbar\n",
    "sm_a = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=0, vmax=20))\n",
    "sm_a._A = []  # dummy array for colorbar\n",
    "cbar_a = plt.colorbar(sm_a, cax=cax_a)\n",
    "# --- Removed the label for subplot (a)'s colorbar ---\n",
    "# cbar_a.set_label(\"Adoption (%)\", fontsize=12)\n",
    "\n",
    "# Place the subplot label (a) INSIDE top-right\n",
    "ax_a.text(\n",
    "    0.98, 0.98, '(a)',\n",
    "    transform=ax_a.transAxes,\n",
    "    ha='right', va='top',\n",
    "    fontsize=15\n",
    ")\n",
    "\n",
    "# Optional: Label each district with its 'council_di'\n",
    "for idx, row in districts.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax_a.text(\n",
    "        centroid.x, centroid.y,\n",
    "        str(row['council_di']),\n",
    "        color='white',\n",
    "        fontsize=12,\n",
    "        ha='center', va='center',\n",
    "        path_effects=[pe.withStroke(linewidth=2, foreground='black')]\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. Subplot (b): District-Level Raincatching Adoption (Top-Right)\n",
    "# --------------------------------------------------------------------\n",
    "ax_b = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "districts.plot(\n",
    "    column='raincatching_adoption_level',\n",
    "    ax=ax_b,\n",
    "    cmap='viridis',\n",
    "    vmin=0,\n",
    "    vmax=20,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "# Append colorbar axis to the right of ax_b\n",
    "divider_b = make_axes_locatable(ax_b)\n",
    "cax_b = divider_b.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "sm_b = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=0, vmax=20))\n",
    "sm_b._A = []\n",
    "cbar_b = plt.colorbar(sm_b, cax=cax_b)\n",
    "cbar_b.set_label(\"Adoption (%)\", fontsize=12)\n",
    "\n",
    "# Place the subplot label (b) INSIDE top-right\n",
    "ax_b.text(\n",
    "    0.98, 0.98, '(b)',\n",
    "    transform=ax_b.transAxes,\n",
    "    ha='right', va='top',\n",
    "    fontsize=15\n",
    ")\n",
    "\n",
    "# Optional: Label each district\n",
    "for idx, row in districts.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax_b.text(\n",
    "        centroid.x, centroid.y,\n",
    "        str(row['council_di']),\n",
    "        color='white',\n",
    "        fontsize=12,\n",
    "        ha='center', va='center',\n",
    "        path_effects=[pe.withStroke(linewidth=2, foreground='black')]\n",
    "    )\n",
    "\n",
    "# Ensure both maps share the same geographic extent\n",
    "x_min, y_min, x_max, y_max = districts.total_bounds\n",
    "for ax in [ax_a, ax_b]:\n",
    "    ax.set_xlim(left=x_min, right=x_max)\n",
    "    ax.set_ylim(bottom=y_min, top=y_max)\n",
    "    ax.set_aspect('equal', 'box')\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5. Subplot (c): Grouped Bar Chart (Bottom Row, Spanning Both Columns)\n",
    "# --------------------------------------------------------------------\n",
    "ax_c = fig.add_subplot(gs[1, :])\n",
    "\n",
    "# Place the subplot label (c) INSIDE top-right\n",
    "ax_c.text(\n",
    "    0.98, 0.98, '(c)',\n",
    "    transform=ax_c.transAxes,\n",
    "    ha='right', va='top',\n",
    "    fontsize=15\n",
    ")\n",
    "\n",
    "# Extract sorted data for the bar chart\n",
    "district_ids = districts_sorted['council_di'].astype(str).tolist()\n",
    "struct_vals  = districts_sorted['structural_adoption_level'].values\n",
    "rain_vals    = districts_sorted['raincatching_adoption_level'].values\n",
    "\n",
    "x = np.arange(len(district_ids))\n",
    "bar_width = 0.3\n",
    "\n",
    "# Plot bars\n",
    "bars_struct = ax_c.bar(\n",
    "    x - bar_width, struct_vals,\n",
    "    width=bar_width, color='blue',\n",
    "    label='Structural'\n",
    ")\n",
    "bars_rain = ax_c.bar(\n",
    "    x + bar_width, rain_vals,\n",
    "    width=bar_width, color='green',\n",
    "    label='Raincatching'\n",
    ")\n",
    "\n",
    "# Ticks & Labels\n",
    "ax_c.set_xticks(x)\n",
    "ax_c.set_xticklabels(district_ids, fontsize=12)\n",
    "ax_c.tick_params(axis='y', labelsize=12)\n",
    "ax_c.set_ylim(0, 20)\n",
    "\n",
    "# Add dotted lines for averages\n",
    "struct_mean = np.mean(struct_vals)\n",
    "rain_mean   = np.mean(rain_vals)\n",
    "ax_c.axhline(struct_mean, color='blue', linestyle='--', label='Structural Avg')\n",
    "ax_c.axhline(rain_mean,   color='green', linestyle='--', label='Raincatching Avg')\n",
    "\n",
    "# Axis Labels\n",
    "ax_c.set_xlabel(\"District ID\", fontsize=15)\n",
    "ax_c.set_ylabel(\"Adoption (%)\", fontsize=15)\n",
    "\n",
    "# Legend INSIDE, top-right; shift downward so (c) label is visible\n",
    "ax_c.legend(\n",
    "    loc='upper right',\n",
    "    bbox_to_anchor=(.52, 0.95),\n",
    "    fontsize=14\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6. Final Layout and Save\n",
    "# --------------------------------------------------------------------\n",
    "plt.tight_layout()  # Ensures colorbars fit inside subplots and alignment is correct\n",
    "\n",
    "save_directory = 'yourlocation/Figures'\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "filename = 'gsi_adoption_final_step.png'\n",
    "save_path = os.path.join(save_directory, filename)\n",
    "\n",
    "plt.savefig(save_path, dpi=1200, bbox_inches='tight')\n",
    "print(f\"Figure saved at {save_path}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e705847-2837-4739-9b0e-da7f5fabba5e",
   "metadata": {},
   "source": [
    "## Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20c78c-3b1e-49ab-a92c-82f788829cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe  # For text outline if needed\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Prepare Data and Colors\n",
    "# -------------------------------\n",
    "# Suppose adoption_over_time_district is a dict like:\n",
    "# adoption_over_time_district[district_id] = {\n",
    "#     \"structural\":    [... % over time ...],\n",
    "#     \"raincatching\":  [... % over time ...]\n",
    "# }\n",
    "\n",
    "# 1a. Get the list of district IDs from your data\n",
    "original_district_ids = list(adoption_over_time_district.keys())\n",
    "\n",
    "# 1b. Sort district IDs numerically (so District 1, 2, 3, etc.)\n",
    "sorted_district_ids = sorted(original_district_ids, key=lambda x: int(x))\n",
    "\n",
    "# 1c. Build a color dictionary so each district has a distinct color\n",
    "colors = plt.cm.tab10.colors  # tab10 has 10 distinct colors\n",
    "color_dict = {\n",
    "    d_id: colors[i % len(colors)] for i, d_id in enumerate(sorted_district_ids)\n",
    "}\n",
    "\n",
    "# 1d. Convert your structural/raincatching data into NumPy arrays for easy stats\n",
    "# Rows = districts (in sorted order), Columns = time steps\n",
    "structural_all = np.array([\n",
    "    adoption_over_time_district[d_id][\"structural\"]\n",
    "    for d_id in sorted_district_ids\n",
    "])\n",
    "rain_all = np.array([\n",
    "    adoption_over_time_district[d_id][\"raincatching\"]\n",
    "    for d_id in sorted_district_ids\n",
    "])\n",
    "\n",
    "# 1e. Compute mean, min, max for structural and raincatching (across districts)\n",
    "mean_struct = structural_all.mean(axis=0)\n",
    "min_struct  = structural_all.min(axis=0)\n",
    "max_struct  = structural_all.max(axis=0)\n",
    "\n",
    "mean_rain = rain_all.mean(axis=0)\n",
    "min_rain  = rain_all.min(axis=0)\n",
    "max_rain  = rain_all.max(axis=0)\n",
    "\n",
    "# 1f. Determine number of time steps (T) and number of districts (N)\n",
    "N, T = structural_all.shape  # N = # districts, T = # time steps\n",
    "\n",
    "# ----------------------------------------\n",
    "# 2. Compute Ranks for Bump Plots\n",
    "# ----------------------------------------\n",
    "# We want rank=1 for the highest adoption.\n",
    "# For each time step t, we sort the districts by descending adoption.\n",
    "\n",
    "# 2a. Structural Ranks\n",
    "struct_ranks = np.zeros((N, T))\n",
    "for t in range(T):\n",
    "    # Get adoption values at time t for all districts\n",
    "    values_t = structural_all[:, t]\n",
    "    # Sort in descending order\n",
    "    sorted_indices = np.argsort(-values_t)\n",
    "    # Assign rank 1 to the highest adoption, etc.\n",
    "    rank_array = np.zeros(N)\n",
    "    for rank_position, idx_dist in enumerate(sorted_indices):\n",
    "        rank_array[idx_dist] = rank_position + 1\n",
    "    struct_ranks[:, t] = rank_array\n",
    "\n",
    "# 2b. Raincatching Ranks\n",
    "rain_ranks = np.zeros((N, T))\n",
    "for t in range(T):\n",
    "    values_t = rain_all[:, t]\n",
    "    sorted_indices = np.argsort(-values_t)\n",
    "    rank_array = np.zeros(N)\n",
    "    for rank_position, idx_dist in enumerate(sorted_indices):\n",
    "        rank_array[idx_dist] = rank_position + 1\n",
    "    rain_ranks[:, t] = rank_array\n",
    "\n",
    "# ----------------------------------------\n",
    "# 3. Create the 2x2 Subplots\n",
    "# ----------------------------------------\n",
    "plt.style.use('classic')  # or any style you prefer\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(22, 12), facecolor='white', dpi=500)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# ----------------------------------------\n",
    "# (a) Structural: Mean & Range\n",
    "# ----------------------------------------\n",
    "ax[0, 0].plot(range(T), mean_struct, color='blue', label='Mean')\n",
    "ax[0, 0].fill_between(range(T), min_struct, max_struct, color='blue', alpha=0.2,\n",
    "                      label='MinMax Range')\n",
    "ax[0, 0].set_title(\"(a) Structural: Mean & Range\", fontsize=16)\n",
    "ax[0, 0].set_xlabel(\"Steps\", fontsize=14)\n",
    "ax[0, 0].set_ylabel(\"Adoption (%)\", fontsize=14)\n",
    "ax[0, 0].grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "ax[0, 0].spines['top'].set_visible(False)\n",
    "ax[0, 0].spines['right'].set_visible(False)\n",
    "# Legend placed in the top left with larger font\n",
    "ax[0, 0].legend(loc='upper left', fontsize=12)\n",
    "\n",
    "# ----------------------------------------\n",
    "# (b) Structural: Rank Over Time (Bump Plot)\n",
    "# ----------------------------------------\n",
    "for i, d_id in enumerate(sorted_district_ids):\n",
    "    ax[0, 1].plot(range(T), struct_ranks[i, :],\n",
    "                  color=color_dict[d_id],\n",
    "                  label=f\"District {d_id}\",\n",
    "                  linewidth=2)\n",
    "# Invert y-axis so rank=1 is at the top\n",
    "ax[0, 1].invert_yaxis()\n",
    "ax[0, 1].set_title(\"(b) Structural: Rank Over Time\", fontsize=16)\n",
    "ax[0, 1].set_xlabel(\"Steps\", fontsize=14)\n",
    "ax[0, 1].set_ylabel(\"Rank (1 = Highest)\", fontsize=14)\n",
    "ax[0, 1].grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "ax[0, 1].spines['top'].set_visible(False)\n",
    "ax[0, 1].spines['right'].set_visible(False)\n",
    "\n",
    "# Create a local legend with unique district labels; place it in bottom right and increase fontsize\n",
    "handles_b, labels_b = ax[0, 1].get_legend_handles_labels()\n",
    "unique_dict_b = dict(zip(labels_b, handles_b))  # Remove duplicates\n",
    "ax[0, 1].legend(unique_dict_b.values(), unique_dict_b.keys(),\n",
    "                loc='lower right', fontsize=12, ncol=2)\n",
    "\n",
    "# ----------------------------------------\n",
    "# (c) Raincatching: Mean & Range\n",
    "# ----------------------------------------\n",
    "ax[1, 0].plot(range(T), mean_rain, color='green', label='Mean')\n",
    "ax[1, 0].fill_between(range(T), min_rain, max_rain, color='green', alpha=0.2,\n",
    "                      label='MinMax Range')\n",
    "ax[1, 0].set_title(\"(c) Raincatching: Mean & Range\", fontsize=16)\n",
    "ax[1, 0].set_xlabel(\"Steps\", fontsize=14)\n",
    "ax[1, 0].set_ylabel(\"Adoption (%)\", fontsize=14)\n",
    "ax[1, 0].grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "ax[1, 0].spines['top'].set_visible(False)\n",
    "ax[1, 0].spines['right'].set_visible(False)\n",
    "# Legend placed in the top left with larger font\n",
    "ax[1, 0].legend(loc='upper left', fontsize=12)\n",
    "\n",
    "# ----------------------------------------\n",
    "# (d) Raincatching: Rank Over Time (Bump Plot)\n",
    "# ----------------------------------------\n",
    "for i, d_id in enumerate(sorted_district_ids):\n",
    "    ax[1, 1].plot(range(T), rain_ranks[i, :],\n",
    "                  color=color_dict[d_id],\n",
    "                  label=f\"District {d_id}\",\n",
    "                  linewidth=2)\n",
    "ax[1, 1].invert_yaxis()\n",
    "ax[1, 1].set_title(\"(d) Raincatching: Rank Over Time\", fontsize=16)\n",
    "ax[1, 1].set_xlabel(\"Steps\", fontsize=14)\n",
    "ax[1, 1].set_ylabel(\"Rank (1 = Highest)\", fontsize=14)\n",
    "ax[1, 1].grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "ax[1, 1].spines['top'].set_visible(False)\n",
    "ax[1, 1].spines['right'].set_visible(False)\n",
    "\n",
    "# Create a local legend with unique district labels; place it in top right with larger font\n",
    "handles_d, labels_d = ax[1, 1].get_legend_handles_labels()\n",
    "unique_dict_d = dict(zip(labels_d, handles_d))\n",
    "ax[1, 1].legend(unique_dict_d.values(), unique_dict_d.keys(),\n",
    "                loc='upper right', fontsize=12, ncol=2)\n",
    "\n",
    "# ----------------------------------------\n",
    "# 4. Final Layout and Save\n",
    "# ----------------------------------------\n",
    "plt.tight_layout()\n",
    "\n",
    "save_directory = 'C:/Users/ka34292/Desktop/Agent Based/Figures'\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "filename_png = 'adoption_over_time_2x2_plots.png'\n",
    "save_path_png = os.path.join(save_directory, filename_png)\n",
    "\n",
    "plt.savefig(save_path_png, dpi=500, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2287ccd-5c62-42b2-a693-184bbd719c90",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53957d7-3f81-402f-a285-0615a79c9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcdefaults()\n",
    "\n",
    "save_directory = 'yourlocation/Shapefiles/New folder/'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_latent_variable_heatmap_with_mean_final_adopters(model, save_directory):\n",
    "    \"\"\"\n",
    "    Creates a figure with 8 subplots (4 rows x 2 columns) showing heatmaps of the density of \n",
    "    latent variable values over time for each latent variable and behavior, aggregated over all \n",
    "    final adopters. Very low density values (below 1% of max) are set to white using a modified \n",
    "    custom colormap that goes from white to dark blue. The mean time series is overlaid as a black line.\n",
    "    \n",
    "    Parameters:\n",
    "      model: The simulation model containing agent latent variable histories.\n",
    "      save_directory (str): Directory where the resulting figure will be saved.\n",
    "    \"\"\"\n",
    "    latent_variables = ['attitude', 'awareness', 'personal_norms', 'social_capital']\n",
    "    behaviors = ['structural', 'raincatching']\n",
    "    \n",
    "    # Create a custom colormap that goes from white to dark blue.\n",
    "    custom_palette = [\n",
    "        '#ffffff',  # White for the lowest density.\n",
    "        '#add8e6',  # LightBlue.\n",
    "        '#6495ed',  # CornflowerBlue.\n",
    "        '#00008b'   # DarkBlue.\n",
    "    ]\n",
    "    base_cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_blue\", custom_palette, N=256)\n",
    "    cmap_colors = base_cmap(np.linspace(0, 1, 256))\n",
    "    # Ensure the lowest density remains white.\n",
    "    cmap_colors[0] = [1, 1, 1, 1]\n",
    "    custom_cmap = mcolors.ListedColormap(cmap_colors)\n",
    "    \n",
    "    # Create 4x2 subplots.\n",
    "    fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(14, 12), dpi=1000)\n",
    "    subplot_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)', '(g)', '(h)']\n",
    "    label_index = 0\n",
    "    \n",
    "    # We will keep track of the last heatmap (imshow) so we can create a shared colorbar.\n",
    "    last_imshow = None\n",
    "    \n",
    "    # Loop over each latent variable and behavior.\n",
    "    for i, var in enumerate(latent_variables):\n",
    "        for j, beh in enumerate(behaviors):\n",
    "            ax = axs[i, j]\n",
    "            all_data = []  # Store (time_index, value) pairs from all final adopters.\n",
    "            \n",
    "            # Gather data from agents that are final adopters for this behavior.\n",
    "            for agent in model.schedule.agents:\n",
    "                if agent.adoption_flags[beh]:\n",
    "                    # For 'social_capital', use overall history; otherwise, use behavior-specific data.\n",
    "                    series = agent.latent_variables_history['social_capital'] if var == 'social_capital' \\\n",
    "                             else agent.latent_variables_history[var][beh]\n",
    "                    for t, v in enumerate(series):\n",
    "                        all_data.append((t, v))\n",
    "            \n",
    "            if not all_data:\n",
    "                ax.text(0.5, 0.5, \"No data\", transform=ax.transAxes, \n",
    "                        ha='center', va='center', fontsize=10)\n",
    "                continue\n",
    "            \n",
    "            all_data = np.array(all_data)\n",
    "            times = all_data[:, 0]\n",
    "            values = all_data[:, 1]\n",
    "            \n",
    "            # Define bins for time and value.\n",
    "            num_time_bins = int(np.max(times)) + 1\n",
    "            num_value_bins = 200  # Adjust as needed.\n",
    "            \n",
    "            # Create a 2D histogram (density map).\n",
    "            heatmap, xedges, yedges = np.histogram2d(times, values, bins=[num_time_bins, num_value_bins])\n",
    "            # Threshold: set densities below 10% of max to zero (will be shown as white).\n",
    "            threshold = 0.1 * np.nanmax(heatmap)\n",
    "            heatmap[heatmap < threshold] = 0\n",
    "            \n",
    "            extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n",
    "            \n",
    "            # Plot the heatmap using the custom colormap.\n",
    "            im = ax.imshow(\n",
    "                heatmap.T, origin='lower', aspect='auto', extent=extent, \n",
    "                cmap=custom_cmap, vmin=0, alpha=0.6\n",
    "            )\n",
    "            last_imshow = im  # Save reference for the colorbar\n",
    "            \n",
    "            # Compute mean time series from the raw data for each time step.\n",
    "            time_steps = np.arange(num_time_bins)\n",
    "            mean_series = []\n",
    "            for t in time_steps:\n",
    "                vals_at_t = values[times == t]\n",
    "                mean_series.append(np.mean(vals_at_t) if len(vals_at_t) > 0 else np.nan)\n",
    "            mean_series = np.array(mean_series)\n",
    "            \n",
    "            # Overlay the mean time series as a black line.\n",
    "            ax.plot(time_steps, mean_series, color='black', linewidth=2, alpha=1.0)\n",
    "            \n",
    "            ax.set_xlabel(\"Steps\", fontsize=10)\n",
    "            \n",
    "            # Add subplot label.\n",
    "            ax.text(0.98, 0.98, subplot_labels[label_index],\n",
    "                    transform=ax.transAxes, ha='right', va='top', fontsize=10)\n",
    "            label_index += 1\n",
    "            \n",
    "            # Set tick labels to display as floats with 1 decimal place.\n",
    "            ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.1f}'))\n",
    "            ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f'{y:.1f}'))\n",
    "            \n",
    "            # Set tick parameters to mimic the time series style.\n",
    "            ax.tick_params(axis='both', which='major', labelsize=8)\n",
    "            # Optionally, remove the top and right spines for a cleaner look.\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Explicitly set y-axis limits for each row.\n",
    "    row_ylims = [(-3, 1), (-1, 1), (-1.7, 1.3), (-0.2, 0.2)]\n",
    "    for i in range(4):\n",
    "        for j in range(2):\n",
    "            axs[i, j].set_ylim(row_ylims[i])\n",
    "    \n",
    "    # Label the y-axes for each latent variable in the left column.\n",
    "    y_labels = [\"Attitude\", \"Awareness\", \"Personal Norms\", \"Social Capital\"]\n",
    "    for i, label in enumerate(y_labels):\n",
    "        axs[i, 0].set_ylabel(label, fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add a single horizontal colorbar at the bottom if we have at least one heatmap.\n",
    "    if last_imshow is not None:\n",
    "        fig.subplots_adjust(bottom=0.12)\n",
    "        cbar_ax = fig.add_axes([0.15, 0.06, 0.7, 0.02])  # [left, bottom, width, height]\n",
    "        cbar = fig.colorbar(last_imshow, cax=cbar_ax, orientation='horizontal')\n",
    "        cbar.set_label(\"Number of Households\", fontsize=10)\n",
    "        cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "    # Define filename and ensure the save directory exists.\n",
    "    filename = \"latent_variable_heatmap_with_mean_final_adopters1.png\"\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    save_path = os.path.join(save_directory, filename)\n",
    "    \n",
    "    plt.savefig(save_path, dpi=1200, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# Example usage:\n",
    "plot_latent_variable_heatmap_with_mean_final_adopters(\n",
    "    model=model, \n",
    "    save_directory=save_directory\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647db20-45d8-4a71-b0d2-b002dab6fc03",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604dbbdd-b1c9-487a-abbb-816e4015d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "###############################################################################\n",
    "#     CODE ASSUMES plot_df IS ALREADY CREATED AS IN YOUR MAIN SCRIPT.         #\n",
    "#     (WITH: 'median_income_thousands', 'percent_communities_of_color',       #\n",
    "#      'percent_no_adv_degree', 'structural_adoption_percent',               #\n",
    "#      'raincatching_adoption_percent')                                      #\n",
    "###############################################################################\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# This script creates a 2x3 grid of HEXBIN plots:\n",
    "#   Row 0 (a, b, c): For structural_adoption_percent (with a color scale).\n",
    "#   Row 1 (d, e, f): For raincatching_adoption_percent (with a color scale).\n",
    "#\n",
    "# Plots:\n",
    "#   (a) X=Median Income vs Y=% Communities of Color\n",
    "#   (b) X=Median Income vs Y=% No Advanced Degree\n",
    "#   (c) X=% Communities of Color vs Y=% No Advanced Degree\n",
    "#   (d) X=Median Income vs Y=% Communities of Color\n",
    "#   (e) X=Median Income vs Y=% No Advanced Degree\n",
    "#   (f) X=% Communities of Color vs Y=% No Advanced Degree\n",
    "#\n",
    "# Horizontal colorbars at the bottom of each row. Each subplot also has\n",
    "# dashed red lines marking the average for X and Y variables.\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Define two behaviors for the rows\n",
    "behaviors = [\n",
    "    (\"structural\", \"structural_adoption_percent\"),      # Row 0: a, b, c\n",
    "    (\"raincatching\", \"raincatching_adoption_percent\")   # Row 1: d, e, f\n",
    "]\n",
    "\n",
    "# Define normalization for each row's color scale\n",
    "norm_top = mpl.colors.Normalize(vmin=0, vmax=6)      # For structural row\n",
    "norm_bottom = mpl.colors.Normalize(vmin=0, vmax=18)  # For raincatching row\n",
    "\n",
    "# Define two colormaps (feel free to adjust)\n",
    "cmap_top = plt.cm.viridis_r\n",
    "cmap_bottom = plt.cm.plasma_r\n",
    "\n",
    "# Create a 2x3 subplot grid\n",
    "fig, axs = plt.subplots(2, 3, figsize=(24, 16), dpi=1200, facecolor='white')\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "# Subplot labels for top row and bottom row\n",
    "subplot_labels = [\n",
    "    ['(a)', '(b)', '(c)'],\n",
    "    ['(d)', '(e)', '(f)']\n",
    "]\n",
    "\n",
    "# Precompute some averages for dashed lines\n",
    "avg_income = plot_df['median_income_thousands'].mean()\n",
    "avg_color = plot_df['percent_communities_of_color'].mean()\n",
    "avg_no_adv = plot_df['percent_no_adv_degree'].mean()\n",
    "\n",
    "# We'll define the variable pairs for each column:\n",
    "#  0th column: X=Median Income, Y=Communities of Color\n",
    "#  1st column: X=Median Income, Y=No Adv Degree\n",
    "#  2nd column: X=Communities of Color, Y=No Adv Degree\n",
    "# Each \"row\" we do the same pairs, but color them by different adoption columns.\n",
    "\n",
    "pairs = [\n",
    "    ('median_income_thousands', 'percent_communities_of_color'),\n",
    "    ('median_income_thousands', 'percent_no_adv_degree'),\n",
    "    ('percent_communities_of_color', 'percent_no_adv_degree')\n",
    "]\n",
    "\n",
    "# We'll store references to the hexbin objects for colorbars\n",
    "hb_top = None\n",
    "hb_bottom = None\n",
    "\n",
    "for row, (behavior, adoption_col) in enumerate(behaviors):\n",
    "    # Determine which normalization and colormap to use\n",
    "    if row == 0:\n",
    "        norm = norm_top\n",
    "        current_cmap = cmap_top\n",
    "    else:\n",
    "        norm = norm_bottom\n",
    "        current_cmap = cmap_bottom\n",
    "\n",
    "    for col, (x_var, y_var) in enumerate(pairs):\n",
    "        ax = axs[row, col]\n",
    "\n",
    "        # Create the hexbin plot\n",
    "        # We color the hex cells by the average ADOPTION (using np.mean).\n",
    "        hb = ax.hexbin(\n",
    "            plot_df[x_var],\n",
    "            plot_df[y_var],\n",
    "            C=plot_df[adoption_col],\n",
    "            reduce_C_function=np.mean,\n",
    "            gridsize=30,\n",
    "            cmap=current_cmap,\n",
    "            norm=norm,\n",
    "            alpha=0.9,\n",
    "            edgecolor='none'\n",
    "        )\n",
    "\n",
    "        # Title, axes labels, subplot label\n",
    "        ax.set_title(subplot_labels[row][col], fontsize=16, loc='center', pad=20)\n",
    "\n",
    "        # X/Y labels (customize as needed):\n",
    "        if x_var == 'median_income_thousands':\n",
    "            ax.set_xlabel(\"Median Income ($1000/yr)\", fontsize=14)\n",
    "            ax.set_xlim(0, 200)  # If desired\n",
    "        elif x_var == 'percent_communities_of_color':\n",
    "            ax.set_xlabel(\"Percent Communities of Color\", fontsize=14)\n",
    "            ax.set_xlim(0, 100)\n",
    "\n",
    "        if y_var == 'percent_communities_of_color':\n",
    "            ax.set_ylabel(\"Percent Communities of Color\", fontsize=14)\n",
    "            ax.set_ylim(0, 100)\n",
    "        elif y_var == 'percent_no_adv_degree':\n",
    "            ax.set_ylabel(\"Percent No Advanced Degree\", fontsize=14)\n",
    "            ax.set_ylim(0, 100)\n",
    "\n",
    "        # Add dotted lines for the average x and y\n",
    "        # We'll use the precomputed means for the relevant column\n",
    "        if x_var == 'median_income_thousands':\n",
    "            ax.axvline(avg_income, color='red', linestyle='--', linewidth=1)\n",
    "        elif x_var == 'percent_communities_of_color':\n",
    "            ax.axvline(avg_color, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "        if y_var == 'percent_communities_of_color':\n",
    "            ax.axhline(avg_color, color='red', linestyle='--', linewidth=1)\n",
    "        elif y_var == 'percent_no_adv_degree':\n",
    "            ax.axhline(avg_no_adv, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "        # Remove grid lines, hide top & right spines\n",
    "        ax.grid(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "\n",
    "        # Store references for colorbars\n",
    "        if row == 0:\n",
    "            hb_top = hb  # Only the last subplot in the row matters for colorbar\n",
    "        else:\n",
    "            hb_bottom = hb\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Add Colorbars\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Top colorbar (row 0): structural adoption\n",
    "cbar_top = fig.colorbar(\n",
    "    mpl.cm.ScalarMappable(norm=norm_top, cmap=cmap_top),\n",
    "    ax=axs[0, :],\n",
    "    orientation='horizontal',\n",
    "    pad=0.2,\n",
    "    fraction=0.07\n",
    ")\n",
    "cbar_top.set_label(\"Structural Adoption Percentage (%)\", fontsize=14)\n",
    "cbar_top.ax.tick_params(labelsize=12)\n",
    "\n",
    "# Bottom colorbar (row 1): raincatching adoption\n",
    "cbar_bottom = fig.colorbar(\n",
    "    mpl.cm.ScalarMappable(norm=norm_bottom, cmap=cmap_bottom),\n",
    "    ax=axs[1, :],\n",
    "    orientation='horizontal',\n",
    "    pad=0.2,\n",
    "    fraction=0.07\n",
    ")\n",
    "cbar_bottom.set_label(\"Raincatching Adoption Percentage (%)\", fontsize=14)\n",
    "cbar_bottom.ax.tick_params(labelsize=12)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Save & Show\n",
    "# ---------------------------------------------------\n",
    "\n",
    "save_directory = 'yourlocation/Shapefiles/New folder/'\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "filename_png = 'hexbin_adoption_2x3.png'\n",
    "filename_svg = 'hexbin_adoption_2x3.svg'\n",
    "\n",
    "save_path_png = os.path.join(save_directory, filename_png)\n",
    "save_path_svg = os.path.join(save_directory, filename_svg)\n",
    "\n",
    "plt.savefig(save_path_png, dpi=1200, bbox_inches='tight')\n",
    "plt.savefig(save_path_svg, dpi=1200, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8483ea7-8b55-4fa0-8117-2bdc056d7cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652234a-da1b-4c03-a28d-c392188d4a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d37bd-9932-47db-8ab2-1401967142b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8911e6-0cb9-4618-a58c-4c326f4859fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as pe  # For text outline if needed\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# ----- STEP 1: Set Up the Style and Figure -----\n",
    "plt.style.use('classic')  # Apply classic theme\n",
    "\n",
    "# Create a 2x2 subplot with increased DPI for better on-screen resolution\n",
    "fig, ax = plt.subplots(2, 2, figsize=(22, 12), facecolor='white', dpi=500)  # Increased DPI for better resolution\n",
    "fig.patch.set_facecolor('white')  # Set figure background to white\n",
    "\n",
    "# ----- STEP 2: Define District Orders -----\n",
    "# Original plotting order (as per adoption_over_time_district)\n",
    "original_district_ids = list(adoption_over_time_district.keys())\n",
    "\n",
    "# Sorted district IDs for the legend (numerically sorted)\n",
    "# Assuming district IDs are integers or strings representing integers\n",
    "sorted_district_ids = sorted(original_district_ids, key=lambda x: int(x))\n",
    "\n",
    "# ----- STEP 3: Define Colors for Each District Based on Sorted Order -----\n",
    "colors = plt.cm.tab10.colors  # Get a list of distinct colors (tab10 has 10 colors)\n",
    "color_dict = {district_id: colors[i % len(colors)] for i, district_id in enumerate(sorted_district_ids)}\n",
    "\n",
    "# ----- STEP 4: Plot Structural Adoption Counts -----\n",
    "for district_id in original_district_ids:\n",
    "    ax[0, 0].plot(\n",
    "        range(len(adoption_over_time_district[district_id][\"structural_count\"])),\n",
    "        adoption_over_time_district[district_id][\"structural_count\"],\n",
    "        label=f'District {district_id}',  # Labels are not used in individual legends\n",
    "        color=color_dict[district_id],\n",
    "        linewidth=2\n",
    "    )\n",
    "ax[0, 0].set_title(\"(a) Structural Adoption Count Over Time\", fontsize=16)\n",
    "ax[0, 0].set_xlabel(\"Steps\", fontsize=14)\n",
    "ax[0, 0].set_ylabel(\"Adopted Households\", fontsize=14)\n",
    "ax[0, 0].grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "ax[0, 0].spines['top'].set_visible(False)\n",
    "ax[0, 0].spines['right'].set_visible(False)\n",
    "\n",
    "# ----- STEP 5: Plot Raincatching Adoption Counts -----\n",
    "for district_id in original_district_ids:\n",
    "    ax[0, 1].plot(\n",
    "        range(len(adoption_over_time_district[district_id][\"raincatching_count\"])),\n",
    "        adoption_over_time_district[district_id][\"raincatching_count\"],\n",
    "        label=f'District {district_id}',\n",
    "        color=color_dict[district_id],\n",
    "        linewidth=2\n",
    "    )\n",
    "ax[0, 1].set_title(\"(b) Raincatching Adoption Count Over Time\", fontsize=16)\n",
    "ax[0, 1].set_xlabel(\"Steps\", fontsize=14)\n",
    "ax[0, 1].set_ylabel(\"Adopted Households\", fontsize=14)\n",
    "ax[0, 1].grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "ax[0, 1].spines['top'].set_visible(False)\n",
    "ax[0, 1].spines['right'].set_visible(False)\n",
    "\n",
    "# ----- STEP 6: Plot Structural Adoption Percentages -----\n",
    "for district_id in original_district_ids:\n",
    "    ax[1, 0].plot(\n",
    "        range(len(adoption_over_time_district[district_id][\"structural\"])),\n",
    "        adoption_over_time_district[district_id][\"structural\"],\n",
    "        label=f'District {district_id}',\n",
    "        color=color_dict[district_id],\n",
    "        linewidth=2\n",
    "    )\n",
    "ax[1, 0].set_title(\"(c) Structural Adoption Percentage Over Time\", fontsize=16)\n",
    "ax[1, 0].set_xlabel(\"Steps\", fontsize=14)\n",
    "ax[1, 0].set_ylabel(\"Adoption (%)\", fontsize=14)\n",
    "ax[1, 0].grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "ax[1, 0].spines['top'].set_visible(False)\n",
    "ax[1, 0].spines['right'].set_visible(False)\n",
    "\n",
    "# ----- STEP 7: Plot Raincatching Adoption Percentages -----\n",
    "for district_id in original_district_ids:\n",
    "    ax[1, 1].plot(\n",
    "        range(len(adoption_over_time_district[district_id][\"raincatching\"])),\n",
    "        adoption_over_time_district[district_id][\"raincatching\"],\n",
    "        label=f'District {district_id}',\n",
    "        color=color_dict[district_id],\n",
    "        linewidth=2\n",
    "    )\n",
    "ax[1, 1].set_title(\"(d) Raincatching Adoption Percentage Over Time\", fontsize=16)\n",
    "ax[1, 1].set_xlabel(\"Steps\", fontsize=14)\n",
    "ax[1, 1].set_ylabel(\"Adoption (%)\", fontsize=14)\n",
    "ax[1, 1].grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "ax[1, 1].spines['top'].set_visible(False)\n",
    "ax[1, 1].spines['right'].set_visible(False)\n",
    "\n",
    "# ----- STEP 8: Create a Single, Ordered Legend at the Bottom -----\n",
    "# Create custom legend handles based on sorted_district_ids\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color=color_dict[d_id], lw=2, label=f'District {d_id}')\n",
    "    for d_id in sorted_district_ids\n",
    "]\n",
    "\n",
    "# Add the legend to the bottom center of the figure with increased fontsize\n",
    "fig.legend(\n",
    "    handles=legend_elements,\n",
    "    loc='lower center',\n",
    "    ncol=min(len(sorted_district_ids), 10),  # Adjust number of columns based on districts\n",
    "    fontsize=14,  # Increased fontsize for better readability\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "# Adjust layout to make space for the legend\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 1])  # Leave space at the bottom for the legend\n",
    "\n",
    "# ----- STEP 9: Save and Display the Figure -----\n",
    "# Define the directory to save the figure\n",
    "save_directory = 'C:/Users/ka34292/Desktop/Agent Based/Figures'\n",
    "os.makedirs(save_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Define the filename (customize as needed)\n",
    "filename_png = 'adoption_over_time_2x2_plots.png'\n",
    "filename_svg = 'adoption_over_time_2x2_plots.svg'  # Vector format for high resolution\n",
    "\n",
    "# Full path to save the figure\n",
    "save_path_png = os.path.join(save_directory, filename_png)\n",
    "save_path_svg = os.path.join(save_directory, filename_svg)\n",
    "\n",
    "# Save the figure in PNG format with higher DPI\n",
    "plt.savefig(save_path_png, dpi=500, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcff09-b3db-4294-8bfc-7651bad4d364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf3a48f-402e-450b-9df7-26d257d2278e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06ddbd-e648-4fe9-b964-eacdc2ddc916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495c18a-8e30-4a4a-b9cf-d4acb8a8009d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b61e63b-247a-4504-90ec-a551540ae37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639a3d4-581b-469d-b142-2145cbc959b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FINAL TABLE OF PLOTTED VALUES -----\n",
    "\n",
    "# Initialize an empty list to collect data\n",
    "data_records = []\n",
    "\n",
    "# Iterate over each district\n",
    "for district_id in original_district_ids:\n",
    "    structural_counts = adoption_over_time_district[district_id][\"structural_count\"]\n",
    "    raincatching_counts = adoption_over_time_district[district_id][\"raincatching_count\"]\n",
    "    structural_percentages = adoption_over_time_district[district_id][\"structural\"]\n",
    "    raincatching_percentages = adoption_over_time_district[district_id][\"raincatching\"]\n",
    "    \n",
    "    # Ensure all lists have the same length\n",
    "    num_steps = min(len(structural_counts), len(raincatching_counts),\n",
    "                   len(structural_percentages), len(raincatching_percentages))\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        record = {\n",
    "            'District ID': district_id,\n",
    "            'Step': step,\n",
    "            'Structural Count': structural_counts[step],\n",
    "            'Raincatching Count': raincatching_counts[step],\n",
    "            'Structural Adoption (%)': structural_percentages[step],\n",
    "            'Raincatching Adoption (%)': raincatching_percentages[step]\n",
    "        }\n",
    "        data_records.append(record)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "final_table = pd.DataFrame(data_records)\n",
    "\n",
    "# Optional: Set display options for better readability\n",
    "pd.set_option('display.max_rows', 100)  # Adjust as needed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Display the table\n",
    "display(final_table)\n",
    "\n",
    "# ----- OPTIONAL: Save the Table to a CSV File -----\n",
    "# Define the filename\n",
    "csv_filename = 'plotted_adoption_data.csv'\n",
    "\n",
    "# Save the DataFrame to the specified directory\n",
    "save_path_csv = os.path.join(save_directory, csv_filename)\n",
    "final_table.to_csv(save_path_csv, index=False)\n",
    "\n",
    "print(f\"\\nFinal table has been saved to {save_path_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd3de5-88e4-4e93-b38f-d86a06c2bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- FINAL TABLE OF DEMOGRAPHICS AND ADOPTION PERCENTAGES -----\n",
    "\n",
    "# Ensure that 'plot_df' has been created by the previous code\n",
    "if 'plot_df' in locals():\n",
    "    # Select relevant columns for the table\n",
    "    # Exclude geometry columns to ensure compatibility with CSV format\n",
    "    # You can adjust the columns as needed based on your requirements\n",
    "    table_columns = [\n",
    "        'GEOID',\n",
    "        'percent_non_white',\n",
    "        'median_income_thousands',\n",
    "        'percent_no_adv_degree',\n",
    "        'structural_adoption_percent',\n",
    "        'raincatching_adoption_percent'\n",
    "    ]\n",
    "    \n",
    "    # Check if all desired columns exist in plot_df\n",
    "    missing_columns = set(table_columns) - set(plot_df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The following required columns are missing in 'plot_df': {missing_columns}\")\n",
    "    \n",
    "    # Create the final table by selecting the desired columns\n",
    "    final_table = plot_df[table_columns].copy()\n",
    "    \n",
    "    # Optional: Rename columns for better readability\n",
    "    final_table.rename(columns={\n",
    "        'GEOID': 'Tract GEOID',\n",
    "        'percent_non_white': 'Percent Non-White (%)',\n",
    "        'median_income_thousands': 'Median Income ($1000/yr)',\n",
    "        'percent_no_adv_degree': 'Percent No Advanced Degree (%)',\n",
    "        'structural_adoption_percent': 'Structural Adoption (%)',\n",
    "        'raincatching_adoption_percent': 'Raincatching Adoption (%)'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # Optional: Set display options for better readability in environments like Jupyter Notebook\n",
    "    pd.set_option('display.max_rows', 100)       # Adjust as needed\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.float_format', '{:.2f}'.format)  # Format floats to two decimal places\n",
    "    \n",
    "    # Display the table\n",
    "    print(\"Final Table of Demographics and Adoption Percentages:\")\n",
    "    display(final_table)\n",
    "    \n",
    "    # ----- OPTIONAL: Save the Table to a CSV File -----\n",
    "    # Define the filename\n",
    "    csv_filename_final_table = 'demographics_and_adoption_percentages.csv'\n",
    "    \n",
    "    # Define the full path to save the CSV file\n",
    "    save_path_csv_final_table = os.path.join(save_directory, csv_filename_final_table)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file without the index\n",
    "    final_table.to_csv(save_path_csv_final_table, index=False)\n",
    "    \n",
    "    print(f\"\\nFinal table has been saved to {save_path_csv_final_table}\")\n",
    "else:\n",
    "    print(\"Error: 'plot_df' DataFrame does not exist. Please ensure that the previous code has been executed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a99093c-92a4-4d33-b19d-75497508bf52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dad789-f14f-4a62-b530-72b06e22293e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78aaed77-2910-4762-be30-59179157c568",
   "metadata": {},
   "source": [
    "### Latent Varibles dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e4e6a-ab0d-4410-801d-4b7c594f91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ----- STEP 1: Set Up the Style and Color Mapping -----\n",
    "plt.style.use('classic')  # Apply classic theme\n",
    "\n",
    "# Create a directory to save figures if it doesn't exist\n",
    "save_directory = 'C:/Users/ka34292/Desktop/Agent Based/Figures'\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Define behaviors to plot\n",
    "behaviors = ['structural', 'raincatching']\n",
    "\n",
    "# Define latent variables to plot for each behavior\n",
    "latent_variables_behavior = {\n",
    "    'structural': ['attitude', 'awareness', 'personal_norms', 'social_capital'],\n",
    "    'raincatching': ['attitude', 'awareness', 'personal_norms', 'social_capital']\n",
    "}\n",
    "\n",
    "# Get all unique districts sorted numerically from the model\n",
    "district_ids = sorted(model.district_ids, key=lambda x: int(x))\n",
    "sorted_district_ids = district_ids\n",
    "\n",
    "# Define a color palette based on sorted districts\n",
    "colors = plt.cm.tab10.colors  # tab10 has 10 distinct colors\n",
    "color_dict = {district_id: colors[i % len(colors)] for i, district_id in enumerate(sorted_district_ids)}\n",
    "\n",
    "# Define behavior score columns (if needed for plotting behavior scores)\n",
    "behavior_score_columns = {\n",
    "    'structural': 'behavior_structural',\n",
    "    'raincatching': 'behavior_raincatching'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54961c-c25d-4ac8-ad95-008fad2eee3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05d182-4101-466f-a5f2-f3aa27cacea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43febe-14ec-44be-8153-4fc585cc2194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a212a-abf9-4da0-87ba-bae451a1b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "def plot_latent_variable_time_series_with_95ci(\n",
    "    model, behavior, adoption_status, color_dict, save_directory, sorted_district_ids\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the time series of latent variables across all agents in each district,\n",
    "    with a shaded area representing the 95% confidence interval of the mean based on the central 90% of the data.\n",
    "\n",
    "    Parameters:\n",
    "        model (Model): The GSIAdoptionModel with agents and latent variable histories.\n",
    "        behavior (str): The behavior type ('structural' or 'raincatching').\n",
    "        adoption_status (str): The adoption status ('Adopter' or 'Non-Adopter').\n",
    "        color_dict (dict): Dictionary mapping district IDs to colors.\n",
    "        save_directory (str): Directory to save the plots.\n",
    "        sorted_district_ids (list): List of sorted district IDs.\n",
    "    \"\"\"\n",
    "    # Define latent variables for this behavior\n",
    "    latent_variables = ['attitude', 'awareness', 'personal_norms', 'social_capital']\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(18, 12), facecolor='white', dpi=500, sharex=True)\n",
    "    fig.suptitle(f\"Latent Variable Time Series for {adoption_status}s ({behavior.capitalize()})\", fontsize=20)\n",
    "\n",
    "    # Define subplot labels\n",
    "    subplot_labels = ['(a)', '(b)', '(c)', '(d)']\n",
    "    label_counter = 0\n",
    "\n",
    "    # Mapping latent variables to subplot indices\n",
    "    var_subplot_map = {\n",
    "        latent_variables[0]: (0, 0),\n",
    "        latent_variables[1]: (0, 1),\n",
    "        latent_variables[2]: (1, 0),\n",
    "        latent_variables[3]: (1, 1)\n",
    "    }\n",
    "\n",
    "    # Z-score for 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    alpha = 1 - confidence_level\n",
    "    z_score = stats.norm.ppf(1 - alpha/2)  # 1.96 for 95% CI\n",
    "\n",
    "    for var in latent_variables:\n",
    "        ax_row, ax_col = var_subplot_map[var]\n",
    "        ax = axs[ax_row][ax_col]\n",
    "\n",
    "        for district_id in sorted_district_ids:\n",
    "            # Collect all individual time series for agents in the district and specified adoption status\n",
    "            all_time_series = []\n",
    "            for agent in model.schedule.agents:\n",
    "                if (agent.DistrictID == district_id and \n",
    "                    agent.adoption_flags[behavior] == (adoption_status == 'Adopter')):\n",
    "                    if var == 'social_capital':\n",
    "                        series = agent.latent_variables_history[var]\n",
    "                    else:\n",
    "                        series = agent.latent_variables_history[var][behavior]\n",
    "                    all_time_series.append(series)\n",
    "\n",
    "            if not all_time_series:\n",
    "                continue  # Skip if no data for this district and adoption status\n",
    "\n",
    "            # Determine the maximum length of time series\n",
    "            max_length = max(len(series) for series in all_time_series)\n",
    "            # Pad shorter series with NaN to match the maximum length\n",
    "            padded_series = [np.pad(series, (0, max_length - len(series)), constant_values=np.nan) \n",
    "                            for series in all_time_series]\n",
    "            padded_series = np.array(padded_series)\n",
    "\n",
    "            # Initialize arrays to store mean and CI bounds per time step\n",
    "            mean_series = np.zeros(max_length)\n",
    "            ci_lower = np.zeros(max_length)\n",
    "            ci_upper = np.zeros(max_length)\n",
    "\n",
    "            for t in range(max_length):\n",
    "                # Extract all values at time step t, ignoring NaNs\n",
    "                values = padded_series[:, t]\n",
    "                values = values[~np.isnan(values)]\n",
    "                \n",
    "                if len(values) == 0:\n",
    "                    mean_series[t] = np.nan\n",
    "                    ci_lower[t] = np.nan\n",
    "                    ci_upper[t] = np.nan\n",
    "                    continue\n",
    "                \n",
    "                # Trim the lowest 5% and highest 5% to focus on central 90%\n",
    "                lower_percentile = np.percentile(values, 5)\n",
    "                upper_percentile = np.percentile(values, 95)\n",
    "                trimmed_values = values[(values >= lower_percentile) & (values <= upper_percentile)]\n",
    "                \n",
    "                if len(trimmed_values) == 0:\n",
    "                    mean_series[t] = np.nan\n",
    "                    ci_lower[t] = np.nan\n",
    "                    ci_upper[t] = np.nan\n",
    "                else:\n",
    "                    # Calculate mean and standard error from trimmed data\n",
    "                    mean = np.mean(trimmed_values)\n",
    "                    std = np.std(trimmed_values, ddof=1)  # Sample SD\n",
    "                    n = len(trimmed_values)\n",
    "                    se = std / np.sqrt(n)\n",
    "                    \n",
    "                    # Calculate confidence interval\n",
    "                    ci = z_score * se\n",
    "                    mean_series[t] = mean\n",
    "                    ci_lower[t] = mean - ci\n",
    "                    ci_upper[t] = mean + ci\n",
    "\n",
    "            # Plot the confidence interval as a shaded area\n",
    "            ax.fill_between(range(max_length), ci_lower, ci_upper, \n",
    "                            color=color_dict[district_id], alpha=0.2)\n",
    "\n",
    "            # Plot the mean as a thick line\n",
    "            ax.plot(mean_series, color=color_dict[district_id], linewidth=2, label=f'District {district_id}')\n",
    "\n",
    "        # Set title and labels\n",
    "        var_name = var.replace(\"_\", \" \").capitalize()\n",
    "        ax.set_title(f\"{subplot_labels[label_counter]} Mean {var_name}\", fontsize=16)\n",
    "        ax.set_xlabel(\"Steps\", fontsize=14)\n",
    "        ax.set_ylabel(f\"Mean {var_name}\", fontsize=14)\n",
    "        ax.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        label_counter += 1\n",
    "\n",
    "    # Create a custom legend with Line2D elements (without 'Mean' and without CI patch)\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color=color_dict[district_id], lw=2, label=f'District {district_id}')\n",
    "        for district_id in sorted_district_ids if district_id in color_dict\n",
    "    ]\n",
    "\n",
    "    fig.legend(handles=legend_elements, loc='lower center', ncol=min(len(sorted_district_ids), 10), \n",
    "               fontsize=14, frameon=False)\n",
    "\n",
    "    # Adjust layout to make space for the legend\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 1])  # Leave space at the bottom for the legend\n",
    "\n",
    "    # Save the figure\n",
    "    filename_png = f\"{behavior.lower()}_{adoption_status.lower()}_latent_variables_time_series_95ci.png\"\n",
    "    save_path_png = os.path.join(save_directory, filename_png)\n",
    "    plt.savefig(save_path_png, dpi=600, bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# Example of how to use the function\n",
    "for behavior in ['structural', 'raincatching']:\n",
    "    plot_latent_variable_time_series_with_95ci(\n",
    "        model=model,\n",
    "        behavior=behavior,\n",
    "        adoption_status='Adopter',\n",
    "        color_dict=color_dict,\n",
    "        save_directory=save_directory,\n",
    "        sorted_district_ids=sorted_district_ids\n",
    "    )\n",
    "\n",
    "\n",
    "############\n",
    "#The solid lines depict the mean values of [Latent Variable] over time for each district, while the shaded\n",
    "# regions represent the 95% confidence intervals around the mean. The confidence intervals indicate the\n",
    "# range within which the true mean is expected to lie with 95% certainty, accounting for sample\n",
    "# variability. This visualization allows for the comparison of [Latent Variable] trends \n",
    "# and the reliability of these estimates across different districts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63374917-0577-425a-a875-0cecd25276db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "def plot_latent_variable_time_series_with_95ci_non_adopters(\n",
    "    model, behavior, color_dict, save_directory, sorted_district_ids\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the time series of latent variables across all Non-Adopter agents in each district,\n",
    "    with a shaded area representing the 95% confidence interval of the mean based on the central 90% of the data.\n",
    "\n",
    "    Parameters:\n",
    "        model (Model): The GSIAdoptionModel with agents and latent variable histories.\n",
    "        behavior (str): The behavior type ('structural' or 'raincatching').\n",
    "        color_dict (dict): Dictionary mapping district IDs to colors.\n",
    "        save_directory (str): Directory to save the plots.\n",
    "        sorted_district_ids (list): List of sorted district IDs.\n",
    "    \"\"\"\n",
    "    # Define latent variables for this behavior\n",
    "    latent_variables = ['attitude', 'awareness', 'personal_norms', 'social_capital']\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(18, 12), facecolor='white', dpi=500, sharex=True)\n",
    "    fig.suptitle(f\"Latent Variable Time Series for Non-Adopters ({behavior.capitalize()})\", fontsize=20)\n",
    "\n",
    "    # Define subplot labels\n",
    "    subplot_labels = ['(a)', '(b)', '(c)', '(d)']\n",
    "    label_counter = 0\n",
    "\n",
    "    # Mapping latent variables to subplot indices\n",
    "    var_subplot_map = {\n",
    "        latent_variables[0]: (0, 0),\n",
    "        latent_variables[1]: (0, 1),\n",
    "        latent_variables[2]: (1, 0),\n",
    "        latent_variables[3]: (1, 1)\n",
    "    }\n",
    "\n",
    "    # Z-score for 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    alpha = 1 - confidence_level\n",
    "    z_score = stats.norm.ppf(1 - alpha/2)  # 1.96 for 95% CI\n",
    "\n",
    "    for var in latent_variables:\n",
    "        ax_row, ax_col = var_subplot_map[var]\n",
    "        ax = axs[ax_row][ax_col]\n",
    "\n",
    "        for district_id in sorted_district_ids:\n",
    "            # Collect all individual time series for Non-Adopters in the district\n",
    "            all_time_series = []\n",
    "            for agent in model.schedule.agents:\n",
    "                if agent.DistrictID == district_id and not agent.adoption_flags[behavior]:  # Only Non-Adopters\n",
    "                    if var == 'social_capital':\n",
    "                        series = agent.latent_variables_history[var]\n",
    "                    else:\n",
    "                        series = agent.latent_variables_history[var][behavior]\n",
    "                    all_time_series.append(series)\n",
    "\n",
    "            if not all_time_series:\n",
    "                continue  # Skip if no data for this district and behavior\n",
    "\n",
    "            # Determine the maximum length of time series\n",
    "            max_length = max(len(series) for series in all_time_series)\n",
    "            # Pad shorter series with NaN to match the maximum length\n",
    "            padded_series = [\n",
    "                np.pad(series, (0, max_length - len(series)), constant_values=np.nan) \n",
    "                for series in all_time_series\n",
    "            ]\n",
    "            padded_series = np.array(padded_series)\n",
    "\n",
    "            # Initialize arrays to store mean and CI bounds per time step\n",
    "            mean_series = np.zeros(max_length)\n",
    "            ci_lower = np.zeros(max_length)\n",
    "            ci_upper = np.zeros(max_length)\n",
    "\n",
    "            for t in range(max_length):\n",
    "                # Extract all values at time step t, ignoring NaNs\n",
    "                values = padded_series[:, t]\n",
    "                values = values[~np.isnan(values)]\n",
    "                \n",
    "                if len(values) == 0:\n",
    "                    mean_series[t] = np.nan\n",
    "                    ci_lower[t] = np.nan\n",
    "                    ci_upper[t] = np.nan\n",
    "                    continue\n",
    "                \n",
    "                # Trim the lowest 5% and highest 5% to focus on central 90%\n",
    "                lower_percentile = np.percentile(values, 5)\n",
    "                upper_percentile = np.percentile(values, 95)\n",
    "                trimmed_values = values[(values >= lower_percentile) & (values <= upper_percentile)]\n",
    "                \n",
    "                if len(trimmed_values) == 0:\n",
    "                    mean_series[t] = np.nan\n",
    "                    ci_lower[t] = np.nan\n",
    "                    ci_upper[t] = np.nan\n",
    "                else:\n",
    "                    # Calculate mean and standard error from trimmed data\n",
    "                    mean = np.mean(trimmed_values)\n",
    "                    std = np.std(trimmed_values, ddof=1)  # Sample SD\n",
    "                    n = len(trimmed_values)\n",
    "                    se = std / np.sqrt(n)\n",
    "                    \n",
    "                    # Calculate confidence interval\n",
    "                    ci = z_score * se\n",
    "                    mean_series[t] = mean\n",
    "                    ci_lower[t] = mean - ci\n",
    "                    ci_upper[t] = mean + ci\n",
    "\n",
    "            # Plot the confidence interval as a shaded area\n",
    "            ax.fill_between(\n",
    "                range(max_length), \n",
    "                ci_lower, \n",
    "                ci_upper, \n",
    "                color=color_dict[district_id], \n",
    "                alpha=0.2\n",
    "            )\n",
    "\n",
    "            # Plot the mean as a thick line\n",
    "            ax.plot(\n",
    "                mean_series, \n",
    "                color=color_dict[district_id], \n",
    "                linewidth=2, \n",
    "                label=f'District {district_id}'\n",
    "            )\n",
    "\n",
    "        # Set title and labels\n",
    "        var_name = var.replace(\"_\", \" \").capitalize()\n",
    "        ax.set_title(f\"{subplot_labels[label_counter]} Mean {var_name}\", fontsize=16)\n",
    "        ax.set_xlabel(\"Steps\", fontsize=14)\n",
    "        ax.set_ylabel(f\"Mean {var_name}\", fontsize=14)\n",
    "        ax.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        label_counter += 1\n",
    "\n",
    "    # Create a custom legend with Line2D elements (without 'Mean' and without CI patch)\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color=color_dict[district_id], lw=2, label=f'District {district_id}')\n",
    "        for district_id in sorted_district_ids if district_id in color_dict\n",
    "    ]\n",
    "\n",
    "    fig.legend(\n",
    "        handles=legend_elements, \n",
    "        loc='lower center', \n",
    "        ncol=min(len(sorted_district_ids), 10), \n",
    "        fontsize=14, \n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    # Adjust layout to make space for the legend\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 1])  # Leave space at the bottom for the legend\n",
    "\n",
    "    # Save the figure\n",
    "    filename_png = f\"{behavior.lower()}_non_adopter_latent_variables_time_series_95ci.png\"\n",
    "    save_path_png = os.path.join(save_directory, filename_png)\n",
    "    plt.savefig(save_path_png, dpi=600, bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# Example of how to use the function for non-adopters\n",
    "for behavior in ['structural', 'raincatching']:\n",
    "    plot_latent_variable_time_series_with_95ci_non_adopters(\n",
    "        model=model,\n",
    "        behavior=behavior,\n",
    "        color_dict=color_dict,\n",
    "        save_directory=save_directory,\n",
    "        sorted_district_ids=sorted_district_ids\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a0572-ce5d-4028-a290-270fa88dcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "def plot_average_behavior_scores_with_95ci(\n",
    "    model, color_dict, save_directory, sorted_district_ids\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the time series of average behavior scores for adopters and non-adopters,\n",
    "    with a shaded area representing the 95% confidence interval of the mean.\n",
    "\n",
    "    Parameters:\n",
    "        model (Model): The GSIAdoptionModel with agents and behavior histories.\n",
    "        color_dict (dict): Dictionary mapping district IDs to colors.\n",
    "        save_directory (str): Directory to save the plots.\n",
    "        sorted_district_ids (list): List of sorted district IDs.\n",
    "    \"\"\"\n",
    "    # Define behaviors for this analysis\n",
    "    behaviors = ['structural', 'raincatching']\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(18, 12), facecolor='white', dpi=500, sharex=True)\n",
    "    fig.suptitle(\"Average Behavior Scores with 95% CI\", fontsize=20)\n",
    "\n",
    "    # Define subplot labels\n",
    "    subplot_labels = ['(a)', '(b)', '(c)', '(d)']\n",
    "    label_counter = 0\n",
    "\n",
    "    # Z-score for 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    alpha = 1 - confidence_level\n",
    "    z_score = stats.norm.ppf(1 - alpha/2)  # 1.96 for 95% CI\n",
    "\n",
    "    # Plot each behavior and adoption status\n",
    "    for behavior_idx, behavior in enumerate(behaviors):\n",
    "        for adoption_status_idx, adoption_status in enumerate(['Adopter', 'Non-Adopter']):\n",
    "            ax = axs[behavior_idx][adoption_status_idx]\n",
    "\n",
    "            for district_id in sorted_district_ids:\n",
    "                # Collect all individual time series for agents in the district and specified adoption status\n",
    "                all_time_series = []\n",
    "                for agent in model.schedule.agents:\n",
    "                    if agent.DistrictID == district_id and agent.adoption_flags[behavior] == (adoption_status == 'Adopter'):\n",
    "                        series = agent.latent_variables_history['behavior'][behavior]\n",
    "                        all_time_series.append(series)\n",
    "\n",
    "                if not all_time_series:\n",
    "                    continue  # Skip if no data for this district and adoption status\n",
    "\n",
    "                # Determine the maximum length of time series\n",
    "                max_length = max(len(series) for series in all_time_series)\n",
    "                # Pad shorter series with NaN to match the maximum length\n",
    "                padded_series = [np.pad(series, (0, max_length - len(series)), constant_values=np.nan) for series in all_time_series]\n",
    "                padded_series = np.array(padded_series)\n",
    "\n",
    "                # Initialize arrays to store mean and CI bounds per time step\n",
    "                mean_series = np.zeros(max_length)\n",
    "                ci_lower = np.zeros(max_length)\n",
    "                ci_upper = np.zeros(max_length)\n",
    "\n",
    "                for t in range(max_length):\n",
    "                    # Extract all values at time step t, ignoring NaNs\n",
    "                    values = padded_series[:, t]\n",
    "                    values = values[~np.isnan(values)]\n",
    "                    \n",
    "                    if len(values) == 0:\n",
    "                        mean_series[t] = np.nan\n",
    "                        ci_lower[t] = np.nan\n",
    "                        ci_upper[t] = np.nan\n",
    "                        continue\n",
    "                    \n",
    "                    # Trim the lowest 5% and highest 5% to focus on central 90%\n",
    "                    lower_percentile = np.percentile(values, 5)\n",
    "                    upper_percentile = np.percentile(values, 95)\n",
    "                    trimmed_values = values[(values >= lower_percentile) & (values <= upper_percentile)]\n",
    "                    \n",
    "                    if len(trimmed_values) == 0:\n",
    "                        mean_series[t] = np.nan\n",
    "                        ci_lower[t] = np.nan\n",
    "                        ci_upper[t] = np.nan\n",
    "                    else:\n",
    "                        # Calculate mean and standard error from trimmed data\n",
    "                        mean = np.mean(trimmed_values)\n",
    "                        std = np.std(trimmed_values, ddof=1)  # Sample SD\n",
    "                        n = len(trimmed_values)\n",
    "                        se = std / np.sqrt(n)\n",
    "                        \n",
    "                        # Calculate confidence interval\n",
    "                        ci = z_score * se\n",
    "                        mean_series[t] = mean\n",
    "                        ci_lower[t] = mean - ci\n",
    "                        ci_upper[t] = mean + ci\n",
    "\n",
    "                #\n",
    "                # Plot the confidence interval as a shaded area\n",
    "                ax.fill_between(range(max_length), ci_lower, ci_upper, color=color_dict[district_id], alpha=0.2)\n",
    "\n",
    "                # Plot the mean as a thick line\n",
    "                ax.plot(mean_series, color=color_dict[district_id], linewidth=2, label=f'District {district_id}')\n",
    "                \n",
    "\n",
    "            # Set title and labels\n",
    "            ax.set_title(f\"{subplot_labels[label_counter]} Mean {behavior.capitalize()} Behavior - {adoption_status}\", fontsize=16)\n",
    "            ax.set_xlabel(\"Steps\", fontsize=14)\n",
    "            ax.set_ylabel(\"Mean Behavior Score\", fontsize=14)\n",
    "            ax.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "            label_counter += 1\n",
    "\n",
    "    # Create a custom legend with Line2D elements (without 'Mean' and without CI patch)\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color=color_dict[district_id], lw=2, label=f'District {district_id}')\n",
    "        for district_id in sorted_district_ids if district_id in color_dict\n",
    "    ]\n",
    "\n",
    "    fig.legend(handles=legend_elements, loc='lower center', ncol=min(len(sorted_district_ids), 10), \n",
    "               fontsize=14, frameon=False)\n",
    "\n",
    "    # Adjust layout to make space for the legend\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 1])  # Leave space at the bottom for the legend\n",
    "\n",
    "    # Save the figure\n",
    "    filename_png = f\"average_behavior_scores_time_series_95ci.png\"\n",
    "    save_path_png = os.path.join(save_directory, filename_png)\n",
    "    plt.savefig(save_path_png, dpi=600, bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# Example of how to use the function\n",
    "plot_average_behavior_scores_with_95ci(\n",
    "    model=model,\n",
    "    color_dict=color_dict,\n",
    "    save_directory=save_directory,\n",
    "    sorted_district_ids=sorted_district_ids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd163ff1-9941-412f-ad30-fca2a27aa174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "def plot_combined_average_behavior_scores_with_95ci(\n",
    "    model, color_dict, save_directory, sorted_district_ids\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots the combined average behavior scores for structural and rain-catching\n",
    "    practices across all agents (adopters and non-adopters) in each district,\n",
    "    with a shaded area representing the 95% confidence interval of the mean.\n",
    "\n",
    "    Parameters:\n",
    "        model (Model): The GSIAdoptionModel with agents and behavior histories.\n",
    "        color_dict (dict): Dictionary mapping district IDs to colors.\n",
    "        save_directory (str): Directory to save the plots.\n",
    "        sorted_district_ids (list): List of sorted district IDs.\n",
    "    \"\"\"\n",
    "    # Define behaviors for this analysis\n",
    "    behaviors = ['structural', 'raincatching']\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(18, 8), facecolor='white', dpi=500, sharex=True)\n",
    "    fig.suptitle(\"Combined Average Behavior Scores with 95% CI (All Agents)\", fontsize=20)\n",
    "\n",
    "    # Define subplot labels\n",
    "    subplot_labels = ['(a)', '(b)']\n",
    "    label_counter = 0\n",
    "\n",
    "    # Z-score for 95% confidence interval\n",
    "    confidence_level = 0.95\n",
    "    alpha = 1 - confidence_level\n",
    "    z_score = stats.norm.ppf(1 - alpha/2)  # 1.96 for 95% CI\n",
    "\n",
    "    # Plot each behavior, combining adopters and non-adopters\n",
    "    for behavior_idx, behavior in enumerate(behaviors):\n",
    "        ax = axs[behavior_idx]\n",
    "\n",
    "        for district_id in sorted_district_ids:\n",
    "            # Collect all individual time series for agents in the district, regardless of adoption status\n",
    "            all_time_series = []\n",
    "            for agent in model.schedule.agents:\n",
    "                if agent.DistrictID == district_id:\n",
    "                    series = agent.latent_variables_history['behavior'][behavior]\n",
    "                    all_time_series.append(series)\n",
    "\n",
    "            if not all_time_series:\n",
    "                continue  # Skip if no data for this district\n",
    "\n",
    "            # Determine the maximum length of time series\n",
    "            max_length = max(len(series) for series in all_time_series)\n",
    "            # Pad shorter series with NaN to match the maximum length\n",
    "            padded_series = [np.pad(series, (0, max_length - len(series)), constant_values=np.nan) for series in all_time_series]\n",
    "            padded_series = np.array(padded_series)\n",
    "\n",
    "            # Initialize arrays to store mean and CI bounds per time step\n",
    "            mean_series = np.zeros(max_length)\n",
    "            ci_lower = np.zeros(max_length)\n",
    "            ci_upper = np.zeros(max_length)\n",
    "\n",
    "            for t in range(max_length):\n",
    "                # Extract all values at time step t, ignoring NaNs\n",
    "                values = padded_series[:, t]\n",
    "                values = values[~np.isnan(values)]\n",
    "                \n",
    "                if len(values) == 0:\n",
    "                    mean_series[t] = np.nan\n",
    "                    ci_lower[t] = np.nan\n",
    "                    ci_upper[t] = np.nan\n",
    "                    continue\n",
    "                \n",
    "                # Trim the lowest 5% and highest 5% to focus on central 90%\n",
    "                lower_percentile = np.percentile(values, 5)\n",
    "                upper_percentile = np.percentile(values, 95)\n",
    "                trimmed_values = values[(values >= lower_percentile) & (values <= upper_percentile)]\n",
    "                \n",
    "                if len(trimmed_values) == 0:\n",
    "                    mean_series[t] = np.nan\n",
    "                    ci_lower[t] = np.nan\n",
    "                    ci_upper[t] = np.nan\n",
    "                else:\n",
    "                    # Calculate mean and standard error from trimmed data\n",
    "                    mean = np.mean(trimmed_values)\n",
    "                    std = np.std(trimmed_values, ddof=1)  # Sample SD\n",
    "                    n = len(trimmed_values)\n",
    "                    se = std / np.sqrt(n)\n",
    "                    \n",
    "                    # Calculate confidence interval\n",
    "                    ci = z_score * se\n",
    "                    mean_series[t] = mean\n",
    "                    ci_lower[t] = mean - ci\n",
    "                    ci_upper[t] = mean + ci\n",
    "\n",
    "            # Plot the confidence interval as a shaded area\n",
    "            ax.fill_between(range(max_length), ci_lower, ci_upper, color=color_dict[district_id], alpha=0.2)\n",
    "\n",
    "            # Plot the mean as a thick line\n",
    "            ax.plot(mean_series, color=color_dict[district_id], linewidth=2, label=f'District {district_id}')\n",
    "\n",
    "        # Set title and labels\n",
    "        ax.set_title(f\"{subplot_labels[label_counter]} Mean {behavior.capitalize()} Behavior (All Agents)\", fontsize=16)\n",
    "        ax.set_xlabel(\"Steps\", fontsize=14)\n",
    "        ax.set_ylabel(\"Mean Behavior Score\", fontsize=14)\n",
    "        ax.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        label_counter += 1\n",
    "\n",
    "    # Create a custom legend with Line2D elements (without 'Mean' and without CI patch)\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color=color_dict[district_id], lw=2, label=f'District {district_id}')\n",
    "        for district_id in sorted_district_ids if district_id in color_dict\n",
    "    ]\n",
    "\n",
    "    fig.legend(handles=legend_elements, loc='lower center', ncol=min(len(sorted_district_ids), 10), \n",
    "               fontsize=14, frameon=False)\n",
    "\n",
    "    # Adjust layout to make space for the legend\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 1])  # Leave space at the bottom for the legend\n",
    "\n",
    "    # Save the figure\n",
    "    filename_png = \"combined_average_behavior_scores_time_series_95ci.png\"\n",
    "    save_path_png = os.path.join(save_directory, filename_png)\n",
    "    plt.savefig(save_path_png, dpi=600, bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# Example of how to use the function\n",
    "plot_combined_average_behavior_scores_with_95ci(\n",
    "    model=model,\n",
    "    color_dict=color_dict,\n",
    "    save_directory=save_directory,\n",
    "    sorted_district_ids=sorted_district_ids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d857b-5696-4066-a967-fc695ab9a9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2666e3d9-b606-46ff-93c4-1414e7b8b3aa",
   "metadata": {},
   "source": [
    "### Calibration Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a482f1-fcfd-453f-94d7-66bf4c16a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve adoption counts from the model\n",
    "adoption_counts_df = model.datacollector.get_model_vars_dataframe()\n",
    "\n",
    "# Extract time steps\n",
    "time_steps = range(len(adoption_counts_df))\n",
    "\n",
    "# Initialize lists to store the summed adoption counts over all tracts\n",
    "structural_adoption_counts = []\n",
    "raincatching_adoption_counts = []\n",
    "structural_plan_to_action_counts = []\n",
    "raincatching_plan_to_action_counts = []\n",
    "\n",
    "# Calculate the Plan-to-Action sums only for step0\n",
    "step0_adoption_counts = adoption_counts_df.iloc[0][\"AdoptionCounts\"].values()\n",
    "\n",
    "# Ensure that AdoptionCounts is iterable and contains the expected keys\n",
    "# If AdoptionCounts is a dictionary or similar structure, adjust accordingly\n",
    "structural_plan_to_action_sum = sum(\n",
    "    adopt['structural_plan_to_action'] for adopt in step0_adoption_counts\n",
    ")\n",
    "raincatching_plan_to_action_sum = sum(\n",
    "    adopt['raincatching_plan_to_action'] for adopt in step0_adoption_counts\n",
    ")\n",
    "\n",
    "# Iterate over each time step to calculate adoption sums\n",
    "for step in time_steps:\n",
    "    # Sum the 'structural' adoption counts for the current step\n",
    "    structural_adoption_sum = sum(\n",
    "        adopt['structural'] for adopt in adoption_counts_df.iloc[step][\"AdoptionCounts\"].values()\n",
    "    )\n",
    "    \n",
    "    # Sum the 'raincatching' adoption counts for the current step\n",
    "    raincatching_adoption_sum = sum(\n",
    "        adopt['raincatching'] for adopt in adoption_counts_df.iloc[step][\"AdoptionCounts\"].values()\n",
    "    )\n",
    "    \n",
    "    # Append the summed 'structural' and 'raincatching' adoption counts\n",
    "    structural_adoption_counts.append(structural_adoption_sum)\n",
    "    raincatching_adoption_counts.append(raincatching_adoption_sum)\n",
    "    \n",
    "    # Append the constant Plan-to-Action counts from step0\n",
    "    structural_plan_to_action_counts.append(structural_plan_to_action_sum)\n",
    "    raincatching_plan_to_action_counts.append(raincatching_plan_to_action_sum)\n",
    "\n",
    "# Plot the adoption counts over time\n",
    "plt.figure(figsize=(12, 5), facecolor=\"white\")  # Set the background to white\n",
    "\n",
    "# Plot Structural Adoption Counts\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(\n",
    "    time_steps,\n",
    "    structural_adoption_counts,\n",
    "    label='Structural Adoption',\n",
    "    color='blue',\n",
    "    marker='o'\n",
    ")\n",
    "plt.plot(\n",
    "    time_steps,\n",
    "    structural_plan_to_action_counts,\n",
    "    label='Structural Plan-to-Action Adoption',\n",
    "    color='red',\n",
    "    linestyle='--',\n",
    "    marker='x'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Time Steps\", fontsize=12)\n",
    "plt.ylabel(\"Adopted Households\", fontsize=12)\n",
    "plt.legend(fontsize=10, frameon=False)  # Smaller legend font and no box\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.5)  # Light grid for readability\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Plot Raincatching Adoption Counts\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(\n",
    "    time_steps,\n",
    "    raincatching_adoption_counts,\n",
    "    label='Raincatching Adoption',\n",
    "    color='green',\n",
    "    marker='o'\n",
    ")\n",
    "plt.plot(\n",
    "    time_steps,\n",
    "    raincatching_plan_to_action_counts,\n",
    "    label='Raincatching Plan-to-Action Adoption',\n",
    "    color='orange',\n",
    "    linestyle='--',\n",
    "    marker='x'\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Time Steps\", fontsize=12)\n",
    "plt.ylabel(\"Adopted Households\", fontsize=12)\n",
    "plt.legend(fontsize=10, frameon=False)  # Smaller legend font and no box\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.5)  # Light grid for readability\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust layout for better spacing and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a65f3-7c73-4a15-a092-df7ebb776de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b20ac3d-004d-4751-ac4c-6e820cc1d5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ccd5a4-a536-44c0-847e-5d74b8759971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.lines import Line2D\n",
    "import os\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Define Variable Names and Select Variables to Plot\n",
    "# ------------------------------\n",
    "\n",
    "# Updated list of variable names excluding the removed ones\n",
    "variable_names = [\n",
    "    'Attitude', 'Awareness', 'Social Capital', 'Personal Norms', 'Income', 'Race', 'Advanced Degree',\n",
    "    'Structural Behavior Score', 'Raincatching Behavior Score',\n",
    "    'Structural Practices Potential', 'Rain Catching Potential', 'Building Area', 'Parcel Area',\n",
    "    'Structural Behavior Threshold', 'Raincatching Behavior Threshold',\n",
    "    'Rain Barrel Score', 'Cistern Score', 'Rain Garden Score',\n",
    "    'Yard Slope Change Score', 'Yard Vegetation Change Score', 'Reducing Irrigation Water Score',\n",
    "    'Price Sensitivity Rain Barrel', 'Price Sensitivity Cistern', 'Price Sensitivity Rain Garden',\n",
    "    'Price Sensitivity Yard Vegetation Change', 'Price Sensitivity Reducing Irrigation Water', 'Price Sensitivity Yard Slope Change'\n",
    "]\n",
    "\n",
    "# Define the column indices to plot (2 to 32 inclusive, excluding columns 11,12,19,20)\n",
    "excluded_columns = [11, 12, 19, 20]  # 0-based indexing\n",
    "columns_to_plot = [i for i in range(2, 33) if i not in excluded_columns]  # Columns 2 to 32, excluding specified\n",
    "\n",
    "# Ensure that the number of columns_to_plot matches the length of variable_names\n",
    "assert len(columns_to_plot) == len(variable_names), \"Mismatch between columns_to_plot and variable_names length.\"\n",
    "\n",
    "# Extract data for each variable\n",
    "# Assuming household_data is a list of lists or a 2D array where household_data[row][column]\n",
    "variables_to_plot = [\n",
    "    [row[i] for row in household_data] for i in columns_to_plot\n",
    "]\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: Set Up the Plotting Style and Figure\n",
    "# ------------------------------\n",
    "\n",
    "# Reset to default style to restore default background\n",
    "plt.style.use('default')  # This ensures no custom styles are applied\n",
    "\n",
    "# Define the number of variables to plot\n",
    "num_variables = len(variable_names)  # 27 variables\n",
    "\n",
    "# Calculate the number of rows and columns for subplots (7x4 grid)\n",
    "# 7 rows x 4 columns = 28 subplots, with 1 subplot unused (for the legend)\n",
    "n_rows = 7\n",
    "n_cols = 4\n",
    "\n",
    "# Create the figure and axes without setting facecolor to keep default background\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(24, 42), dpi=300)  # Adjust figsize as needed\n",
    "# No need to set facecolor; it uses Matplotlib's default\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axs = axs.flatten()\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Plot the Histograms with Consistent Styling\n",
    "# ------------------------------\n",
    "\n",
    "# Define colors and alpha for histograms\n",
    "hist_color = 'blue'\n",
    "hist_alpha = 0.7\n",
    "\n",
    "# Labels for subplots\n",
    "subplot_labels = [f\"({chr(97 + i)})\" for i in range(num_variables)]  # (a), (b), ..., etc.\n",
    "\n",
    "for i in range(n_rows * n_cols):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    if i < num_variables:\n",
    "        # Plot histogram for the ith variable\n",
    "        ax.hist(variables_to_plot[i], bins=20, color=hist_color, alpha=hist_alpha, edgecolor='black')\n",
    "        \n",
    "        # Set subplot title with label and variable name, removing 'Score' where specified\n",
    "        # Adjust this logic based on your specific requirements\n",
    "        if 'Score' in variable_names[i]:\n",
    "            title_var = variable_names[i].replace(' Score', '')\n",
    "        else:\n",
    "            title_var = variable_names[i]\n",
    "        \n",
    "        # Set subplot title with label and adjusted variable name\n",
    "        ax.set_title(f\"{subplot_labels[i]} {title_var}\", fontsize=16)  # Removed fontweight='bold'\n",
    "        \n",
    "        # Set x and y labels\n",
    "        ax.set_xlabel('Value', fontsize=14)\n",
    "        ax.set_ylabel('Frequency', fontsize=14)\n",
    "        \n",
    "        # Add grid lines\n",
    "        ax.grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Remove top and right spines for a cleaner look\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    elif i == n_rows * n_cols -1:\n",
    "        # This is the last subplot (28th), use it for the legend\n",
    "        # Create a custom legend\n",
    "        legend_elements = [\n",
    "            Line2D([0], [0], color=hist_color, lw=10, label='Household Data')\n",
    "        ]\n",
    "        ax.legend(\n",
    "            handles=legend_elements,\n",
    "            loc='center',\n",
    "            fontsize=14,\n",
    "            frameon=False\n",
    "        )\n",
    "        ax.axis('off')  # Hide the axes but keep the legend\n",
    "    else:\n",
    "        # Hide any other unused subplots (if any)\n",
    "        ax.axis('off')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Save and Display the Figure\n",
    "# ------------------------------\n",
    "\n",
    "# Define the directory to save the figure\n",
    "save_directory = 'C:/Users/ka34292/Desktop/Agent Based/Figures'\n",
    "os.makedirs(save_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# Define the filename (customize as needed)\n",
    "filename_png = 'household_variables_histograms.png'\n",
    "filename_svg = 'household_variables_histograms.svg'  # Vector format for high resolution\n",
    "\n",
    "# Full path to save the figure\n",
    "save_path_png = os.path.join(save_directory, filename_png)\n",
    "save_path_svg = os.path.join(save_directory, filename_svg)\n",
    "\n",
    "# Save the figure in PNG format with higher DPI\n",
    "plt.savefig(save_path_png, dpi=300, bbox_inches='tight')\n",
    "print(f\"Histograms plot saved at {save_path_png}\")\n",
    "\n",
    "# Additionally, save in SVG format for scalability\n",
    "plt.savefig(save_path_svg, format='svg', bbox_inches='tight')\n",
    "print(f\"Histograms plot saved at {save_path_svg}\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee9c72-cb06-4a0e-b686-88bd24331f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f674c18e-29a0-43ef-b104-f6b2c9f336b3",
   "metadata": {},
   "source": [
    "## Sensitivity Analyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4388e-3d3f-4eda-a8b9-1b6d8bd13f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_tract_with_most_households(households_per_tract):\n",
    "    \"\"\"\n",
    "    Select the tract that has the maximum number of households.\n",
    "\n",
    "    Args:\n",
    "        households_per_tract (dict): { tract_id: number_of_households }\n",
    "\n",
    "    Returns:\n",
    "        A tuple (chosen_tract_id, max_households)\n",
    "          - chosen_tract_id: The tract_id with the most households\n",
    "          - max_households: The number of households in that tract\n",
    "    \"\"\"\n",
    "    if not households_per_tract:\n",
    "        print(\"No data in households_per_tract, returning None.\")\n",
    "        return None, 0\n",
    "\n",
    "    # Sort by the number of households descending\n",
    "    sorted_by_households = sorted(households_per_tract.items(), key=lambda x: x[1], reverse=True)\n",
    "    # The first element in sorted_by_households is (tract_id, #households) with the max #households\n",
    "    chosen_tract_id, max_households = sorted_by_households[0]\n",
    "    return chosen_tract_id, max_households\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696aa0fb-13fb-4139-8abd-daa76d9fdaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fa883f-b063-473d-b855-b3a1a3815c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "########################################\n",
    "# 0) Optional: MASTER SEED or replicate seeds\n",
    "########################################\n",
    "# If you want one single seed for all runs:\n",
    "MASTER_SEED = 1234\n",
    "\n",
    "# Or if you want multiple replicates per scenario, define replicate_seeds = [111,222,333,...]\n",
    "# For simplicity, we'll do one run per scenario here.\n",
    "\n",
    "########################################\n",
    "# 1) run_single_tract_model\n",
    "########################################\n",
    "def run_single_tract_model(\n",
    "    tract_id,\n",
    "    tracts_gdf,\n",
    "    parcels_gdf,\n",
    "    eligible_parcels_gdf,\n",
    "    all_household_data,\n",
    "    steps=40,\n",
    "    # The parameters below:\n",
    "    self_weight=0.85,\n",
    "    adopter_weight=0.3,\n",
    "    SEM_attitude_factor=0.4,\n",
    "    SEM_awareness_factor=0.4,\n",
    "    SEM_personal_norms_factor=0.4,\n",
    "    w_similarity=0.3,\n",
    "    w_communication=0.1,\n",
    "    w_visual=0.3,\n",
    "    w_spatial=0.3,\n",
    "    network_influence_rate=0.001,\n",
    "    neighborhood_influence_rate=0.001,\n",
    "    # If you'd like to fix the seed inside each call:\n",
    "    scenario_seed=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs GSIAdoptionModel for a single tract with specified parameters.\n",
    "    Returns: dict with \"structural_perc\" and \"raincatching_perc\".\n",
    "    \n",
    "    If scenario_seed is not None, we re-seed random and numpy so that\n",
    "    every run with the same scenario_seed reproduces identical random draws.\n",
    "    \"\"\"\n",
    "\n",
    "    # If you want to fix the randomness for each scenario:\n",
    "    if scenario_seed is not None:\n",
    "        random.seed(scenario_seed)\n",
    "        np.random.seed(scenario_seed)\n",
    "\n",
    "    # 1) Filter data to single tract\n",
    "    tract_single = tracts_gdf[tracts_gdf[\"GEOID\"] == tract_id]\n",
    "    parcels_single = parcels_gdf[parcels_gdf[\"GEOID\"] == tract_id]\n",
    "    eligible_parcels_single = eligible_parcels_gdf[eligible_parcels_gdf[\"GEOID\"] == tract_id]\n",
    "\n",
    "    # 2) Filter household_data to that tract\n",
    "    single_tract_households = [h for h in all_household_data if h[0] == tract_id]\n",
    "    if len(single_tract_households) == 0:\n",
    "        print(f\"No households found in tract {tract_id}. Returning 0 adoption.\")\n",
    "        return {\"structural_perc\": 0.0, \"raincatching_perc\": 0.0}\n",
    "\n",
    "    # 3) Create mini-model\n",
    "    mini_model = GSIAdoptionModel(\n",
    "        tract_single,\n",
    "        parcels_single,\n",
    "        eligible_parcels_single,\n",
    "        single_tract_households\n",
    "    )\n",
    "\n",
    "    # 4) Overwrite agent parameters\n",
    "    for agent in mini_model.schedule.agents:\n",
    "        agent.self_weight = self_weight\n",
    "        agent.adopter_weight_nonadopter = adopter_weight\n",
    "        agent.SEM_attitude_factor = SEM_attitude_factor\n",
    "        agent.SEM_awareness_factor = SEM_awareness_factor\n",
    "        agent.SEM_personal_norms_factor = SEM_personal_norms_factor\n",
    "        agent.W_SIMILARITY = w_similarity\n",
    "        agent.W_COMMUNICATION = w_communication\n",
    "        agent.W_VISUAL = w_visual\n",
    "        agent.W_SPATIAL = w_spatial\n",
    "        agent.network_influence_rate = network_influence_rate\n",
    "        agent.neighborhood_influence_rate = neighborhood_influence_rate\n",
    "\n",
    "    # 5) Run the model for 'steps'\n",
    "    for _ in range(steps):\n",
    "        mini_model.step()\n",
    "\n",
    "    # 6) Collect final adoption\n",
    "    final_adopts = mini_model.adoption_counts.get(tract_id, {})\n",
    "    total_households = len(single_tract_households)\n",
    "    if total_households == 0:\n",
    "        return {\"structural_perc\": 0.0, \"raincatching_perc\": 0.0}\n",
    "\n",
    "    struct_count = final_adopts.get(\"structural\", 0)\n",
    "    rain_count   = final_adopts.get(\"raincatching\", 0)\n",
    "\n",
    "    struct_perc = (struct_count / total_households) * 100\n",
    "    rain_perc   = (rain_count / total_households) * 100\n",
    "\n",
    "    return {\n",
    "        \"structural_perc\": struct_perc,\n",
    "        \"raincatching_perc\": rain_perc\n",
    "    }\n",
    "\n",
    "\n",
    "########################################\n",
    "# 2) fixed_baseline_sensitivity_six_variations\n",
    "########################################\n",
    "def fixed_baseline_sensitivity_six_variations(\n",
    "    tract_id,\n",
    "    tracts,\n",
    "    parcels_with_tracts,\n",
    "    eligible_parcels,\n",
    "    household_data,\n",
    "    steps=40,\n",
    "    use_fixed_seed=True\n",
    "):\n",
    "    \"\"\"\n",
    "    For each parameter, we run 6 variations: -15%, -10%, -5%, +5%, +10%, +15%.\n",
    "    We'll compute the absolute change in structural & rain adoption from baseline.\n",
    "    \n",
    "    Returns:\n",
    "        (results, sensitivity_levels, baseline_params, baseline_struct, baseline_rain)\n",
    "    \n",
    "    If use_fixed_seed=True, we fix the same MASTER_SEED for baseline & each scenario\n",
    "    so that only parameter differences matter, not random differences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Baseline parameter dictionary\n",
    "    baseline_params = {\n",
    "        \"self_weight\": 0.85,\n",
    "        \"adopter_weight\": 0.3,\n",
    "        \"SEM_attitude_factor\": 0.4,\n",
    "        \"SEM_awareness_factor\": 0.4,\n",
    "        \"SEM_personal_norms_factor\": 0.4,\n",
    "        \"w_similarity\": 0.3,\n",
    "        \"w_communication\": 0.1,\n",
    "        \"w_visual\": 0.3,\n",
    "        \"w_spatial\": 0.3,\n",
    "        \"network_influence_rate\": 0.001,\n",
    "        \"neighborhood_influence_rate\": 0.001\n",
    "    }\n",
    "\n",
    "    # Six sensitivity levels\n",
    "    sensitivity_levels = [-0.15, -0.10, -0.05, 0.05, 0.10, 0.15]\n",
    "\n",
    "    # 1) Run single baseline\n",
    "    #    If we fix seed, do it here:\n",
    "    scenario_seed = MASTER_SEED if use_fixed_seed else None\n",
    "\n",
    "    baseline_result = run_single_tract_model(\n",
    "        tract_id=tract_id,\n",
    "        tracts_gdf=tracts,\n",
    "        parcels_gdf=parcels_with_tracts,\n",
    "        eligible_parcels_gdf=eligible_parcels,\n",
    "        all_household_data=household_data,\n",
    "        steps=steps,\n",
    "        scenario_seed=scenario_seed,   # seed if desired\n",
    "        **baseline_params\n",
    "    )\n",
    "    base_struct = baseline_result[\"structural_perc\"]\n",
    "    base_rain   = baseline_result[\"raincatching_perc\"]\n",
    "    print(f\"Baseline => struct={base_struct:.2f}%, rain={base_rain:.2f}%\")\n",
    "\n",
    "    # We'll accumulate results in a dictionary\n",
    "    results = {param: {\"structural\": [], \"raincatching\": []} for param in baseline_params}\n",
    "\n",
    "    # 2) For each parameter, run 6 variations\n",
    "    for param, base_val in baseline_params.items():\n",
    "        for factor in sensitivity_levels:\n",
    "            new_val = base_val * (1 + factor)\n",
    "            test_params = baseline_params.copy()\n",
    "            test_params[param] = new_val\n",
    "\n",
    "            # If we fix seeds, each scenario uses the same MASTER_SEED again\n",
    "            scenario_seed = MASTER_SEED if use_fixed_seed else None\n",
    "\n",
    "            run_result = run_single_tract_model(\n",
    "                tract_id=tract_id,\n",
    "                tracts_gdf=tracts,\n",
    "                parcels_gdf=parcels_with_tracts,\n",
    "                eligible_parcels_gdf=eligible_parcels,\n",
    "                all_household_data=household_data,\n",
    "                steps=steps,\n",
    "                scenario_seed=scenario_seed,  # same seed => identical randomness\n",
    "                **test_params\n",
    "            )\n",
    "\n",
    "            # Absolute change\n",
    "            struct_change = run_result[\"structural_perc\"] - base_struct\n",
    "            rain_change   = run_result[\"raincatching_perc\"] - base_rain\n",
    "\n",
    "            results[param][\"structural\"].append(struct_change)\n",
    "            results[param][\"raincatching\"].append(rain_change)\n",
    "\n",
    "    return results, sensitivity_levels, baseline_params, base_struct, base_rain\n",
    "\n",
    "\n",
    "########################################\n",
    "# 3) Actually run the sensitivity & store\n",
    "########################################\n",
    "\n",
    "# chosen_tract_id from your prior cell or selection method:\n",
    "# chosen_tract_id = ...\n",
    "\n",
    "# Now run:\n",
    "stored_results, stored_sensitivity_levels, baseline_params, baseline_struct, baseline_rain = (\n",
    "    fixed_baseline_sensitivity_six_variations(\n",
    "        tract_id=chosen_tract_id,\n",
    "        tracts=tracts,\n",
    "        parcels_with_tracts=parcels_with_tracts,\n",
    "        eligible_parcels=eligible_parcels,\n",
    "        household_data=household_data,\n",
    "        steps=40,\n",
    "        use_fixed_seed=True   # ensures identical random draws across runs\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Sensitivity results stored in 'stored_results'.\")\n",
    "print(f\"Baseline is struct={baseline_struct:.2f}, rain={baseline_rain:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e69dd-120b-4101-afa0-a56f8f0f992a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314dba3d-9f0e-4568-8896-68bfe6a61ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf9c04c-4db7-4c27-bd1a-3ad2ff231828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "\n",
    "def format_param_name(param_name):\n",
    "    \"\"\"Format parameter names for readability.\"\"\"\n",
    "    # Remove 'W' prefixes if present\n",
    "    param_name = param_name.lstrip('w_')\n",
    "    param_name = param_name.replace('_', ' ')  # Replace underscores with spaces\n",
    "    param_name = param_name.strip()  # Remove leading/trailing spaces\n",
    "    param_name = param_name.capitalize()  # Capitalize the first letter\n",
    "    return param_name\n",
    "\n",
    "def plot_sensitivity_heatmaps_change(stored_results, stored_sensitivity_levels, baseline_params):\n",
    "    \"\"\"\n",
    "    Plots two heatmaps (structural & rain) where each cell shows the *change from baseline* \n",
    "    in adoption percentage.\n",
    "\n",
    "    Rows = parameters (from baseline_params keys)\n",
    "    Columns = the 6 factor changes: [-15%, -10%, -5%, +5%, +10%, +15%]\n",
    "\n",
    "    :param stored_results: results dict from fixed_baseline_sensitivity_six_variations,\n",
    "           e.g. stored_results[param][\"structural\"] -> list of length 6 with changes from baseline\n",
    "    :param stored_sensitivity_levels: [-0.15, -0.10, -0.05, 0.05, 0.10, 0.15]\n",
    "    :param baseline_params: the baseline param dictionary used in the sensitivity\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare row and column labels\n",
    "    param_list = [format_param_name(param) for param in baseline_params.keys()]\n",
    "    param_list_wrapped = ['\\n'.join(wrap(param, width=15)) for param in param_list]  # Wrap long names\n",
    "    factor_labels = [f\"{int(f * 100)}%\" for f in stored_sensitivity_levels]  # e.g. -15%, -10%, etc.\n",
    "    num_params = len(param_list)\n",
    "    num_factors = len(stored_sensitivity_levels)  # should be 6\n",
    "\n",
    "    # Build data matrices for structural & rain\n",
    "    struct_matrix = np.zeros((num_params, num_factors), dtype=float)\n",
    "    rain_matrix = np.zeros((num_params, num_factors), dtype=float)\n",
    "\n",
    "    # Fill them with the *change from baseline* stored in stored_results\n",
    "    for i, param in enumerate(baseline_params.keys()):\n",
    "        changes_struct = stored_results[param][\"structural\"]  # length=6\n",
    "        changes_rain = stored_results[param][\"raincatching\"]  # length=6\n",
    "        for j in range(num_factors):\n",
    "            struct_matrix[i, j] = changes_struct[j]\n",
    "            rain_matrix[i, j] = changes_rain[j]\n",
    "\n",
    "    # Determine color scale\n",
    "    max_abs_change = max(np.abs(struct_matrix).max(), np.abs(rain_matrix).max())\n",
    "\n",
    "    # Plot the heatmaps in a single figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, max(4, num_params * 0.5)))\n",
    "\n",
    "    # Structural heatmap\n",
    "    sns.heatmap(\n",
    "        struct_matrix,\n",
    "        annot=True, fmt=\".2f\",\n",
    "        cmap=\"seismic\",\n",
    "        center=0,\n",
    "        vmin=-max_abs_change, vmax=max_abs_change,\n",
    "        xticklabels=factor_labels,\n",
    "        yticklabels=param_list_wrapped,\n",
    "        cbar_kws={\"label\": \"Change from Baseline \"},\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(\"(a) Structural Adoption: Change from Baseline\", fontname=\"Times New Roman\", fontsize=14)\n",
    "    axes[0].set_xlabel(\"Parameter Variation\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    axes[0].set_ylabel(\"Parameters\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    axes[0].tick_params(axis='both', labelsize=10)\n",
    "    for label in axes[0].get_xticklabels() + axes[0].get_yticklabels():\n",
    "        label.set_fontname(\"Times New Roman\")  # Set font for tick labels\n",
    "    # Set legend font to Times New Roman\n",
    "    axes[0].collections[0].colorbar.set_label(\"Change from Baseline\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    for text in axes[0].collections[0].colorbar.ax.yaxis.get_ticklabels():\n",
    "        text.set_fontname(\"Times New Roman\")\n",
    "\n",
    "    # Raincatching heatmap\n",
    "    sns.heatmap(\n",
    "        rain_matrix,\n",
    "        annot=True, fmt=\".2f\",\n",
    "        cmap=\"seismic\",\n",
    "        center=0,\n",
    "        vmin=-max_abs_change, vmax=max_abs_change,\n",
    "        xticklabels=factor_labels,\n",
    "        yticklabels=param_list_wrapped,\n",
    "        cbar_kws={\"label\": \"Change from Baseline (%)\"},\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(\"(b) Raincatching Adoption: Change from Baseline\", fontname=\"Times New Roman\", fontsize=14)\n",
    "    axes[1].set_xlabel(\"Parameter Variation\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    axes[1].set_ylabel(\"Parameters\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    axes[1].tick_params(axis='both', labelsize=10)\n",
    "    for label in axes[1].get_xticklabels() + axes[1].get_yticklabels():\n",
    "        label.set_fontname(\"Times New Roman\")  # Set font for tick labels\n",
    "    # Set legend font to Times New Roman\n",
    "    axes[1].collections[0].colorbar.set_label(\"Change from Baseline\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    for text in axes[1].collections[0].colorbar.ax.yaxis.get_ticklabels():\n",
    "        text.set_fontname(\"Times New Roman\")\n",
    "\n",
    "    # Add borders around each subplot (entire plot area)\n",
    "    for ax in axes:\n",
    "        rect = plt.Rectangle(\n",
    "            (0, 0), 1, 1, transform=ax.transAxes,\n",
    "            linewidth=1, edgecolor='black', facecolor='none', zorder=3\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_sensitivity_heatmaps_change(stored_results, stored_sensitivity_levels, baseline_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a06843-59e3-4266-9b0a-4e868e662cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e9c5d-0573-4bbe-87b1-a138332960d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "\n",
    "########################################\n",
    "# 0) Optional: MASTER SEED or replicate seeds\n",
    "########################################\n",
    "# If you want one single seed for all runs:\n",
    "MASTER_SEED = 1234\n",
    "\n",
    "# Or if you want multiple replicates per scenario, define replicate_seeds = [111,222,333,...]\n",
    "# For simplicity, we'll do one run per scenario here.\n",
    "\n",
    "########################################\n",
    "# 1) Selection Function: Select All Tracts in District 7\n",
    "########################################\n",
    "def select_district_tracts(district_id, tracts_gdf):\n",
    "    \"\"\"\n",
    "    Select all tracts within the specified district.\n",
    "\n",
    "    Args:\n",
    "        district_id (str or int): The identifier for the district.\n",
    "        tracts_gdf (GeoDataFrame): GeoDataFrame containing tract information with 'concl_d' column for district ID.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tract_ids (GEOIDs) within the specified district.\n",
    "    \"\"\"\n",
    "    selected_tracts = tracts_gdf[tracts_gdf['concl_d'] == str(district_id)]\n",
    "    tract_ids = selected_tracts['GEOID'].tolist()\n",
    "    if not tract_ids:\n",
    "        print(f\"No tracts found in District {district_id}.\")\n",
    "    else:\n",
    "        print(f\"Selected Tracts in District {district_id}: {tract_ids}\")\n",
    "    return tract_ids\n",
    "\n",
    "########################################\n",
    "# 2) Run District Model with Multiple Tracts\n",
    "########################################\n",
    "def run_district_model(\n",
    "    district_id,\n",
    "    tracts_gdf,\n",
    "    parcels_gdf,\n",
    "    eligible_parcels_gdf,\n",
    "    all_household_data,\n",
    "    steps=40,\n",
    "    # The parameters below:\n",
    "    self_weight=0.85,\n",
    "    adopter_weight=0.3,\n",
    "    SEM_attitude_factor=0.4,\n",
    "    SEM_awareness_factor=0.4,\n",
    "    SEM_personal_norms_factor=0.4,\n",
    "    w_similarity=0.3,\n",
    "    w_communication=0.1,\n",
    "    w_visual=0.3,\n",
    "    w_spatial=0.3,\n",
    "    network_influence_rate=0.001,\n",
    "    neighborhood_influence_rate=0.001,\n",
    "    # If you'd like to fix the seed inside each call:\n",
    "    scenario_seed=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs GSIAdoptionModel for a district with specified parameters.\n",
    "    Returns: dict with \"structural_perc\" and \"raincatching_perc\".\n",
    "    \n",
    "    If scenario_seed is not None, we re-seed random and numpy so that\n",
    "    every run with the same scenario_seed reproduces identical random draws.\n",
    "    \"\"\"\n",
    "\n",
    "    # If you want to fix the randomness for each scenario:\n",
    "    if scenario_seed is not None:\n",
    "        random.seed(scenario_seed)\n",
    "        np.random.seed(scenario_seed)\n",
    "\n",
    "    # 1) Select all tracts in the district\n",
    "    tract_ids = select_district_tracts(district_id, tracts_gdf)\n",
    "    if not tract_ids:\n",
    "        print(f\"No tracts found for District {district_id}. Returning None.\")\n",
    "        return None, 0\n",
    "\n",
    "    # 2) Filter data to multiple tracts\n",
    "    tracts_selected = tracts_gdf[tracts_gdf[\"GEOID\"].isin(tract_ids)]\n",
    "    parcels_selected = parcels_gdf[parcels_gdf[\"GEOID\"].isin(tract_ids)]\n",
    "    eligible_parcels_selected = eligible_parcels_gdf[eligible_parcels_gdf[\"GEOID\"].isin(tract_ids)]\n",
    "\n",
    "    # 3) Filter household_data to those tracts\n",
    "    district_households = [h for h in all_household_data if h[0] in tract_ids]\n",
    "    if len(district_households) == 0:\n",
    "        print(f\"No households found in District {district_id}. Returning 0 adoption.\")\n",
    "        return {\"structural_perc\": 0.0, \"raincatching_perc\": 0.0}\n",
    "\n",
    "    # 4) Create mini-model\n",
    "    mini_model = GSIAdoptionModel(\n",
    "        tracts=tracts_selected,\n",
    "        parcels=parcels_selected,\n",
    "        eligible_parcels=eligible_parcels_selected,\n",
    "        household_data=district_households\n",
    "    )\n",
    "\n",
    "    # 5) Overwrite agent parameters\n",
    "    for agent in mini_model.schedule.agents:\n",
    "        agent.self_weight = self_weight\n",
    "        agent.adopter_weight_nonadopter = adopter_weight\n",
    "        agent.SEM_attitude_factor = SEM_attitude_factor\n",
    "        agent.SEM_awareness_factor = SEM_awareness_factor\n",
    "        agent.SEM_personal_norms_factor = SEM_personal_norms_factor\n",
    "        agent.W_SIMILARITY = w_similarity\n",
    "        agent.W_COMMUNICATION = w_communication\n",
    "        agent.W_VISUAL = w_visual\n",
    "        agent.W_SPATIAL = w_spatial\n",
    "        agent.network_influence_rate = network_influence_rate\n",
    "        agent.neighborhood_influence_rate = neighborhood_influence_rate\n",
    "\n",
    "    # 6) Run the model for 'steps'\n",
    "    for _ in range(steps):\n",
    "        mini_model.step()\n",
    "\n",
    "    # 7) Collect final adoption across all tracts in the district\n",
    "    total_households = len(district_households)\n",
    "    if total_households == 0:\n",
    "        return {\"structural_perc\": 0.0, \"raincatching_perc\": 0.0}\n",
    "\n",
    "    total_struct_count = sum(\n",
    "        mini_model.adoption_counts[tract_id].get(\"structural\", 0) for tract_id in tract_ids\n",
    "    )\n",
    "    total_rain_count = sum(\n",
    "        mini_model.adoption_counts[tract_id].get(\"raincatching\", 0) for tract_id in tract_ids\n",
    "    )\n",
    "\n",
    "    struct_perc = (total_struct_count / total_households) * 100\n",
    "    rain_perc = (total_rain_count / total_households) * 100\n",
    "\n",
    "    return {\n",
    "        \"structural_perc\": struct_perc,\n",
    "        \"raincatching_perc\": rain_perc\n",
    "    }\n",
    "\n",
    "########################################\n",
    "# 3) Fixed Baseline Sensitivity for District\n",
    "########################################\n",
    "def fixed_baseline_sensitivity_six_variations_district(\n",
    "    district_id,\n",
    "    tracts,\n",
    "    parcels_with_tracts,\n",
    "    eligible_parcels,\n",
    "    household_data,\n",
    "    steps=40,\n",
    "    use_fixed_seed=True\n",
    "):\n",
    "    \"\"\"\n",
    "    For each parameter, we run 6 variations: -15%, -10%, -5%, +5%, +10%, +15%.\n",
    "    We'll compute the absolute change in structural & rain adoption from baseline.\n",
    "    \n",
    "    Returns:\n",
    "        (results, sensitivity_levels, baseline_params, baseline_struct, baseline_rain)\n",
    "    \n",
    "    If use_fixed_seed=True, we fix the same MASTER_SEED for baseline & each scenario\n",
    "    so that only parameter differences matter, not random differences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Baseline parameter dictionary\n",
    "    baseline_params = {\n",
    "        \"self_weight\": 0.85,\n",
    "        \"adopter_weight\": 0.3,\n",
    "        \"SEM_attitude_factor\": 0.4,\n",
    "        \"SEM_awareness_factor\": 0.4,\n",
    "        \"SEM_personal_norms_factor\": 0.4,\n",
    "        \"w_similarity\": 0.3,\n",
    "        \"w_communication\": 0.1,\n",
    "        \"w_visual\": 0.3,\n",
    "        \"w_spatial\": 0.3,\n",
    "        \"network_influence_rate\": 0.001,\n",
    "        \"neighborhood_influence_rate\": 0.001\n",
    "    }\n",
    "\n",
    "    # Six sensitivity levels\n",
    "    sensitivity_levels = [-0.15, -0.10, -0.05, 0.05, 0.10, 0.15]\n",
    "\n",
    "    # 1) Run single baseline\n",
    "    #    If we fix seed, do it here:\n",
    "    scenario_seed = MASTER_SEED if use_fixed_seed else None\n",
    "\n",
    "    baseline_result = run_district_model(\n",
    "        district_id=district_id,\n",
    "        tracts_gdf=tracts,\n",
    "        parcels_gdf=parcels_with_tracts,\n",
    "        eligible_parcels_gdf=eligible_parcels,\n",
    "        all_household_data=household_data,\n",
    "        steps=steps,\n",
    "        scenario_seed=scenario_seed,   # seed if desired\n",
    "        **baseline_params\n",
    "    )\n",
    "    if baseline_result is None:\n",
    "        print(\"Baseline model did not run due to lack of data.\")\n",
    "        return None, sensitivity_levels, baseline_params, 0, 0\n",
    "\n",
    "    base_struct = baseline_result[\"structural_perc\"]\n",
    "    base_rain   = baseline_result[\"raincatching_perc\"]\n",
    "    print(f\"Baseline => Structural Adoption: {base_struct:.2f}%, Raincatching Adoption: {base_rain:.2f}%\")\n",
    "\n",
    "    # We'll accumulate results in a dictionary\n",
    "    results = {param: {\"structural\": [], \"raincatching\": []} for param in baseline_params}\n",
    "\n",
    "    # 2) For each parameter, run 6 variations\n",
    "    for param, base_val in baseline_params.items():\n",
    "        for factor in sensitivity_levels:\n",
    "            new_val = base_val * (1 + factor)\n",
    "            test_params = baseline_params.copy()\n",
    "            test_params[param] = new_val\n",
    "\n",
    "            # If we fix seeds, each scenario uses the same MASTER_SEED again\n",
    "            scenario_seed = MASTER_SEED if use_fixed_seed else None\n",
    "\n",
    "            run_result = run_district_model(\n",
    "                district_id=district_id,\n",
    "                tracts_gdf=tracts,\n",
    "                parcels_gdf=parcels_with_tracts,\n",
    "                eligible_parcels_gdf=eligible_parcels,\n",
    "                all_household_data=household_data,\n",
    "                steps=steps,\n",
    "                scenario_seed=scenario_seed,  # same seed => identical randomness\n",
    "                **test_params\n",
    "            )\n",
    "\n",
    "            if run_result is None:\n",
    "                struct_change = 0\n",
    "                rain_change = 0\n",
    "            else:\n",
    "                # Absolute change\n",
    "                struct_change = run_result[\"structural_perc\"] - base_struct\n",
    "                rain_change   = run_result[\"raincatching_perc\"] - base_rain\n",
    "\n",
    "            results[param][\"structural\"].append(struct_change)\n",
    "            results[param][\"raincatching\"].append(rain_change)\n",
    "\n",
    "    return results, sensitivity_levels, baseline_params, base_struct, base_rain\n",
    "\n",
    "########################################\n",
    "# 4) Plotting Function: Sensitivity Heatmaps\n",
    "########################################\n",
    "def format_param_name(param_name):\n",
    "    \"\"\"Format parameter names for readability.\"\"\"\n",
    "    # Remove 'W' prefixes if present\n",
    "    param_name = param_name.lstrip('w_')\n",
    "    param_name = param_name.replace('_', ' ')  # Replace underscores with spaces\n",
    "    param_name = param_name.strip()  # Remove leading/trailing spaces\n",
    "    param_name = param_name.capitalize()  # Capitalize the first letter\n",
    "    return param_name\n",
    "\n",
    "def plot_sensitivity_heatmaps_change(stored_results, stored_sensitivity_levels, baseline_params):\n",
    "    \"\"\"\n",
    "    Plots two heatmaps (structural & rain) where each cell shows the *change from baseline* \n",
    "    in adoption percentage.\n",
    "\n",
    "    Rows = parameters (from baseline_params keys)\n",
    "    Columns = the 6 factor changes: [-15%, -10%, -5%, +5%, +10%, +15%]\n",
    "\n",
    "    :param stored_results: results dict from fixed_baseline_sensitivity_six_variations_district,\n",
    "           e.g. stored_results[param][\"structural\"] -> list of length 6 with changes from baseline\n",
    "    :param stored_sensitivity_levels: [-0.15, -0.10, -0.05, 0.05, 0.10, 0.15]\n",
    "    :param baseline_params: the baseline param dictionary used in the sensitivity\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare row and column labels\n",
    "    param_list = [format_param_name(param) for param in baseline_params.keys()]\n",
    "    param_list_wrapped = ['\\n'.join(wrap(param, width=15)) for param in param_list]  # Wrap long names\n",
    "    factor_labels = [f\"{int(f * 100)}%\" for f in stored_sensitivity_levels]  # e.g. -15%, -10%, etc.\n",
    "    num_params = len(param_list)\n",
    "    num_factors = len(stored_sensitivity_levels)  # should be 6\n",
    "\n",
    "    # Build data matrices for structural & rain\n",
    "    struct_matrix = np.zeros((num_params, num_factors), dtype=float)\n",
    "    rain_matrix = np.zeros((num_params, num_factors), dtype=float)\n",
    "\n",
    "    # Fill them with the *change from baseline* stored in stored_results\n",
    "    for i, param in enumerate(baseline_params.keys()):\n",
    "        changes_struct = stored_results[param][\"structural\"]  # length=6\n",
    "        changes_rain = stored_results[param][\"raincatching\"]  # length=6\n",
    "        for j in range(num_factors):\n",
    "            struct_matrix[i, j] = changes_struct[j]\n",
    "            rain_matrix[i, j] = changes_rain[j]\n",
    "\n",
    "    # Determine color scale\n",
    "    max_abs_change = max(np.abs(struct_matrix).max(), np.abs(rain_matrix).max())\n",
    "    if max_abs_change == 0:\n",
    "        max_abs_change = 1  # Prevent division by zero\n",
    "\n",
    "    # Plot the heatmaps in a single figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, max(4, num_params * 0.5)))\n",
    "\n",
    "    # Structural heatmap\n",
    "    sns.heatmap(\n",
    "        struct_matrix,\n",
    "        annot=True, fmt=\".2f\",\n",
    "        cmap=\"seismic\",\n",
    "        center=0,\n",
    "        vmin=-max_abs_change, vmax=max_abs_change,\n",
    "        xticklabels=factor_labels,\n",
    "        yticklabels=param_list_wrapped,\n",
    "        cbar_kws={\"label\": \"Change from Baseline (%)\"},\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(\"(a) Structural Adoption: Change from Baseline\", fontname=\"Times New Roman\", fontsize=14)\n",
    "    axes[0].set_xlabel(\"Parameter Variation\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    axes[0].set_ylabel(\"Parameters\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    axes[0].tick_params(axis='both', labelsize=10)\n",
    "    for label in axes[0].get_xticklabels() + axes[0].get_yticklabels():\n",
    "        label.set_fontname(\"Times New Roman\")  # Set font for tick labels\n",
    "    # Set legend font to Times New Roman\n",
    "    axes[0].collections[0].colorbar.set_label(\"Change from Baseline (%)\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    for text in axes[0].collections[0].colorbar.ax.yaxis.get_ticklabels():\n",
    "        text.set_fontname(\"Times New Roman\")\n",
    "\n",
    "    # Raincatching heatmap\n",
    "    sns.heatmap(\n",
    "        rain_matrix,\n",
    "        annot=True, fmt=\".2f\",\n",
    "        cmap=\"seismic\",\n",
    "        center=0,\n",
    "        vmin=-max_abs_change, vmax=max_abs_change,\n",
    "        xticklabels=factor_labels,\n",
    "        yticklabels=param_list_wrapped,\n",
    "        cbar_kws={\"label\": \"Change from Baseline (%)\"},\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(\"(b) Raincatching Adoption: Change from Baseline\", fontname=\"Times New Roman\", fontsize=14)\n",
    "    axes[1].set_xlabel(\"Parameter Variation\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    axes[1].set_ylabel(\"Parameters\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    axes[1].tick_params(axis='both', labelsize=10)\n",
    "    for label in axes[1].get_xticklabels() + axes[1].get_yticklabels():\n",
    "        label.set_fontname(\"Times New Roman\")  # Set font for tick labels\n",
    "    # Set legend font to Times New Roman\n",
    "    axes[1].collections[0].colorbar.set_label(\"Change from Baseline (%)\", fontname=\"Times New Roman\", fontsize=12)\n",
    "    for text in axes[1].collections[0].colorbar.ax.yaxis.get_ticklabels():\n",
    "        text.set_fontname(\"Times New Roman\")\n",
    "\n",
    "    # Add borders around each subplot (entire plot area)\n",
    "    for ax in axes:\n",
    "        rect = plt.Rectangle(\n",
    "            (0, 0), 1, 1, transform=ax.transAxes,\n",
    "            linewidth=1, edgecolor='black', facecolor='none', zorder=3\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "########################################\n",
    "# 5) Running the Sensitivity Analysis for District 7\n",
    "########################################\n",
    "\n",
    "# Ensure that your spatial and household data are already loaded and preprocessed.\n",
    "# For example:\n",
    "# tracts = gpd.read_file(\"path_to_tracts_shapefile.shp\")\n",
    "# parcels_with_tracts = gpd.read_file(\"path_to_parcels_shapefile.shp\")\n",
    "# eligible_parcels = ... # Your existing filtering logic\n",
    "# household_data = ... # Your existing household data\n",
    "\n",
    "# Define District ID for District 7\n",
    "DISTRICT_ID = '7'  # Adjust based on your 'concl_d' encoding\n",
    "\n",
    "# Now run the sensitivity analysis for District 7\n",
    "stored_results, stored_sensitivity_levels, baseline_params, baseline_struct, baseline_rain = (\n",
    "    fixed_baseline_sensitivity_six_variations_district(\n",
    "        district_id=DISTRICT_ID,\n",
    "        tracts=tracts,\n",
    "        parcels_with_tracts=parcels_with_tracts,\n",
    "        eligible_parcels=eligible_parcels,\n",
    "        household_data=household_data,\n",
    "        steps=40,\n",
    "        use_fixed_seed=True   # ensures identical random draws across runs\n",
    "    )\n",
    ")\n",
    "\n",
    "if stored_results is not None:\n",
    "    print(\"Sensitivity results stored in 'stored_results'.\")\n",
    "    print(f\"Baseline is Structural Adoption = {baseline_struct:.2f}%, Raincatching Adoption = {baseline_rain:.2f}%\")\n",
    "\n",
    "    # Plot the sensitivity heatmaps\n",
    "    plot_sensitivity_heatmaps_change(stored_results, stored_sensitivity_levels, baseline_params)\n",
    "else:\n",
    "    print(\"Sensitivity analysis could not be performed due to lack of data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2cac1-d5af-4654-a1a9-cf471405752c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
